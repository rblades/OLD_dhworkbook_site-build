{
    "docs": [
        {
            "location": "/", 
            "text": "Crafting Digital History, A Workbook by Shawn Graham and Rob Blades\n\n\n\n\nThis workbook supports \n#HIST3814o\n in the Late Summer term at \nCarleton University\n. This is the third major reworking of the workbook. The original \nWinter 2015 version is available on GitHub.\n\n\nFor more advanced tutorials and help, please see\n\n\n\n\n\n\nThe Programming Historian\n\n\n\n\n\n\nDigital History Methods in R\n\n\n\n\n\n\nShawn Graham, shawn dot graham at carleton dot ca, \n@electricarchaeo\n\n\n\n\n\n\nDisclaimer: Photosensitivity and eye strain\n\n\nSome of the digital tools and tutorial videos used in this workbook can potentially cause eye strain or effect those with photosensitivity. Make sure to take breaks often, relax your eyes, and stretch.\n\n\nOriginal content by Shawn Graham is \n\n\nlicensed under a \nCreative Commons Attribution-NonCommercial 4.0 International License\n.\n\n\nCite this workbook:", 
            "title": "Home"
        }, 
        {
            "location": "/#crafting-digital-history-a-workbook-by-shawn-graham-and-rob-blades", 
            "text": "This workbook supports  #HIST3814o  in the Late Summer term at  Carleton University . This is the third major reworking of the workbook. The original  Winter 2015 version is available on GitHub.  For more advanced tutorials and help, please see    The Programming Historian    Digital History Methods in R    Shawn Graham, shawn dot graham at carleton dot ca,  @electricarchaeo", 
            "title": "Crafting Digital History, A Workbook by Shawn Graham and Rob Blades"
        }, 
        {
            "location": "/#disclaimer-photosensitivity-and-eye-strain", 
            "text": "Some of the digital tools and tutorial videos used in this workbook can potentially cause eye strain or effect those with photosensitivity. Make sure to take breaks often, relax your eyes, and stretch.  Original content by Shawn Graham is   licensed under a  Creative Commons Attribution-NonCommercial 4.0 International License .  Cite this workbook:", 
            "title": "Disclaimer: Photosensitivity and eye strain"
        }, 
        {
            "location": "/introduction/crafting-digital-history/", 
            "text": "Getting yourself ready July 4th - 9th\n\n\n\n\n\"'Getting Ready for the Fire, Kasier Wilhelm' Bain Collection, Library of Congress hdl.loc.gov/loc.pnp/ggbain.10412 Call Number: LC-B2- 2400-7\"\n\n\nWelcome! This workbook is made by converting several plain-text files into a fully-operational website using the \nMkDocs\n static website generator\n. That means, if you want to keep a copy of all these files for your own records, you may. Simply click on the 'edit on github' link at the top right. This will bring you to the repository that houses this workbook. Then, when you're signed into GitHub, you can \nfork\n (that is, make a copy) the repo into your own account. Why 'forking'? It seems an odd phrase. Think of it like this:\n\n\nWriting, crafting, coding: these are like a small river, flowing in one direction into the future. You get new ideas, new hunches: the river branches. There's a fork in its path. Sometimes new ideas, new hunches fold back into that original stream: they merge.\n\n\nGitHub is a way of mapping that stream, and a guide to revisiting the interesting parts of it.\n\n\nIt's not the best metaphor, but it'll do. No doubt you've had that experience where, after working on an essay for days, you make a change that you regret. You wish you could go back to fix it, but ctrl+z only goes so far. You realize that everything you've done for the last week needs to be thrown out, and you start over.\n\n\nWell, with 'versioning control', you can travel back upriver, back to where things were good. There are two tools that we will use to make this happen: \ngit\n and \nGitHub\n. You'll learn more about making those things work in \nModule 1\n. You'll see why you'd want to do that, and how to future-proof your work, writing things \nin a plain-text format called 'Markdown'\n.\n\n\nIn \nModule 2\n, we'll start exploring the wide variety of historical data out there. We'll talk about some of the ethical dilemmas posed by having so much data out there. \nData\n are not neutral 'things given'; rather, they are \ncapta\n: things \ntaken\n.\n\n\nNB\n I am assuming, in this course, that the digital materials we want have already been digitized. Digitizing, adding meta-data (information that describes the data), and structuring it \nproperly\n are very complex topics on their own that could be the subject of a complete course! If you are interested in those problems, a good place to start is this open course from SOAS on \ndatabase design for historians\n.\n\n\nIn \nModule 3\n, we'll see that data/capta are \nmessy\n, and that they make a kind of illusory order that as historians, we are not normally in the habit of thinking about. The big secret of digital history is that the majority of your time on \nany\n digital history project won't be finding the capta, won't be analyzing the capta, won't be thinking about the historical meaning of the capta. We'll be \ncleaning it up\n. The exercises in this module will help you do that more efficiently (and be aware that 'efficiency' in computing terms is not a theoretically neutral idea!)\n\n\nIn \nModule 4\n, we talk about doing the analysis. I present finding, cleaning, and analyzing as if they were sequential steps, but in reality they are circular. Movement in one aspect often requires revisiting another one! This module explores how we do this, and what it means for us as historians.\n\n\nIn \nModule 5\n, we begin at last to think about how we communicate all of this to our audiences. Look at how \none university lays out the expectations for digital history work\n (and, do you see how this ties back to ideas about \nparadata\n?). We will think about things like typography and layout. We'll look at ways of making our visualizations more compelling, more effective. We will also create an online digital exhibit using \nOmeka\n.\n\n\nFinally, while there is no formal requirement for you to do this, it would be a great way of launching yourself as a digital historian to think about how you would formally reveal your project work in this class to the wider world: and then do it. There's a lot of noise on the internet, especially when it comes to history. How do you, as a digital historian, make the strongest possible signal?\n\n\nWhat you need to do this week\n\n\n\n\nFollow the instructions below to set up your digital history workspace\n\n\nAnnotate the course manual for any parts of it that are unclear (or alternatively, that have you excited)\n\n\nRespond to the readings and the reading questions by annotating the readings themselves - see the instructions below.\n\n\nSubmit your work to the course submission form\n\n\n\n\nSetting up a your workspace\n\n\nA digital historian needs to have a digital workshop/lab/studio/performance space. Such a space serves a number of functions:\n\n\n\n\nA scratch pad / fail log and code repository so that we remember what we were doing, or (more importantly) \nwhat\n we we did - that is to say, the actual commands we typed, the sequence of manipulations or \ndata moves\n.\n\n\nA narrative that connects the dots, that explains the \nwhy\n of that what and how. You can use this narrative to point to when sharing your work with others. Digital history is not done in isolation or in a vaccuum. Sometimes, you will need to share a link to your work (often on twitter) asking, 'does anybody know why this isn't working?' or, 'does anybody know a better way of accomplishing this?', or, 'hey, I'm the first to do this!'\n\n\nA way of keeping notes on things we've read/come across on the web. There are a number of ways of accomplishing this. In this course, I will mandate one particular solution: (\nHypothes.is\n).\n\n\nWhen you're working with academic databases such as JSTOR, then you'll also need a bibliography manager. We don't go into this aspect very much in this course (if you take other courses with me, you will) but you might want to check out \nZotero\n.\n\n\nIt can sometimes be useful to make little videos of your work to explain when something isn't working - \nScreen-cast-o-matic\n is free and does a good job.\n\n\n\n\nNow, the final part of your studio/lab/workshop is a domain (website + complete access to the webserver that powers it, so that you can install other platforms/services) of your own. A typical setup will be something along the lines of:\n\n\n\n\nyour-domain-name.org\n for your narrative, annotations, and anything you build regarding your work.\n\n\ngithub.com/your-name\n for your scratch pad and code repository.\n\n\n\n\nOn your blog/narrative, you'll have a page that collates all of your web annotations as well. You'll also have - typically - an 'about' page where you can signal the kinds of history you're interested in, and the preferred way for people to get in touch with you. You do not have to use your real name. Remember the \nreal names policy\n\n\nBecause you have complete access and control over your domain, you can install other services as well. For instance, maybe you use Dropbox or Google Drive to sync your files across machines, or to function as a backup? You can install a service called 'OwnCloud' on your own domain that does the same thing, so that you have control over all your own materials.\n\n\nSo let's get started.\n\n\nReclaim Hosting\n\n\nIn the course space for cuLearn, I gave you a code to use to pay for a domain of your own. I have already purchased domain space for you from an academic web hosting service, \nReclaim Hosting\n. This space will last for one year, at which point you have the option of paying to renew it or letting it die. Previous students in this course have used their domain to help with their applications for jobs and graduate school. One such is \nMelissa\n.\n\n\n\n\nYou will be asked for the name you want to have for your space. You need to be thinking of branding here. Think of a professional name that conveys something of your personality and approach to history. I for instance own the domain, 'Electric Archaeology', which I chose to convey that I'm interested in digital archaeology, but also, that I move fast and cover a lot of breaking develops in the field (hey. It's my blog. I like the name). Please choose wisely. Some combination of your first and last name is often the best idea, since your own name is your own best calling card ('shawn graham' is such a generic name, that it was already long gone by the time I started doing digital history). Type in a name, select a top-level domain (ie, .com, .ca, .org, etc. I'll suggest .ca), and click on the 'check availability' button.\n\n\nIf the pop-up says 'congratulations, domain available!' then click on the continue button. (You may be offered free id protection, where Reclaim Hosting will hide your details is someone does a 'who-is' search on the domain name. If it does, then tick off the check box to confirm that you want this, and hit continue).\n\n\nOn the next screen, (the billing screen) fill in all of the information. The balance should be $0. At the bottom left, it will also say that you\u2019ve used a one-time promotional code. Hit the green button at the bottom to complete the purchase (which is not costing you anything).\n\n\n\n\nCongratulations! You now own your very own domain. \nIt might take a bit of time for your domain to appear live on the web\n. During this time, you can log into your cPanel and install Wordpress and so on - see the next section below.\n\n\nGiving you a space of your own is my political act of protest against the centralization of learning inside learning management systems. Learning isn't 'managed', it's cultivated. Unlike cuLearn, I cannot monitor your activity inside your own domain. I can only see what you choose to make public. Unlike Facebook or Snapchat or Instagram, I am not trying to monitize your personality on the web.\n\n\nWordpress for your blog\n\n\nWordpress is probably the best option for a platform for writing the narrative bits of your digital history work.\n\n\n\n\nClick the 'client area. It will tell you that you have one active account (with Reclaim Hosting) and one active domain. When the time comes to renew your account or to close it down, this is where you do it. Note also that there is a link to 'Support', which will put you in touch with Reclaim Hosting's help desk. They are extremely fast and good at providing support; always treat any help request you make with them \nas if\n you were writing a formal email to me. Be polite and considerate, and thank them. The owners of the business often are the ones who provide the help! Without them, we couldn't do this class.\n\n\nGo to 'cPanel' - this is where you can install all sorts of different kinds of software on your account. Search for and select 'web applications'\n\n\nClick on Wordpress. Then click on 'install this application'.\n\n\nThe next screen presents you with a number of options. Leave these set to their defaults. For 'location', leave this blank (you want to leave the directory option blank). That tells the installtron to put Wordpress at your-domain.ca . (When/if you install other pieces of software, you'd change this variable so that the new software doesn't overwrite this software!)\n\n\nFurther down the page, under 'settings', you need to change 'administrator username', 'administrator password', 'web site title', 'website tagline'. \nthis is the username and password to actually do things with your blog, and the name of your blog itself\n. Leave everything else alone.\n\n\nClick Install!\n\n\nOnce it's done, the installer will remind you of the URL to your site, and the URL to the blog's dashboard (where you go to write posts, make changes to the look and feel of the site). Open these in a new browser window, and bookmark them. To login to your blog, remember to go to the dashboard URL (eg. \nhttp://your-domain.ca/wp-admin\n), enter your blog administrator username and password.\n\n\n\n\nYou can close the cPanel webpage now (log out first).\n\n\nCustomising your blog\n\n\nIf you look at the dashboard for your blog, you'll see a lot of options down the left hand side of your screen. If you want to change the look of your blog, click on 'appearance' then click on 'themes'. A number of themes are pre-installed. You can click on the different themes and select 'preview' to see how your blog might look. When you find one you like, select 'activate'. Go to the other browser window where you have your-domain.ca open. Reload the page to see the changes take effect!\n\n\nIf you're the sort of person who likes to sketch ideas out on paper, Lauren Heywood of the \nDisruptive Media Learning Lab\n has designed a paper-based exercise to prototype ideas. Why not give it a try? \nPrint this PDF of Wordpress design (opens in a new windows)\n and follow the instructions.\n\n\nTo write new content, know that there is a difference between a 'post' and a 'page'. A page is a new link on your site, while posts appear in chronological order on a page, with the most recent on top. Most themes show the most recent post by default, and pages appear in any menus on the site. \nWhen you are logged into your blog\n any post or page will have an 'edit' button at the bottom that you can click. You'll then be presented with an editing box with the standard toolbar across the top (allowing you to change the font, insert images, change the alignment of the text and so on). At the top right will be a button to save or publish/update the post/page.\n\n\nYour blog will have a default 'about' page. Change that default text now to reflect something about who you are and why you are taking this course\n\n\nTo create new pages, you click on the 'pages' link in your dashboard and select 'add new'\n\n\nTo create new posts, you click on the 'posts' link in  your dashboard and select 'add new'.\n\n\nExplore the options for your blog; customize and make the space your own.\n\n\nPassword protected posts\n If for any reason you feel that you don't want a post to be viewable by the web-at-large, you can hide it behind a password. At the top right where the 'publish' button hides, click on 'visibility' and select 'password protected'. Remember though: you'll have to share the password with me for grading purposes.\n\n\n\n\nFor more information about controlling visibility of your posts and so on, \nvisit the Wordpress content visibility help page\n.\n\n\nHypothes.is\n\n\nHypothes.is is an overlay on the web that allows you to highlight or annotate any text you come across (including \ninside pdfs\n). All of your annotations can then be collected together. It is a very effective research tool.\n\n\n\n\nCreate an account with \nHypothes.is\n.\n\n\nGet the \nHypothes.is plugin for chrome\n. If you don't have/use chrome, go to \nthe Hypothes.is start page\n and click on the 'other browsers' link.\n\n\n\n\nOnce you're logged into Hypothes.is, and you have the plugin installed, highlight THIS TEXT and leave an annotation! Who will be first? There are a few different kinds of annotations you can make; \nhere is a list with videos showing them\n.\n\n\nIf you need step-by-step instructions for installing and using Hypothes.is, please \nvisit the Hypothes.is help page\n and/or watch the video below:\n\n\n\n\n\nAnnotations are public by default. When you are making a new annotation you can toggle the visibility so that they are private and so visible only to you.\n\n\nYou can also 'tag' any annotation you make. If many people use the same set of tags, you can collect annotations by tag. This can make it easier to do group research projects, for instance.\n\n\nPlease always tag your annotations with 'hist3814o'. That way, everyone's tags will show up on \nthe course Hypothes.is page\n\n\nCollecting your own annotations on your blog\n\n\nHypothes.is has an \nAPI\n that allows you to do some neat things. 'API' stands for 'application programming interface', which is just a fancy way of saying, 'you can write a program that interacts with this web service'. \nKris Shaffer\n, a professor at the University of Mary Washington, has written a \nplugin\n for Wordpress that allows you to automatically collect annotations you make across the web and to display them all on a single page on your blog. So, we'll go get that plugin and install it on your blog:\n\n\nOpen \nKris Shaffer's Hypothes.is aggregator\n in a new browser window.\n\n\n\n\nClick \u201cClone or Download\u201d (the green button at the top right).\n\n\nIn the pop-up, click 'Download ZIP'\n\n\nGo over to the dashboard for your blog (if you closed this, you can get there again by opening a new browser window and going to your-domain.ca/wp-admin)\n\n\nIn the dashboard, click on Plugins \n Add New\n\n\nClick \u201cUpload Plugin\u201d. It will open a window asking you to find and select the zip file you downloaded. This is probably in your 'downloads' folder. Select and click ok.\n\n\nOnce it\u2019s uploaded and installed, click \u201cActivate\"\n\n\nIn the dashboard, click on 'pages' then add a new page. Call it 'Web Notes' or 'Annotations' or something similar.\n\n\nShaffer has created a 'shortcode' that tells Wordpress to go over to Hypothes.is and grab the latest information. So, in the text of your new page (in the editor window, make sure to click the 'text' button or else this won't work), enter the shortcode that will grab all of your public annotations: \n[hypothesis user = 'kris.shaffer']\n where you remove the \nkris.shaffer\n and put in your own Hypothes.is username.\n\n\nHit the 'publish' button. Wordpress will report that the page has been published and give you a link to it; click on this link to see your new always-updating list of annotations! Of course, if you haven't made any public annotations yet, nothing will display. Go annotate something, then come back and reload the page.\n\n\n\n\nGitHub\n\n\nFinally, you need a GitHub account. We will go into more detail about how to use GitHub in the next module. For now, go to \nGitHub.com\n and sign up for a new account. Follow all the prompts, and make a note of the direct URL to your account (mine for instance is \nhttp://github.com/shawngraham\n). Put this link in your 'about' page on your blog. You'll learn how to use this space in \nModule 1, Exercise 3\n.\n\n\nYour digital history lab/studio/workshop\n\n\nYou now have a digital history lab equipped with all of the necessary ingredients for doing digital history. You have an open notebook for recording what you are up to (both your narrative and your annotations). In GitHub, you have a scratch pad for keeping track of the actual nuts-and-bolts of any digital work you do (and note that it is entirely possible to do digital history successfully without having to do anything that a computer scientist for instance would call coding). You have a domain that you control completely, and where you can install other platforms and services as seems necessary.\n\n\nVPN \n DH Box\n\n\nTo access our virtual computer, the DH Box, you will need to use Carleton's VPN service. Please visit \nCarleton's VPN help page\n and follow the instructions for your particular computer. Once you've got it installed, you will need to connect to Carleton through the VPN with your mycarletone credentials. Indeed, you should always connect via a VPN whenever you're using a public wifi point (like in coffee shops). The VPN acts like a private tunnel from your computer to Carleton's servers. To the rest of the internet, it will now look as if you actually are on campus. Once you're connected via the VPN, you can \naccess the DH Box through your browser\n. Bookmark the site; you'll use it in the exercises in Module 1.\n\n\nUsing the DH Box\n\n\n\n\nClick the 'sign up' button\n\n\nFill in the form. Choose a username and password that you'll remember. You don't have to use a real email by the way, just something that looks email-like (this is handy if, like me, you end up creating multiple DH Boxes - it's a bad idea to have more than one DH Box with the \nsame\n email address)\n\n\nSelect the most time available (which will either be 1 or 2 months).\n\n\nYour personal DH Box will be created. Your username will now appear in the top right hand side of the purple bar. To enter the DH Box, click the username, select 'apps'.\n\n\nA new tool ribbon appears below the purple bar. Most of what you will do in this course involves the 'command line', 'R studio', and 'File Manager'.\n\n\nAnytime the command line or R Studio should ask for your username or password, you use the DH Box username and password you just created.\n\n\n\n\nA note on using the university computer labs\n if you are using an official University computer lab computer to access DH Box, aspects of the University's security system might block the RStudio aspect. I am working on a solution to this problem. If you know that you are going to have to use Carleton computers, get in touch right away.\n\n\nSome Readings To Kick Things Off\n\n\nWhat is digital history anyway? How is it connected to so-called 'big data'? Read the following pieces. Annotate with Hypothes.is anything that strikes you as interesting; annotate anything that puzzles you - feel free to just say, 'I'm not sure what this means; does it mean.... does anybody have any ideas?' \nand\n if you see someone is asking questions, you can reply to that annotation with thoughts of your own!\n\n\nNB\n Each week, I expect you to respond to at least someone else's annotation in a \nsubstantive\n way. No \"I agree!\" or \"right on!\" or that sort of thing. Make a \nmeaningful\n contribution.\n\n\nOnce you have read and annotated the works, \nwrite a post on your blog that poses the question 'what is digital history for me anyway?'\n . Explain why you're in this class, your level of comfort with digital tech, the kinds of history you're interested in, and what you hope to get out of this course. Your post should link to relevant annotations made by you or by your peers. (Every Hypothes.is annotation has a direct link visible when you click on the 'share' icon for an existing annotation).\n\n\nReadings\n\n\nExcerpts from \nChapter 1, the Historian's Macroscope original draft\n; read from 'Joys of Big Data' to 'Chapter One Conclusion'. Use Hypothes.is to annotate rather than the 'commenting' function on the site.\n\n\nJames Baker \n'The soft digital history that underpins my book' https://cradledincaricature.com/2017/05/24/the-soft-digital-history-that-underpins-my-book/\n\n\nJames Baker \n'The hard digital history that underpins my book' https://cradledincaricature.com/2017/06/06/the-hard-digital-history-that-underpins-my-book/\n\n\nJo Guldi and David Armitage, \n'The History Manifesto: Chapter 4\n'\n\n\nTim Hitchcock \n'Big Data for Dead People' https://historyonics.blogspot.ca/2013/12/big-data-for-dead-people-digital.html\n\n\n'On Diversity in Digital History', \nThe Macroscope\n. Read and follow through the footnotes to at least two more articles\n\n\nAcknowledgements\n\n\nThe writing of this workbook took place alongside the writing of my more formal book on \ndigital methods\n co-authored with the exceptional \nIan Milligan\n and \nScott Weingart\n. I learned far more about doing digital history from them than they ever from me, and someday, I hope to repay the debt. Other folks who've been instrumental in getting this workbook and course off the ground include Melodee Beals, John Bonnett, Chad Gaffield, Tamara Vaughan, the staff of the EDC at Carleton University, \neCampusOntario\n and of course, the digital history community on Twitter. My thanks to you all.\n\n\nThis class was first offered in the Winter 2015 semester at Carleton University in Ottawa Canada as HIST3907b. I am grateful to the participants in that class for the feedback and frank discussions of what worked and what didn't. To see the earlier version of the course, please feel free to browse its \nGitHub repository", 
            "title": "Getting Started"
        }, 
        {
            "location": "/introduction/crafting-digital-history/#getting-yourself-ready-july-4th-9th", 
            "text": "\"'Getting Ready for the Fire, Kasier Wilhelm' Bain Collection, Library of Congress hdl.loc.gov/loc.pnp/ggbain.10412 Call Number: LC-B2- 2400-7\"  Welcome! This workbook is made by converting several plain-text files into a fully-operational website using the  MkDocs  static website generator . That means, if you want to keep a copy of all these files for your own records, you may. Simply click on the 'edit on github' link at the top right. This will bring you to the repository that houses this workbook. Then, when you're signed into GitHub, you can  fork  (that is, make a copy) the repo into your own account. Why 'forking'? It seems an odd phrase. Think of it like this:  Writing, crafting, coding: these are like a small river, flowing in one direction into the future. You get new ideas, new hunches: the river branches. There's a fork in its path. Sometimes new ideas, new hunches fold back into that original stream: they merge.  GitHub is a way of mapping that stream, and a guide to revisiting the interesting parts of it.  It's not the best metaphor, but it'll do. No doubt you've had that experience where, after working on an essay for days, you make a change that you regret. You wish you could go back to fix it, but ctrl+z only goes so far. You realize that everything you've done for the last week needs to be thrown out, and you start over.  Well, with 'versioning control', you can travel back upriver, back to where things were good. There are two tools that we will use to make this happen:  git  and  GitHub . You'll learn more about making those things work in  Module 1 . You'll see why you'd want to do that, and how to future-proof your work, writing things  in a plain-text format called 'Markdown' .  In  Module 2 , we'll start exploring the wide variety of historical data out there. We'll talk about some of the ethical dilemmas posed by having so much data out there.  Data  are not neutral 'things given'; rather, they are  capta : things  taken .  NB  I am assuming, in this course, that the digital materials we want have already been digitized. Digitizing, adding meta-data (information that describes the data), and structuring it  properly  are very complex topics on their own that could be the subject of a complete course! If you are interested in those problems, a good place to start is this open course from SOAS on  database design for historians .  In  Module 3 , we'll see that data/capta are  messy , and that they make a kind of illusory order that as historians, we are not normally in the habit of thinking about. The big secret of digital history is that the majority of your time on  any  digital history project won't be finding the capta, won't be analyzing the capta, won't be thinking about the historical meaning of the capta. We'll be  cleaning it up . The exercises in this module will help you do that more efficiently (and be aware that 'efficiency' in computing terms is not a theoretically neutral idea!)  In  Module 4 , we talk about doing the analysis. I present finding, cleaning, and analyzing as if they were sequential steps, but in reality they are circular. Movement in one aspect often requires revisiting another one! This module explores how we do this, and what it means for us as historians.  In  Module 5 , we begin at last to think about how we communicate all of this to our audiences. Look at how  one university lays out the expectations for digital history work  (and, do you see how this ties back to ideas about  paradata ?). We will think about things like typography and layout. We'll look at ways of making our visualizations more compelling, more effective. We will also create an online digital exhibit using  Omeka .  Finally, while there is no formal requirement for you to do this, it would be a great way of launching yourself as a digital historian to think about how you would formally reveal your project work in this class to the wider world: and then do it. There's a lot of noise on the internet, especially when it comes to history. How do you, as a digital historian, make the strongest possible signal?", 
            "title": "Getting yourself ready July 4th - 9th"
        }, 
        {
            "location": "/introduction/crafting-digital-history/#what-you-need-to-do-this-week", 
            "text": "Follow the instructions below to set up your digital history workspace  Annotate the course manual for any parts of it that are unclear (or alternatively, that have you excited)  Respond to the readings and the reading questions by annotating the readings themselves - see the instructions below.  Submit your work to the course submission form", 
            "title": "What you need to do this week"
        }, 
        {
            "location": "/introduction/crafting-digital-history/#setting-up-a-your-workspace", 
            "text": "A digital historian needs to have a digital workshop/lab/studio/performance space. Such a space serves a number of functions:   A scratch pad / fail log and code repository so that we remember what we were doing, or (more importantly)  what  we we did - that is to say, the actual commands we typed, the sequence of manipulations or  data moves .  A narrative that connects the dots, that explains the  why  of that what and how. You can use this narrative to point to when sharing your work with others. Digital history is not done in isolation or in a vaccuum. Sometimes, you will need to share a link to your work (often on twitter) asking, 'does anybody know why this isn't working?' or, 'does anybody know a better way of accomplishing this?', or, 'hey, I'm the first to do this!'  A way of keeping notes on things we've read/come across on the web. There are a number of ways of accomplishing this. In this course, I will mandate one particular solution: ( Hypothes.is ).  When you're working with academic databases such as JSTOR, then you'll also need a bibliography manager. We don't go into this aspect very much in this course (if you take other courses with me, you will) but you might want to check out  Zotero .  It can sometimes be useful to make little videos of your work to explain when something isn't working -  Screen-cast-o-matic  is free and does a good job.   Now, the final part of your studio/lab/workshop is a domain (website + complete access to the webserver that powers it, so that you can install other platforms/services) of your own. A typical setup will be something along the lines of:   your-domain-name.org  for your narrative, annotations, and anything you build regarding your work.  github.com/your-name  for your scratch pad and code repository.   On your blog/narrative, you'll have a page that collates all of your web annotations as well. You'll also have - typically - an 'about' page where you can signal the kinds of history you're interested in, and the preferred way for people to get in touch with you. You do not have to use your real name. Remember the  real names policy  Because you have complete access and control over your domain, you can install other services as well. For instance, maybe you use Dropbox or Google Drive to sync your files across machines, or to function as a backup? You can install a service called 'OwnCloud' on your own domain that does the same thing, so that you have control over all your own materials.  So let's get started.", 
            "title": "Setting up a your workspace"
        }, 
        {
            "location": "/introduction/crafting-digital-history/#reclaim-hosting", 
            "text": "In the course space for cuLearn, I gave you a code to use to pay for a domain of your own. I have already purchased domain space for you from an academic web hosting service,  Reclaim Hosting . This space will last for one year, at which point you have the option of paying to renew it or letting it die. Previous students in this course have used their domain to help with their applications for jobs and graduate school. One such is  Melissa .   You will be asked for the name you want to have for your space. You need to be thinking of branding here. Think of a professional name that conveys something of your personality and approach to history. I for instance own the domain, 'Electric Archaeology', which I chose to convey that I'm interested in digital archaeology, but also, that I move fast and cover a lot of breaking develops in the field (hey. It's my blog. I like the name). Please choose wisely. Some combination of your first and last name is often the best idea, since your own name is your own best calling card ('shawn graham' is such a generic name, that it was already long gone by the time I started doing digital history). Type in a name, select a top-level domain (ie, .com, .ca, .org, etc. I'll suggest .ca), and click on the 'check availability' button.  If the pop-up says 'congratulations, domain available!' then click on the continue button. (You may be offered free id protection, where Reclaim Hosting will hide your details is someone does a 'who-is' search on the domain name. If it does, then tick off the check box to confirm that you want this, and hit continue).  On the next screen, (the billing screen) fill in all of the information. The balance should be $0. At the bottom left, it will also say that you\u2019ve used a one-time promotional code. Hit the green button at the bottom to complete the purchase (which is not costing you anything).   Congratulations! You now own your very own domain.  It might take a bit of time for your domain to appear live on the web . During this time, you can log into your cPanel and install Wordpress and so on - see the next section below.  Giving you a space of your own is my political act of protest against the centralization of learning inside learning management systems. Learning isn't 'managed', it's cultivated. Unlike cuLearn, I cannot monitor your activity inside your own domain. I can only see what you choose to make public. Unlike Facebook or Snapchat or Instagram, I am not trying to monitize your personality on the web.", 
            "title": "Reclaim Hosting"
        }, 
        {
            "location": "/introduction/crafting-digital-history/#wordpress-for-your-blog", 
            "text": "Wordpress is probably the best option for a platform for writing the narrative bits of your digital history work.   Click the 'client area. It will tell you that you have one active account (with Reclaim Hosting) and one active domain. When the time comes to renew your account or to close it down, this is where you do it. Note also that there is a link to 'Support', which will put you in touch with Reclaim Hosting's help desk. They are extremely fast and good at providing support; always treat any help request you make with them  as if  you were writing a formal email to me. Be polite and considerate, and thank them. The owners of the business often are the ones who provide the help! Without them, we couldn't do this class.  Go to 'cPanel' - this is where you can install all sorts of different kinds of software on your account. Search for and select 'web applications'  Click on Wordpress. Then click on 'install this application'.  The next screen presents you with a number of options. Leave these set to their defaults. For 'location', leave this blank (you want to leave the directory option blank). That tells the installtron to put Wordpress at your-domain.ca . (When/if you install other pieces of software, you'd change this variable so that the new software doesn't overwrite this software!)  Further down the page, under 'settings', you need to change 'administrator username', 'administrator password', 'web site title', 'website tagline'.  this is the username and password to actually do things with your blog, and the name of your blog itself . Leave everything else alone.  Click Install!  Once it's done, the installer will remind you of the URL to your site, and the URL to the blog's dashboard (where you go to write posts, make changes to the look and feel of the site). Open these in a new browser window, and bookmark them. To login to your blog, remember to go to the dashboard URL (eg.  http://your-domain.ca/wp-admin ), enter your blog administrator username and password.   You can close the cPanel webpage now (log out first).", 
            "title": "Wordpress for your blog"
        }, 
        {
            "location": "/introduction/crafting-digital-history/#customising-your-blog", 
            "text": "If you look at the dashboard for your blog, you'll see a lot of options down the left hand side of your screen. If you want to change the look of your blog, click on 'appearance' then click on 'themes'. A number of themes are pre-installed. You can click on the different themes and select 'preview' to see how your blog might look. When you find one you like, select 'activate'. Go to the other browser window where you have your-domain.ca open. Reload the page to see the changes take effect!  If you're the sort of person who likes to sketch ideas out on paper, Lauren Heywood of the  Disruptive Media Learning Lab  has designed a paper-based exercise to prototype ideas. Why not give it a try?  Print this PDF of Wordpress design (opens in a new windows)  and follow the instructions.  To write new content, know that there is a difference between a 'post' and a 'page'. A page is a new link on your site, while posts appear in chronological order on a page, with the most recent on top. Most themes show the most recent post by default, and pages appear in any menus on the site.  When you are logged into your blog  any post or page will have an 'edit' button at the bottom that you can click. You'll then be presented with an editing box with the standard toolbar across the top (allowing you to change the font, insert images, change the alignment of the text and so on). At the top right will be a button to save or publish/update the post/page.  Your blog will have a default 'about' page. Change that default text now to reflect something about who you are and why you are taking this course  To create new pages, you click on the 'pages' link in your dashboard and select 'add new'  To create new posts, you click on the 'posts' link in  your dashboard and select 'add new'.  Explore the options for your blog; customize and make the space your own.  Password protected posts  If for any reason you feel that you don't want a post to be viewable by the web-at-large, you can hide it behind a password. At the top right where the 'publish' button hides, click on 'visibility' and select 'password protected'. Remember though: you'll have to share the password with me for grading purposes.   For more information about controlling visibility of your posts and so on,  visit the Wordpress content visibility help page .", 
            "title": "Customising your blog"
        }, 
        {
            "location": "/introduction/crafting-digital-history/#hypothesis", 
            "text": "Hypothes.is is an overlay on the web that allows you to highlight or annotate any text you come across (including  inside pdfs ). All of your annotations can then be collected together. It is a very effective research tool.   Create an account with  Hypothes.is .  Get the  Hypothes.is plugin for chrome . If you don't have/use chrome, go to  the Hypothes.is start page  and click on the 'other browsers' link.   Once you're logged into Hypothes.is, and you have the plugin installed, highlight THIS TEXT and leave an annotation! Who will be first? There are a few different kinds of annotations you can make;  here is a list with videos showing them .  If you need step-by-step instructions for installing and using Hypothes.is, please  visit the Hypothes.is help page  and/or watch the video below:   Annotations are public by default. When you are making a new annotation you can toggle the visibility so that they are private and so visible only to you.  You can also 'tag' any annotation you make. If many people use the same set of tags, you can collect annotations by tag. This can make it easier to do group research projects, for instance.  Please always tag your annotations with 'hist3814o'. That way, everyone's tags will show up on  the course Hypothes.is page", 
            "title": "Hypothes.is"
        }, 
        {
            "location": "/introduction/crafting-digital-history/#collecting-your-own-annotations-on-your-blog", 
            "text": "Hypothes.is has an  API  that allows you to do some neat things. 'API' stands for 'application programming interface', which is just a fancy way of saying, 'you can write a program that interacts with this web service'.  Kris Shaffer , a professor at the University of Mary Washington, has written a  plugin  for Wordpress that allows you to automatically collect annotations you make across the web and to display them all on a single page on your blog. So, we'll go get that plugin and install it on your blog:  Open  Kris Shaffer's Hypothes.is aggregator  in a new browser window.   Click \u201cClone or Download\u201d (the green button at the top right).  In the pop-up, click 'Download ZIP'  Go over to the dashboard for your blog (if you closed this, you can get there again by opening a new browser window and going to your-domain.ca/wp-admin)  In the dashboard, click on Plugins   Add New  Click \u201cUpload Plugin\u201d. It will open a window asking you to find and select the zip file you downloaded. This is probably in your 'downloads' folder. Select and click ok.  Once it\u2019s uploaded and installed, click \u201cActivate\"  In the dashboard, click on 'pages' then add a new page. Call it 'Web Notes' or 'Annotations' or something similar.  Shaffer has created a 'shortcode' that tells Wordpress to go over to Hypothes.is and grab the latest information. So, in the text of your new page (in the editor window, make sure to click the 'text' button or else this won't work), enter the shortcode that will grab all of your public annotations:  [hypothesis user = 'kris.shaffer']  where you remove the  kris.shaffer  and put in your own Hypothes.is username.  Hit the 'publish' button. Wordpress will report that the page has been published and give you a link to it; click on this link to see your new always-updating list of annotations! Of course, if you haven't made any public annotations yet, nothing will display. Go annotate something, then come back and reload the page.", 
            "title": "Collecting your own annotations on your blog"
        }, 
        {
            "location": "/introduction/crafting-digital-history/#github", 
            "text": "Finally, you need a GitHub account. We will go into more detail about how to use GitHub in the next module. For now, go to  GitHub.com  and sign up for a new account. Follow all the prompts, and make a note of the direct URL to your account (mine for instance is  http://github.com/shawngraham ). Put this link in your 'about' page on your blog. You'll learn how to use this space in  Module 1, Exercise 3 .", 
            "title": "GitHub"
        }, 
        {
            "location": "/introduction/crafting-digital-history/#your-digital-history-labstudioworkshop", 
            "text": "You now have a digital history lab equipped with all of the necessary ingredients for doing digital history. You have an open notebook for recording what you are up to (both your narrative and your annotations). In GitHub, you have a scratch pad for keeping track of the actual nuts-and-bolts of any digital work you do (and note that it is entirely possible to do digital history successfully without having to do anything that a computer scientist for instance would call coding). You have a domain that you control completely, and where you can install other platforms and services as seems necessary.", 
            "title": "Your digital history lab/studio/workshop"
        }, 
        {
            "location": "/introduction/crafting-digital-history/#vpn-dh-box", 
            "text": "To access our virtual computer, the DH Box, you will need to use Carleton's VPN service. Please visit  Carleton's VPN help page  and follow the instructions for your particular computer. Once you've got it installed, you will need to connect to Carleton through the VPN with your mycarletone credentials. Indeed, you should always connect via a VPN whenever you're using a public wifi point (like in coffee shops). The VPN acts like a private tunnel from your computer to Carleton's servers. To the rest of the internet, it will now look as if you actually are on campus. Once you're connected via the VPN, you can  access the DH Box through your browser . Bookmark the site; you'll use it in the exercises in Module 1.", 
            "title": "VPN &amp; DH Box"
        }, 
        {
            "location": "/introduction/crafting-digital-history/#using-the-dh-box", 
            "text": "Click the 'sign up' button  Fill in the form. Choose a username and password that you'll remember. You don't have to use a real email by the way, just something that looks email-like (this is handy if, like me, you end up creating multiple DH Boxes - it's a bad idea to have more than one DH Box with the  same  email address)  Select the most time available (which will either be 1 or 2 months).  Your personal DH Box will be created. Your username will now appear in the top right hand side of the purple bar. To enter the DH Box, click the username, select 'apps'.  A new tool ribbon appears below the purple bar. Most of what you will do in this course involves the 'command line', 'R studio', and 'File Manager'.  Anytime the command line or R Studio should ask for your username or password, you use the DH Box username and password you just created.   A note on using the university computer labs  if you are using an official University computer lab computer to access DH Box, aspects of the University's security system might block the RStudio aspect. I am working on a solution to this problem. If you know that you are going to have to use Carleton computers, get in touch right away.", 
            "title": "Using the DH Box"
        }, 
        {
            "location": "/introduction/crafting-digital-history/#some-readings-to-kick-things-off", 
            "text": "What is digital history anyway? How is it connected to so-called 'big data'? Read the following pieces. Annotate with Hypothes.is anything that strikes you as interesting; annotate anything that puzzles you - feel free to just say, 'I'm not sure what this means; does it mean.... does anybody have any ideas?'  and  if you see someone is asking questions, you can reply to that annotation with thoughts of your own!  NB  Each week, I expect you to respond to at least someone else's annotation in a  substantive  way. No \"I agree!\" or \"right on!\" or that sort of thing. Make a  meaningful  contribution.  Once you have read and annotated the works,  write a post on your blog that poses the question 'what is digital history for me anyway?'  . Explain why you're in this class, your level of comfort with digital tech, the kinds of history you're interested in, and what you hope to get out of this course. Your post should link to relevant annotations made by you or by your peers. (Every Hypothes.is annotation has a direct link visible when you click on the 'share' icon for an existing annotation).", 
            "title": "Some Readings To Kick Things Off"
        }, 
        {
            "location": "/introduction/crafting-digital-history/#readings", 
            "text": "Excerpts from  Chapter 1, the Historian's Macroscope original draft ; read from 'Joys of Big Data' to 'Chapter One Conclusion'. Use Hypothes.is to annotate rather than the 'commenting' function on the site.  James Baker  'The soft digital history that underpins my book' https://cradledincaricature.com/2017/05/24/the-soft-digital-history-that-underpins-my-book/  James Baker  'The hard digital history that underpins my book' https://cradledincaricature.com/2017/06/06/the-hard-digital-history-that-underpins-my-book/  Jo Guldi and David Armitage,  'The History Manifesto: Chapter 4 '  Tim Hitchcock  'Big Data for Dead People' https://historyonics.blogspot.ca/2013/12/big-data-for-dead-people-digital.html  'On Diversity in Digital History',  The Macroscope . Read and follow through the footnotes to at least two more articles", 
            "title": "Readings"
        }, 
        {
            "location": "/introduction/crafting-digital-history/#acknowledgements", 
            "text": "The writing of this workbook took place alongside the writing of my more formal book on  digital methods  co-authored with the exceptional  Ian Milligan  and  Scott Weingart . I learned far more about doing digital history from them than they ever from me, and someday, I hope to repay the debt. Other folks who've been instrumental in getting this workbook and course off the ground include Melodee Beals, John Bonnett, Chad Gaffield, Tamara Vaughan, the staff of the EDC at Carleton University,  eCampusOntario  and of course, the digital history community on Twitter. My thanks to you all.  This class was first offered in the Winter 2015 semester at Carleton University in Ottawa Canada as HIST3907b. I am grateful to the participants in that class for the feedback and frank discussions of what worked and what didn't. To see the earlier version of the course, please feel free to browse its  GitHub repository", 
            "title": "Acknowledgements"
        }, 
        {
            "location": "/module-1/Open-Access-Research/", 
            "text": "Open Access Research - July 10 - 16\n\n\nConcepts\n\n\nAs historians, we aren't all that accustomed to sharing our research notes. We go to the archives, we take our photographs, we spend \nhours\n pouring over documents, photographs, diaries, newspapers... why should someone else benefit from \nour\n work?\n\n\nThere are a number of reasons why you should want to do this. This week we will read and discuss the arguments advanced by various historians, including:\n\n\n\n\nTrevor Owens\n\n\nCaleb McDaniel\n\n\nIan Milligan\n\n\nanother post by Milligan\n\n\nMichelle Moravec\n\n\nKathleen Fitzpatrick\n\n\nSheila Brennan\n\n\n\n\nBut most importantly, \nchange is coming whether historians like it or not\n. Here in Canada, \nSSHRC\n has a \nresearch data archiving policy\n\n\n\n\nAll research data collected with the use of SSHRC funds must be preserved and made available for use by others within a reasonable period of time. SSHRC considers \u201ca reasonable period\u201d to be within two years of the completion of the research project for which the data was collected.\n\n\n\n\nNote the \nconversation that ensued on Twitter after Milligan mentioned all this\n and also \nIan's tweet on documenting research\n\n\nWe will explore\n\n\n\n\nwhy and how to make our research notes open,\n\n\nwhat that implies for how we do research,\n\n\nand how we can use this process to maintain our scholarly voice online.\n\n\n\n\nReally, it's also a kind of \n'knowledge mobilization'\n. In this module you will find exercises related to setting up your GitHub account, how to commit, fork, push and pull files to your own repository and to others'. Really, it's about \nsustainable authorship\n and \npreserving your research data\n.\n\n\nBy the end of this module you will know\n\n\n\n\nhow to work with GitHub to foster collaboration\n\n\nhow to set up, fork, and make changes to files and repositories\n\n\nthe rationale for historians to make their work public\n\n\n\n\nRemember: I do expect you to click through every link I provide, and to read these materials.\n\n\nWhat you need to do this week\n\n\n\n\nRespond to the readings and the reading questions through annotation (taking care to respond to others' annotations as well) - see the instructions below. Remember to tag your annotations with 'hist3814o' so we can find them \non the course Hypothesis group\n\n\nDo the exercises for this module, pushing yourself as far as you can. Annotate the instructions where they might be unclear or confusing; see if others have annotated them as well, and respond to them with help if you can. Keep an eye on our Slack channel - you can always offer help or seek out help there. Write a blog post describing what happened as you went through the exercises (your successes, your failures, the help you may have found/received), and link to your 'faillog' (ie, the notes you upload to your GitHub account - for more on that, see the exercises!).\n\n\nSubmit your work to the course submission form\n\n\n\n\nReadings\n\n\nAs you read the posts linked to above (Brennan, Fitzgerald, Guldi and Armitage, McDaniel, Milligan, Moravec, Owens) click through to their 'about' or 'portfolio' pages. \n\n\n\n\nHow are these scholars portraying themselves? \n\n\nHow do they approach the idea of 'openness'?\n\n\nHow is your own work 'generous'? \n\n\n\n\nPlease annotate their work with your observations and questions; please also respond to someone else's annotation with a substantive observation of your own.\n\n\nThen, make an entry on your blog that contrasts this picture of 'open access research' with what you may have learned about doing history in your other courses. Where are the dangers and where are the opportunities? What does 'open access' mean for you as a student?", 
            "title": "Why you should be open"
        }, 
        {
            "location": "/module-1/Open-Access-Research/#open-access-research-july-10-16", 
            "text": "", 
            "title": "Open Access Research - July 10 - 16"
        }, 
        {
            "location": "/module-1/Open-Access-Research/#concepts", 
            "text": "As historians, we aren't all that accustomed to sharing our research notes. We go to the archives, we take our photographs, we spend  hours  pouring over documents, photographs, diaries, newspapers... why should someone else benefit from  our  work?  There are a number of reasons why you should want to do this. This week we will read and discuss the arguments advanced by various historians, including:   Trevor Owens  Caleb McDaniel  Ian Milligan  another post by Milligan  Michelle Moravec  Kathleen Fitzpatrick  Sheila Brennan   But most importantly,  change is coming whether historians like it or not . Here in Canada,  SSHRC  has a  research data archiving policy   All research data collected with the use of SSHRC funds must be preserved and made available for use by others within a reasonable period of time. SSHRC considers \u201ca reasonable period\u201d to be within two years of the completion of the research project for which the data was collected.   Note the  conversation that ensued on Twitter after Milligan mentioned all this  and also  Ian's tweet on documenting research", 
            "title": "Concepts"
        }, 
        {
            "location": "/module-1/Open-Access-Research/#we-will-explore", 
            "text": "why and how to make our research notes open,  what that implies for how we do research,  and how we can use this process to maintain our scholarly voice online.   Really, it's also a kind of  'knowledge mobilization' . In this module you will find exercises related to setting up your GitHub account, how to commit, fork, push and pull files to your own repository and to others'. Really, it's about  sustainable authorship  and  preserving your research data .", 
            "title": "We will explore"
        }, 
        {
            "location": "/module-1/Open-Access-Research/#by-the-end-of-this-module-you-will-know", 
            "text": "how to work with GitHub to foster collaboration  how to set up, fork, and make changes to files and repositories  the rationale for historians to make their work public   Remember: I do expect you to click through every link I provide, and to read these materials.", 
            "title": "By the end of this module you will know"
        }, 
        {
            "location": "/module-1/Open-Access-Research/#what-you-need-to-do-this-week", 
            "text": "Respond to the readings and the reading questions through annotation (taking care to respond to others' annotations as well) - see the instructions below. Remember to tag your annotations with 'hist3814o' so we can find them  on the course Hypothesis group  Do the exercises for this module, pushing yourself as far as you can. Annotate the instructions where they might be unclear or confusing; see if others have annotated them as well, and respond to them with help if you can. Keep an eye on our Slack channel - you can always offer help or seek out help there. Write a blog post describing what happened as you went through the exercises (your successes, your failures, the help you may have found/received), and link to your 'faillog' (ie, the notes you upload to your GitHub account - for more on that, see the exercises!).  Submit your work to the course submission form", 
            "title": "What you need to do this week"
        }, 
        {
            "location": "/module-1/Open-Access-Research/#readings", 
            "text": "As you read the posts linked to above (Brennan, Fitzgerald, Guldi and Armitage, McDaniel, Milligan, Moravec, Owens) click through to their 'about' or 'portfolio' pages.    How are these scholars portraying themselves?   How do they approach the idea of 'openness'?  How is your own work 'generous'?    Please annotate their work with your observations and questions; please also respond to someone else's annotation with a substantive observation of your own.  Then, make an entry on your blog that contrasts this picture of 'open access research' with what you may have learned about doing history in your other courses. Where are the dangers and where are the opportunities? What does 'open access' mean for you as a student?", 
            "title": "Readings"
        }, 
        {
            "location": "/module-1/Exercises/", 
            "text": "Module 1 Exercises\n\n\nThe exercises in this module are designed to give you the necessary skills to engage with the \ndoing\n of digital history. To ensure success in the course, please do make it through Exercises 1 to 3. Exercise 4 is a bit more complex and not mission-critical. Push yourself if you can.\n\n\nThe exercises in this module cover:\n\n\n\n\nWriting in Markdown\n\n\nUsing the DH Box command line\n\n\nConverting files with the command line\n\n\nSetting up GitHub\n\n\nInteracting with GitHub from the command line\n\n\n\n\nThese exercises walk you through the process of using our DH Box, and of keeping notes about what you're doing, and making those notes open on the web. If you run into trouble, \nask for help\n in our Slack space. Annotate this page with where things are going wrong for you. Contact Dr. Graham. \nYou do not have to suffer in silence!\n To ask for help when doing this work is not a sign of weakness, but of maturity.\n\n\nAll 4 exercises are on this page. Remember to scroll!\n\n\n\n\nExercise 1: Learning Markdown syntax with dillinger.io\n\n\nHave you ever fought with Word or another word processor, trying to get things \njust right\n? Word processing is a mess. It conflates writing with typesetting and layout. Sometimes, you just want to get the words out. Othertimes, you want to make your writing as accessible as possible... but your intended recipient can't open your file, because they don't use the same word processor. Or perhaps you wrote up some great notes that you'd love to have in a slideshow; but you can't, because copying and pasting preserves a whole lot of extra \ngunk\n that messes up your materials.\n\n\nThe answer is to separate your \ncontent\n from your \ntool\n.\n\n\nThis is where the \nMarkdown syntax\n shines. Markdown is a syntax for marking semantic elements within a document explicitly, not in some hidden layer. The idea is to identify units that are meaningful to humans, like titles, sections, subsections, footnotes, and illustrations. At the very least, your files will always remain comprehensible to you, even if the editor you are currently using stops working or \"goes out of business.\"\n\n\nWriting in this way liberates the author from the tool. Markdown can be written in any plain text editor and offers a rich ecosystem of software that can render that text into beautiful looking documents (incidentally, Hypothes.is annotations can be written in Markdown). For this reason, Markdown is currently enjoying a period of growth, not just as as means for writing scholarly papers but as a convention for online editing in general.\n\n\nPopular general purpose plain text editors include \nTextWrangler\n, \nSublime\n, and \nAtom\n for Mac, \nNotepad++\n for Windows, as well as \nGedit\n and \nKate\n for Linux. However, there are also editors that specialize in displaying and editing Markdown. \n\n\nNB\n A text editor is different from the default notepad app that comes with Windows or Mac. A text editor shows you \nexactly\n what is in a file.\n\n\nIn this exercise, I want you to become familiar with Markdown syntax. Check out \nSarah Simpkin's quick primer on Markdown\n. There are a number of 'flavours' for Markdown, but in essence, they all mimic the kinds of conventions you would see in an email, using asterisks to indicate emphasis and so on.\n\n\n\n\n\n\nCheck out \nthe Markdown cheatsheet\n.\n\n\n\n\n\n\nVisit \ndillinger.io\n in a new browser window. This looks like a word processor. The left hand side of the screen is where you write, the right hand side shows you what your text will look like if you converted the text to HTML. Dillinger 'saves' your work in your browser's memory. You can also point it to save to your Dropbox, Google Drive, or GitHub account (under the cogwheel icon).\n\n\n\n\n\n\nWrite a short 200-500 word piece on the most interesting annotation you've seen one of your classmates make. Why is it interesting? Why has it struck you?\n\n\n\n\n\n\n\n\n\n\n\n\nGrab at least two Creative Commons images and link outwards to four websites that are relevant to your piece. The Creative Commons license allows re-use. Do you know how to \nfind Creative Commons images\n?\n\n\n\n\n\n\nMake sure you link to the annotation in question.\n\n\n\n\n\n\nMake sure to add the file type \n.md\n at the end, in the 'document name' slot.\n\n\n\n\n\n\nSelect 'Export to' Markdown to save a copy of the file in your downloads folder.\n\n\n\n\n\n\nTry 'exporting to' PDF or HTML. Since you've separated the content from the format, this illustrates how you can convert your text into other formats as necessary. (The text is converted using a piece of software called \nPandoc\n)\n\n\n\n\n\n\n\n\n\n\n\n\nSee how easy that was? Don't worry about submitting this.... yet.\n\n\nNB\n The next time you go to dillinger.io the last document you were working on will load up. That's because dillinger stashes your work in the browser cache. If you clear your cache (from your browser's tools or settings) you'll lose it, which is why in step 7 I suggested exporting.\n\n\nFor a richer discussion of some more ways Markdown and Pandoc can make your research sustainable, see Tenen and Wythoff, \nSustainable Authorship in Plain Text Using Pandoc and Markdown\n.\n\n\n\n\nExercise 2: Getting familiar with DH Box\n\n\nSetting up DH Box\n\n\nMany of the exercises in this workbook work without issue on a Mac or Linux computer, at the terminal. (On a Mac, you can find the terminal under applications \n utilities). If you're on a Windows machine, it can be more difficult to get all of the various bits and pieces properly configured. If you're on a tablet, most digital history things you might like to do are not really feasible. One solution is for all of us to use the \nsame\n computer. The CUNY Graduate Centre has created a digital-humanities focused virtual computer for just such an occasion, the \nDH Box\n.\n\n\nIn this exercise, you are going to set up an instance of a DH Box for your own use. We will be using the command line interface which is an essential way to interact with your computer. Normally, when you're on your Mac or your PC, you're clicking on icons or menus or windows of various sorts to get things done. You're clicking and dragging text. All of these graphical elements sit on top of sequences of commands that you don't have to type out, that only the computer sees. The command line (i.e. terminal or command prompt) lets you dispense with the graphical elements, and type the commands you want the way you want them. \n\n\nNB this workbook assumes that you are using a DH Box\n\n\n\n\n\n\nCarleton students: If you are on campus or are logged into Carleton's systems via a VPN, go to \nthe sign up page at http://134.117.26.132:5000/signup\n. Other folks: Go to \nthe DH Box sign up page at http://dhbox.org/signup\n. These are two separate installations of DH Box; whichever one you start with, continue to use.\n\n\n\n\n\n\nSelect a username, password, and your email address. Your username must be four characters or longer. \n\n\n\n\n\n\nSelect '1 month'. Then select launch. You now have a virtual computer that you can use for the next month. Whenever you come back to the DH Box, you can now click 'login' on your personal DH computer and your work will be there waiting for you. Once you've logged in, the site will reload to the DH Box welcome screen, but your user name will show at the top right.\n\n\n\n\n\n\nClick on your username, and there will be a new option, \nApps.\n Click here to go into your DH Box.\n\n\n\n\n\n\nInside the DH Box, you can click on:\n\n\n\n\nHome:\n tells you how many days until your DH Box expires. Keep an eye on this, as you'll want to get your materials out of DH Box before that happens.\n\n\nFile Manager:\n allows you to view all of your files, as well as uploading/downloading materials from DH Box to your local computer.\n\n\nCommand Line:\n allows you to interact with the computer at the terminal prompt or command line \n this is where you type in commands to your machine.\n\n\nRStudio:\n is an environment for doing statistical computing.\n\n\nBrackets:\n is a text editor for web development.\n\n\nJupyter Notebooks:\n is an environment for creating documents that have running code inside of them.\n\n\n\n\nFor HIST3814o, we will use the \nFile Manager\n, the \nCommand Line\n, and \nRStudio.\n\n\nUsing the Command Line\n\n\nIn the previous exercise, \ndillinger.io\n could convert your Markdown document into HTML or PDF. We will now add the Pandoc program to DH Box so that you can convert things for yourself. We will get the Pandoc program and bring it into our DH Box using the \nwget\n command. Wget is a program that allows us to download materials off the web. It can be used to only grab certain file types, or all files within a certain directory, or even to take a complete copy of a website! We'll discuss wget more in module 2 (visit also \nthe Programming Historian on wget\n).\n\n\n\n\n\n\nSelect Command Line in your DH Box. You will have to login again with your DH Box username and password. \n\n\nNB In many tutorials or how-tos, you will see the \n$\n sign at the beginning of some text you are meant to type. This is a convention to show that what follows the \n$\n is to be typed at the command prompt.\n\n\nFor example, when you type or copy the command\n\n\n$ wget https://github.com/jgm/pandoc/releases/download/1.19.2.1/pandoc-1.19.2.1-1-amd64.deb\n\n\nyou omit the \n$\n and type or copy just the command\n\n\nwget https://github.com/jgm/pandoc/releases/download/1.19.2.1/pandoc-1.19.2.1-1-amd64.deb\n\n\n\n\n\n\nType \n$ wget https://github.com/jgm/pandoc/releases/download/1.19.2.1/pandoc-1.19.2.1-1-amd64.deb\n\n\nThis wget command gets a copy of the Pandoc program that will work in DH Box. The .deb file extension tells us that this is a Linux file. The next step is to unpack that file.\n\n\n\n\n\n\nType \n$ sudo dpkg -i pandoc-1.19.2.1-1-amd64.deb\n\n\nSome commands can have nasty side effects, and the operating system won't let you do them unless you specify that you \nreally\n want to do them. That's what the \nsudo command\n achieves. The next part, dpkg with the \n-i\n 'flag' is a 'package manager' for installing software. The last part is the file that we used wget to copy to our own machine. \n\n\n\n\n\n\nType \n$ pandoc -v\n to test that Pandoc is installed\n\n\nIf all has gone well, DH Box will tell you what version of Pandoc you have installed, who it was built by, and a copyright notice.\n\n\n\n\n\n\nType \n$ history\n\n\nThis creates a file to keep a record of what commands we have been typing.\n\n\nDH Box returns a list of every command that you've typed. \n\n\n\n\n\n\nType \n$ history \n dhbox-work-today.md\n\n\nThis will \npipe\n that information into a new file.\n\n\nWhen you hit enter, the computer seems to pause for a moment, and then it shows you the command prompt again. How do we know if anything happened? Generally, when working at the command prompt, no news is good news. There are two ways you can check to see if the new file \ndhbox-work-today.md\n was created. \nYou can click on the File Manager\n (the first folder will have your username; click on that, and then you'll see a list of files) and there it is! \nClick on the file name,\n and it will download to your computer where you can open it with a text editor.\n\n\n\n\n\n\n\n\nAlterntively, type \n$ ls\n\n\nThis \nlists\n \n ls \n all the files in the directory. How do you know what directory you're in? You can use the \npwd\n command, which prints the working directory.\n\n\nLet's take a look inside that new file you created. There is a text editor that you can use, called \nNano\n. \n\n\n\n\n\n\nType \n$ nano dhbox-work-today.md\n\n\nThis opens the text editor, and you can see that the history command has copied its output into the editor. \nIf you got an error:\n Sometimes, tools or commands we want to use are not present in the system. DH Box uses the \nUbuntu\n operating system (an operating system is the underlying code that makes a PC a PC, a Mac a Mac, etc). We can install all sorts of useful things by using the \napt-get\n command. \nTo get and install Nano, try re-installing Nano with the following command:\n\n\n$ sudo apt-get install nano\n.\n\n\nThe computer makes things a little more difficult sometimes when you're asking it to do something that changes or adds capabilities. Simply typing \napt-get\n wouldn't work. \nsudo\n tells the computer that I \nreally\n do want to execute the command (it stands for 'super-user do'). Any \nsudo\n command will make the computer ask for your password to confirm that you really do want to run that command.\n\n\nRead the beginner guide for the Nano text editor on How to Geek now\n.\n\n\n\n\n\n\nIn Nano, with your file open, make a header (remember to use \n#\n to indicate a header) at the start of the file which has today's date in it, and add some text explaining what you're trying to do with this exercise. \n\n\n\n\n\n\nHit ctrl+x to exit Nano.\n\n\n\n\n\n\nNano will ask you if you want to 'Save modified buffer?'. \nHit \ny\n, then when it asks you for the file name, hit enter.\n\n\nThe entire key sequence for saving and exiting Nano is \nctrl-x, y, enter\n.\n\n\n\n\n\n\n\n\n\n\n\n\nUse the File Manager to save a copy of \ndhbox-work-today.md\n onto your own computer.\n\n\nOne last thing: let's convert the Markdown file into both Word and HTML. Pandoc is capable of quite sophisticated transformations, but these are two of the easiest. \n\n\n\n\n\n\nType \n$ pandoc -o todayscommands.docx dhbox-work-today.md\n\n\nThis says to Pandoc, create an output file ( the \n-o\n) called \ntodayscommands.docx\n from \ndhbox-work-today.md\n. \n\n\n\n\n\n\nType \n$ ls\n after running this command to see if you've made the file. \n\n\nAny guesses how to create an HTML file? Pandoc is smart enough to know the kind of output you want from the file extension.\n\n\n\n\n\n\nRetype the command but use .html instead of .docx this time. Use the file manager to save copies of the .docx and .html files to your own machine. (Incidentally, if you use the arrow up and arrow down keys on your keyboard when you're at the command line, you can page through commands that you've previously typed).\n\n\n\n\n\n\n\n\n\n\n\n\nYou've done some interesting work \n you've installed software into a remote computer, you've copied all of the commands you typed into a new file, you've used Markdown and a text editor to add information and context to those commands, and you've used Pandoc to transform your basic text into the more complicated formats of Word or HTML.\n\n\nAs you do more work in this workbook, I want you to get in the habit of keeping these copies of what you've done. These are your actual lab notes, and they are invaluable for helping you keep track of what you've been doing and what you've been trying. If you remember the course manual, keeping a lab notebook is part of the assessment of the course.\n\n\nIn the next exercise, we learn to use another piece of software called 'git' which will enable you to set up a remote location for pushing copies of your work to, for safe keeping and for collaboration. For more on why we go to all this trouble, visit the Programming Historian on \n'Preserving Your Research Data'\n.\n\n\nVisit \nUbuntu's handy cheat-sheet of keyboard shortcuts and other useful commands\n for your DH Box command-line work.\n\n\n\n\nExercise 3: Setting up your GitHub space\n\n\nIt's a familiar situation \n you've been working on a paper. It's where you want it to be, and you're certain you're done. You save it as \nfinal.doc\n. Then, you ask your friend to take a look at it. She spots several typos and that you flubbed an entire paragraph. You open it up, make the changes, and save as \nfinal-w-changes.doc\n. Later that day it occurs to you that you don't like those changes, and you go back to the original \nfinal.doc\n, make some changes, and just overwrite the previous version. \n\n\nSoon, you have a folder resembling the following:\n\n\n|-project\n    |-'finalfinal.doc'\n    |-'final-w-changes.doc'\n    |-'final-w-changes2.doc'\n    |-'isthisone-changes.doc'\n    |-'this.doc'\n\n\n\n\nThings can get messy quite quickly. Imagine that you also have several spreadsheets in there as well, images, snippets of code... we don't want this. What we want is a way of managing the evolution of your files. We do this with a program called \ngit\n. Git is not a user-friendly piece of software, and it takes some work to get your head around. Git is also very powerful, but fortunately, the basic uses to which most of us put it to are more or less straightforward. There are many other programs that make use of git for version control; these programs weld a graphical user interface on top of the main git program. For now, we'll content ourselves with the \nGitHub website\n, which does the same thing more or less, but from a browser.\n\n\nFirstly, let's define some terms.\n\n\n\n\ngit\n is a program that keeps snapshots of the contents of a designated folder; it watches for 'difs' or differences between one snapshot and the next. You 'commit' these snapshots to a repository, which lives on your computer (it's just a folder).\n\n\nGitHub\n is a webservice that allows you to share those repositories, and to keep track of those \nversions\n online, and what's more, to share the repositories and files with multiple collaborators, and \nkeep everyone's changes straight!\n There are other services that work like GitHub, such as \nBitbucket\n. GitHub is just one of the better known services. You can browse GitHub repositories for code that solves a particular problem for you, software, and data.\n\n\n\n\nNB Because we are going to use the free version, we cannot make the repositories we create private.\n\n\nIn this exercise, we're going to create a repository via the GitHub website and use it as a kind of back-up space for the files you created in the previous exercise. In the follow up exercise, you will learn how to do this from the command line.\n\n\n\n\n\n\nGo to \nGitHub\n and sign up for an account. Remember, you don't have to use your real name. If you use a pseudonym, please communicate to me privately what your account is called.\n\n\n\n\n\n\nOnce you're logged in, we will create a new repository called \nhist3814o\n. Click on the \n+\n at the top right of the screen, beside your avatar image.\n\n\n\n\n\n\nWrite a short description in the 'description box', and tick off the 'initialize the repository with a readme'. You can also select a license from the drop down box \n this will put some standard wording on your repository page about the conditions under which someone else might use (or cite) your code.\n\n\n\n\n\n\nClick create repository.\n\n\nAt this point, you now have a folder \n a repository \n on the GitHub website into which you can deposit your files. It will be at \nhttp://github.com/your-account-name/hist3814o\n. So let's put some materials into that repository.\n\n\nNotice, when you're on your repository's page, that there is a button to 'create new file' and another for 'upload files'.\n\n\n\n\n\n\nClick on upload files.\n\n\n\n\n\n\nDrag the HTML file you created in the previous exercise (the one you made with Pandoc, and then saved to your computer via the DH Box File Manager) into the large grey box. This will upload the file. You can drag multiple files into the box to upload them into your repository. \n\n\n\n\n\n\nEnter a brief commit message in the commit message box. Then hit the green commit changes button. Git \n and GitHub \n attach messages to any 'commits' you make. These messages are brief notes explaining why you were making the commit. This way, if you ever had to roll back (go back to an earlier version) you can understand the evolution of the repository and find the spot you want. \n\n\n\n\n\n\n\n\n\n\n\n\nInstead of creating multiple versions of a file, you have a single file that has a version history. Neat, eh?\n\n\nThis is perhaps the simplest use case for GitHub. You can create files directly in the repository as well, by hitting the 'create new file' button, and following the prompts. GitHub has a \nbrief tutorial on using the website to collaborate with other people on a repository\n.\n\n\nFor the remainder of the course, use your hist3814o repository as your scratch pad, your fail log, and your open notebook for showing your work across these modules\n\n\nCheck out \nmy example 'fail log' as a model\n. 'Fail' is a pretty harsh word \n I use it to point out that for everything that works perfectly, there's an awful lot of trial-and-error that happened first \nupon which\n our successes are built. We need to keep track of this! James Baker calls this, \nde-wizardification\n.\n\n\nSome useful vocabulary when discussing git, GitHub, and version control:\n\n\n\n\nrepository\n a single folder that holds all of the files and subfolders of your project.\n\n\ncommit\n this means, 'take a snapshot of the current state of my repository'.\n\n\npublish\n take a folder on my computer, and copy it and its contents to the web as a repository at \ngithub.com/myusername/repositoryname\n.\n\n\nsync\n update the web repository with the latest commit from the folder on my computer.\n\n\nbranch\n make a copy of my repository with a 'working name'.\n\n\nmerge\n fold the changes I have made on a branch into another branch (typically, either \nmaster\n or \ngh-pages\n).\n\n\nfork\n to make a copy of someone else's repo.\n\n\nclone\n to copy a repo online onto your own computer.\n\n\npull\n request to ask the original maker of a repo to 'pull' your changes into their master, original, repository.\n\n\npush\n to move your changes from your computer to the online repo.\n\n\n\n\nAn aside\n\n\nMany websites \n including this workbook \n use a GitHub repository as a way of hosting a website. The video below by historian Jack Dougherty shows how this could be done. Note that the HTML code that he pastes into an \nindex.html\n file (the first page of any website is usually called \nindex.html\n) he got from a different service. You could write a document in Markdown, then use Pandoc to convert that into an \nindex.html\n, for example.\n\n\n\n\n\n\n\nExercise 4: A detailed look at using git on the command line\n\n\nAt its heart, git is a way of taking 'snapshots' of the current state of a folder, and saving those snapshots in sequence. (For an excellent brief presentation on git, visit \nAlice Bartlett's presentation on git\n; Bartlett is a senior developer for the Financial Times). In git's lingo, as stated earlier, a folder on your computer is known as a repository or repo. This sequence of snapshots in total lets you see how your project unfolded over time. Each time you wish to take a snapshot, you make a \ncommit\n. A commit is a git command to take a snapshot of the entire repository. Thus, your folder we discussed above, with its proliferation of documents becomes:\n\n\n|-project\n    |-'final.doc'\n\n\n\n\nBut\n its commit history could be visualized like a string of pearls, where each pearl is a unique commit. Each one of those pearls represents a point in time when you the writer made a commit; git compared the state of the file to the earlier state, and saved a snapshot of the \ndifferences\n. What is particularly useful about making a commit is that git requires two more pieces of information about the git: who is making it, and when. The final useful bit about a commit is that you can save a detailed message about \nwhy\n the commit is being made. In our hypothetical situation, your first commit message might look like this:\n\n\nFixed conclusion\n\nJulie pointed out that I had missed\nthe critical bit in the assignment\nregarding stratigraphy. This was\nadded in the concluding section.\n\n\n\n\nThis information is stored in the history of the commits. In this way, you can see exactly how the project evolved and why. Each one of these commits has what is called a \nhash\n. This is a unique fingerprint that you can use to 'time travel' (in Bartlett's felicitous phrasing). If you want to see what your project looked like a few months ago, you \ncheckout\n that commit. This has the effect of 'rewinding' the project. Once you've checked out a commit, don't be alarmed when you look at the folder: your folder (your repository) looks like how it once did all those weeks ago! Any files written after that commit seem as if they've disappeared. Don't worry, they still exist!\n\n\nWhat would happen if you wanted to experiment or take your project in a new direction from that point forward? Git lets you do this. What you will do is create a new \nbranch\n of your project from that point. You can think of a branch as like the branch of a tree, or perhaps better, a branch of a river that eventually merges back to the source. (Another way of thinking about branches is that it is a label that sticks with these particular commits.) \n\n\nIt is generally considered \nbest practice\n to leave your \nmaster\n branch alone, in the sense that it represents the best version of your project. When you want to experiment or do something new, you create a \nbranch\n and work there. If the work on the branch ultimately proves fruitless, you can discard it. \nBut\n, if you decide that you like how it's going, you can \nmerge\n that branch back into your master. A merge is a commit that folds all of the commits from the branch with the commits from the master.\n\n\nGit is also a powerful tool for backing up your work. You can work quite happily with git on your own machine, but when you store those files and the history of commits somewhere remote, you open up the possibility of collaboration \nand\n a safe place where your materials can be recalled if \n perish the thought \n something happened to your computer. In git-speak, the remote location is, well, the \nremote\n. There are many different places on the web that can function as a remote for git repositories. You can even set one up on your own server, if you want. To get material \nout\n of GitHub and onto your own computer, you \nclone\n it. If that hypothetical paper you were writing was part of a group project, your partners could clone it from your GitHub space, and work on it as well!\n\n\nLet us imagine a scenario.... You and Anna are working together on the project. You have made a new project repository in your GitHub space, and you have cloned it to your computer. Anna has cloned it to hers. Let's assume that you have a very productive weekend and you make some real headway on the project. You \ncommit\n your changes, and then \npush\n them from your computer to the GitHub version of your repository. That repository is now one commit \nahead\n of Anna's version. Anna \npulls\n those changes from GitHub to her own version of the repository, which now looks \nexactly\n like your version. What happens if you make changes to the exact same part of the exact same file? This is called a \nconflict\n. Git will make a version of the file that contains text clearly marking off the part of the file where the conflict occurs, with the conflicting information marked out as well. The way to \nresolve\n the conflict is to open the file (typically with a text editor) and to delete the added git text, making a decision on which information is the correct information.\n\n\nCaution\n what follows might take a bit of time. It walks you through setting up a git repository in your DH Box; making changes to it; making different branches; and publishing the repository to your space on GitHub's website.\n\n\n4.1. git init\n\n\nHow do you turn a folder into a repository? With the \ngit init\n command. At the command line (remember, the \n$\n just shows you the prompt; you don't have to type it!):\n\n\n\n\nType \n$ mkdir first-repo\n to make a new directory.\n\n\nType \n$ ls\n (list) to see that the directory exists. Then change directory into it: \ncd first-repo\n. (Remember: if you're ever not sure what directory you're in, type \n$ pwd\n, or print working directory.)\n\n\nMake a new file called \nreadme.md\n. You do this by calling the text editor: \nnano readme.md\n. \n\n\nType an explanation of what this exercise is about. \n\n\nHit ctrl+x to exit, then type y to save, leave the file name as it is. Hit enter. \nIf you get an error to the effect that Nano is not found\n you just need to install it with \n$ sudo apt-get install nano\n. DH Box will ask you for your password again. Once the dust settles, you can make the new file with \n$ nano readme.md\n.\n\n\nType \n$ ls\n again to check that the file is there.\n\n\nType \n$ git init\n to tell the git program that this folder is to be tracked as a repository. If all goes correctly, you should see a variation on this message: \nInitialized empty Git repository in /home/demonstration/first-repo/.git/\n.\n\n\n\n\nType \n$ ls\n again. What do you (not) see?\n\n\n\n\n\n\n\n\n\n\n\n\nThe changes in your repo will now be stored in that \nhidden\n directory, \n.git\n. Most of the time, you will never have reason to search that folder out. But know that the config file that describes your repo is in that folder. There might come a time in the future where you want to alter some of the default behaviour of the git program. You do that by opening the config file (which you can read with a text editor). Google 'show hidden files and folders' for your operating system when that time comes.\n\n\n4.2. git status\n\n\nOpen your readme.md file again with the Nano text editor, from the command line. Add some more information to it, then save and exit the text editor.\n\n\n\n\n\n\nType \n$ git status\n\n\nGit will respond with a couple of pieces of information. It will tell you which \nbranch\n you are on. It will list any untracked files present or new changes that are unstaged. \n\n\n\n\n\n\nWe now will \nstage\n those changes to be added to our commit history by typing \n$ git add -A\n. (the bit that says \n-A\n adds any new, modified, or deleted files to your commit when you make it. There are \nother options or flags\n where you add \nonly\n the new and modified files, \nor\n only the modified and deleted files.)\n\n\n\n\n\n\nLet's check our git status again: type \n$ git status\n\n\n\n\n\n\nYou should see something like this:\n\n\nOn branch master\nInitial commit\nChanges to be committed:\n  (use \"git rm --cached \nfile\n...\" to unstage)\n        new file:   readme.md\n\n\n\n\n\n\n\nLet's take a snapshot: type \n$ git commit -m \"My first commit\"\n. This command represents a bit of a shortcut for making commit messages by using the \n-m\n flag to associate the text in the quotation marks with the commit. \n\n\nWhat happened? Remember, git keeps track not only of the changes, but \nwho\n is making them. If this is your first time working with git in the DH Box, git will ask you for your name and email. \n\n\n\n\n\n\nHelpfully, the Git error message tells you exactly what to do: type \n$ git config --global user.email \"you\\@example.com\"\n and then type \n$ git config --global user.name \"Your Name\"\n. Now try making your first commit.\n\n\n\n\n\n\nOpen up your \nreadme.md\n file again, and add some more text to it. \n\n\n\n\n\n\nSave \nreadme.md\n and exit the text editor. \n\n\n\n\n\n\nAdd the new changes to the snapshot that we will take.\n\n\n\n\n\n\nType \n$ git commit\n. Git automatically opens up the text editor so you can type a longer, more substantive commit message. In this message (unlike in Markdown) the \n#\n indicates a line to be ignored. You'll see that there is already some default text in there telling you what to do. \n\n\n\n\n\n\nType a message indicating the nature of the changes you have made. \n\n\n\n\n\n\nSave and exit the text editor. \nDO NOT\n change the filename!\n\n\n\n\n\n\n\n\n\n\n\n\nCongratulations, you are now able to track your changes, and keep your materials under version control!\n\n\n4.3. git merge\n\n\nGo ahead and make some more changes to your repository. Add some new files. Commit your changes after each new file is created. \n\n\nNow we're going to view the history of your commits. \n\n\n\n\n\n\nType \n$ git log\n. What do you notice about this list of changes? Look at the time stamps. You'll see that the entries are listed in reverse chronological order. Each entry has its own 'hash' or unique ID, the person who made the commit and time are listed, as well as the commit message eg:\n\n\ncommit 253506bc23070753c123accbe7c495af0e8b5a43\nAuthor: Shawn Graham \nshawn.graham@carleton.ca\n\nDate:   Tue Feb 14 18:42:31 2017 +0000\n\nFixed the headings that were broken in the about section of readme.md\n\n\n\n\n\n\n\nWe're going to go back in time and create a new branch. You can escape the \ngit log\n by typing \nq\n. Here's how the command will look: \n$ git checkout -b branchname \ncommit\n where \nbranch\n is the name you want the branch to be called, and \ncommit\n is that unique ID. Make a new branch from your second last commit (don't use \n or \n).\n\n\n\n\nWe typed \ngit checkout -b experiment 253506bc23070753c123accbe7c495af0e8b5a43\n. The response: \nSwitched to a new branch 'experiment'\n \n\n\nCheck git status and then list the contents of your repository. What do you see? You should notice that some of the files you had created before seem to have disappeared \n congratulations, you've time travelled! Those files are not missing; but they \nare\n on a different branch (the master branch) and you can't harm them now. \n\n\nAdd a number of new files, making commits after each one. \n\n\n\n\nCheck your git status, and check your git log as you go to make sure you're getting everything. Make sure there are no unstaged changes \n everything's been committed.\n\n\n\n\n\n\n\n\n\n\n\n\n4.4. git merge continued\n\n\nNow let's assume that your \nexperiment\n branch was successful \n everything you did there you were happy with and you want to integrate all of those changes back into your \nmaster\n branch. We're going to merge things. To merge, we have to go back to the master branch: \n$ git checkout master\n. (Good practice is to keep separate branches for all major experiments or directions you go. In case you lose track of the names of the branches you've created, this command: \ngit branch -va\n will list them for you.)\n\n\n\n\nNow, we merge with \n$ git merge experiment\n. Remember, a merge is a special kind of commit that rolls all previous commits from both branches into one \n git will open your text editor and prompt you to add a message (it will have a default message already there if you want it). \n\n\n\n\nSave and exit and ta da! Your changes have been merged together.\n\n\n\n\n\n\n\n\n\n\n\n\n4.5 git push\n\n\nOne of the most powerful aspects of using git is the possibility of using it to manage collaborations. To do this, we have to make a copy of your repository available to others as a \nremote\n. There are a variety of places on the web where this can be done; one of the most popular at the moment is \nGitHub\n. GitHub allows a user to have an unlimited number of public repositories. \nPublic\n repositories can be viewed and copied by anyone. \nPrivate\n repositories require a paid account, and access is controlled. If you are working on sensitive materials that can only be shared amongst the collaborators on a project, you should invest in an upgraded account (note that you can also control which files get included in commit; \nvisit GitHub for help on ignoring files\n. In essence, you simply list the file names you do not want committed; \nvisit GitHub for an example on listing ignored files\n). \nLet's assume that your materials are not sensitive\n.\n\n\n\n\n\n\nLogin to GitHub.\n\n\n\n\n\n\nOn the upper right part of the screen there is a large + sign. Click on that, and select \nnew repository\n.\n\n\n\n\n\n\nOn the following screen, give your repo a name.\n\n\n\n\n\n\nLeave the repository set to 'Public'\n\n\n\n\n\n\nDO NOT\n 'initialize this repo with a readme.md'. Leave \nadd .gitignore\n and \nadd license\n set to NONE.\n\n\n\n\n\n\nClick the green 'Create Repository' button. You now have a space into which you will publish the repository on your machine. \n\n\n\n\n\n\nAt the command line, we now need to tell git the location of this space. We do that with the following command, where you will change \nyour-username\n and \nyour-new-repo\n appropriately:\n\n\n$ git remote add origin https://github.com/YOUR-USERNAME/YOUR-NEW-REPO.git\n\n\n\n\n\n\n\nNow we push your local copy of the repository onto the web, to the GitHub version of your repo:\n\n\n$ git push -u origin master\n\n\n\nNB If you wanted to push a \nbranch\n to your repository on the web instead, do you see how you would do that? If your branch was called \nexperiment\n, the command would look like this:\n\n\n$ git push origin experiment\n\n\n\nThe changes can sometimes take a few minutes to show up on the website. Now, the next time you make changes to this repository, you can push them to your GitHub account \n which is the 'origin' in the command above.  \n\n\n\n\n\n\nAdd a new text file. Commit the changes. Push the changes to your account.\n\n\n\n\n\n\n\n\n\n\n\n\n4.6. git clone\n\n\nImagine you are collaborating with one of your classmates. Your classmate is in charge of the project, and is keeping track of the 'official' folder of materials (i.e. the repo). You wish to make some changes to the files in that repository. You can manage that collaboration via GitHub by making a copy, what GitHub calls a \nfork\n.\n\n\n\n\n\n\nMake sure you're logged into your GitHub account on the GitHub website. We're going to fork an example repository right now by going to \nGitHub's forking example page\n. \n\n\n\n\n\n\nClick the 'fork' button at top-right. GitHub now makes a copy of the repository in your own GitHub account!\n\n\n\n\n\n\nTo make a copy of that repository on your own machine, you will now clone it with the \ngit clone\n command. (Remember: a 'fork' copies someone's GitHub repo into a repo in your OWN GitHub account; a 'clone' makes a copy on your own MACHINE). Type:\n\n\n$ cd..\n$ pwd\n\n\n\n\n\n\n\nWe do that to make sure you're not \ninside\n any other repo you've made! Make sure you're not inside the repository we used in Exercises 1 to 4, then proceed:\n\n\n$ git clone https://github.com/YOUR-USERNAME/Spoon-Knife.git\n$ ls\n\n\n\nYou now have a folder called 'Spoon-Knife' on your machine! Any changes you make inside that folder can be tracked with commits. You can also \ngit push -u origin master\n when you're inside it, and the changes will show up on your OWN copy (your fork) on \ngithub.com\n.\n\n\n\n\n\n\n\n\n\n\n\n\nMake a fork of, and then clone, one of your classmates' repositories. Create a new branch. Add a new file to the repository on your machine, and then push it to your fork on GitHub. Remember, your new file will appear on the new branch you created, NOT the master branch.\n\n\n\n\n\n\n\n\n\n\n\n\n4.7. pull request\n\n\nNow, you let your collaborator know that you've made a change that you want her to \nmerge\n into the original repository. You do this by issuing a \npull request\n. But first, we have to tell git to keep an eye on that original repository, which we will call \nupstream\n. You do this by adding that repository's location like so:\n\n\n\n\n\n\nType (but change the address appropriately):\n\n\n$ git remote add upstream THE-FULL-URL-TO-THEIR-REPO-ENDING-WITH-.git\n\n\n\n\n\n\n\nYou can keep your version of the remote up-to-date by fetching any new changes your classmate has done:\n\n\n$ git fetch upstream\n\n\n\nNow let's make a \npull\n request (you might want to \nbookmark GitHub's help document for pull requests\n). \n\n\n\n\n\n\nGo to your copy of your classmate's repository at your GitHub account. Make sure you've selected the correct branch you pushed your changes to, by selecting it from the Branches menu drop down list.\n\n\n\n\n\n\nClick the 'new pull request' button.\n\n\n\n\n\n\nThe new page that appears can be confusing, but it is trying to double check with you which changes you want to make, and where. Make sure these are set properly.\n\n\n\n\n\n\nBase branch\n is the branch where you want your changes to go (ie. your classmate's repository). \n\n\n\n\n\n\nHead branch\n is the branch where you made \nyour\n changes. \n\n\n\n\n\n\nRemember: the \nBase\n branch is the \nTO\n, the \nHead\n branch is the \nFROM\n \n the place where you want your changes to go \nTO\n, \nFROM\n the place where you made the changes. \n\n\nFor example, say I clone Dr. Graham's \nR\n repository and create a new \nexperiment\n branch: \n\n\n\n\nIf make changes to my \nexperiment\n branch \n that is the \nHead\n branch. \n\n\nThen I push those changes to Dr. Graham's \nmaster\n branch \n that is the \nBase\n branch.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA pull request has to have a message attached to it, so that your classmate knows what kind of change you're proposing. Fill in the message fields appropriately, then hit the 'create pull request' button.\n\n\n\n\n\n\n\n\n\n\n\n\n4.8. git merge again\n\n\nFinally, the last bit of work to be done is to accept the pull request and \nmerge\n the changes into the original repository.\n\n\n\n\nGo to your repository on your GitHub account. Check to see if there are any 'pull requests' \n these will be listed under the 'pull requests' tab. Click on that tab.\n\n\nYou can merge from the command line, but for now, you can simply click on the green 'merge pull request' button, and then the 'confirm merge' button. The changes your classmate has made have now been folded into your repository.\n\n\nTo get the updates on your local machine, go back to the command line and type\n  $ git pull origin master\n\n\n\n\n\n\n\nPhew. You might want to do \n$ history \n recentcommands.md\n just to remember what you've done. And then commit that to a repository.\n\n\n\n\nConclusion\n\n\nIn the modules to come, we will be using DH Box as we find data, fetch data, wrangle data, analyze data, and visualize data. It becomes very important that you note the kinds of commands you use or try, the thinking that you were doing at that point, and so on. You want to leave yourself (and anybody who comes after) breadcrumbs so that you understand what you were doing. Quick notes written in Markdown, piping the history of what you've done to a file, and keeping those files in a repository alongside any other code or files that you may make will set you on the path to open access research and computational reproducibility. As Martha might say, 'and that's a good thing'.", 
            "title": "Exercises"
        }, 
        {
            "location": "/module-1/Exercises/#module-1-exercises", 
            "text": "The exercises in this module are designed to give you the necessary skills to engage with the  doing  of digital history. To ensure success in the course, please do make it through Exercises 1 to 3. Exercise 4 is a bit more complex and not mission-critical. Push yourself if you can.  The exercises in this module cover:   Writing in Markdown  Using the DH Box command line  Converting files with the command line  Setting up GitHub  Interacting with GitHub from the command line   These exercises walk you through the process of using our DH Box, and of keeping notes about what you're doing, and making those notes open on the web. If you run into trouble,  ask for help  in our Slack space. Annotate this page with where things are going wrong for you. Contact Dr. Graham.  You do not have to suffer in silence!  To ask for help when doing this work is not a sign of weakness, but of maturity.  All 4 exercises are on this page. Remember to scroll!", 
            "title": "Module 1 Exercises"
        }, 
        {
            "location": "/module-1/Exercises/#exercise-1-learning-markdown-syntax-with-dillingerio", 
            "text": "Have you ever fought with Word or another word processor, trying to get things  just right ? Word processing is a mess. It conflates writing with typesetting and layout. Sometimes, you just want to get the words out. Othertimes, you want to make your writing as accessible as possible... but your intended recipient can't open your file, because they don't use the same word processor. Or perhaps you wrote up some great notes that you'd love to have in a slideshow; but you can't, because copying and pasting preserves a whole lot of extra  gunk  that messes up your materials.  The answer is to separate your  content  from your  tool .  This is where the  Markdown syntax  shines. Markdown is a syntax for marking semantic elements within a document explicitly, not in some hidden layer. The idea is to identify units that are meaningful to humans, like titles, sections, subsections, footnotes, and illustrations. At the very least, your files will always remain comprehensible to you, even if the editor you are currently using stops working or \"goes out of business.\"  Writing in this way liberates the author from the tool. Markdown can be written in any plain text editor and offers a rich ecosystem of software that can render that text into beautiful looking documents (incidentally, Hypothes.is annotations can be written in Markdown). For this reason, Markdown is currently enjoying a period of growth, not just as as means for writing scholarly papers but as a convention for online editing in general.  Popular general purpose plain text editors include  TextWrangler ,  Sublime , and  Atom  for Mac,  Notepad++  for Windows, as well as  Gedit  and  Kate  for Linux. However, there are also editors that specialize in displaying and editing Markdown.   NB  A text editor is different from the default notepad app that comes with Windows or Mac. A text editor shows you  exactly  what is in a file.  In this exercise, I want you to become familiar with Markdown syntax. Check out  Sarah Simpkin's quick primer on Markdown . There are a number of 'flavours' for Markdown, but in essence, they all mimic the kinds of conventions you would see in an email, using asterisks to indicate emphasis and so on.    Check out  the Markdown cheatsheet .    Visit  dillinger.io  in a new browser window. This looks like a word processor. The left hand side of the screen is where you write, the right hand side shows you what your text will look like if you converted the text to HTML. Dillinger 'saves' your work in your browser's memory. You can also point it to save to your Dropbox, Google Drive, or GitHub account (under the cogwheel icon).    Write a short 200-500 word piece on the most interesting annotation you've seen one of your classmates make. Why is it interesting? Why has it struck you?       Grab at least two Creative Commons images and link outwards to four websites that are relevant to your piece. The Creative Commons license allows re-use. Do you know how to  find Creative Commons images ?    Make sure you link to the annotation in question.    Make sure to add the file type  .md  at the end, in the 'document name' slot.    Select 'Export to' Markdown to save a copy of the file in your downloads folder.    Try 'exporting to' PDF or HTML. Since you've separated the content from the format, this illustrates how you can convert your text into other formats as necessary. (The text is converted using a piece of software called  Pandoc )       See how easy that was? Don't worry about submitting this.... yet.  NB  The next time you go to dillinger.io the last document you were working on will load up. That's because dillinger stashes your work in the browser cache. If you clear your cache (from your browser's tools or settings) you'll lose it, which is why in step 7 I suggested exporting.  For a richer discussion of some more ways Markdown and Pandoc can make your research sustainable, see Tenen and Wythoff,  Sustainable Authorship in Plain Text Using Pandoc and Markdown .", 
            "title": "Exercise 1: Learning Markdown syntax with dillinger.io"
        }, 
        {
            "location": "/module-1/Exercises/#exercise-2-getting-familiar-with-dh-box", 
            "text": "", 
            "title": "Exercise 2: Getting familiar with DH Box"
        }, 
        {
            "location": "/module-1/Exercises/#setting-up-dh-box", 
            "text": "Many of the exercises in this workbook work without issue on a Mac or Linux computer, at the terminal. (On a Mac, you can find the terminal under applications   utilities). If you're on a Windows machine, it can be more difficult to get all of the various bits and pieces properly configured. If you're on a tablet, most digital history things you might like to do are not really feasible. One solution is for all of us to use the  same  computer. The CUNY Graduate Centre has created a digital-humanities focused virtual computer for just such an occasion, the  DH Box .  In this exercise, you are going to set up an instance of a DH Box for your own use. We will be using the command line interface which is an essential way to interact with your computer. Normally, when you're on your Mac or your PC, you're clicking on icons or menus or windows of various sorts to get things done. You're clicking and dragging text. All of these graphical elements sit on top of sequences of commands that you don't have to type out, that only the computer sees. The command line (i.e. terminal or command prompt) lets you dispense with the graphical elements, and type the commands you want the way you want them.   NB this workbook assumes that you are using a DH Box    Carleton students: If you are on campus or are logged into Carleton's systems via a VPN, go to  the sign up page at http://134.117.26.132:5000/signup . Other folks: Go to  the DH Box sign up page at http://dhbox.org/signup . These are two separate installations of DH Box; whichever one you start with, continue to use.    Select a username, password, and your email address. Your username must be four characters or longer.     Select '1 month'. Then select launch. You now have a virtual computer that you can use for the next month. Whenever you come back to the DH Box, you can now click 'login' on your personal DH computer and your work will be there waiting for you. Once you've logged in, the site will reload to the DH Box welcome screen, but your user name will show at the top right.    Click on your username, and there will be a new option,  Apps.  Click here to go into your DH Box.    Inside the DH Box, you can click on:   Home:  tells you how many days until your DH Box expires. Keep an eye on this, as you'll want to get your materials out of DH Box before that happens.  File Manager:  allows you to view all of your files, as well as uploading/downloading materials from DH Box to your local computer.  Command Line:  allows you to interact with the computer at the terminal prompt or command line   this is where you type in commands to your machine.  RStudio:  is an environment for doing statistical computing.  Brackets:  is a text editor for web development.  Jupyter Notebooks:  is an environment for creating documents that have running code inside of them.   For HIST3814o, we will use the  File Manager , the  Command Line , and  RStudio.", 
            "title": "Setting up DH Box"
        }, 
        {
            "location": "/module-1/Exercises/#using-the-command-line", 
            "text": "In the previous exercise,  dillinger.io  could convert your Markdown document into HTML or PDF. We will now add the Pandoc program to DH Box so that you can convert things for yourself. We will get the Pandoc program and bring it into our DH Box using the  wget  command. Wget is a program that allows us to download materials off the web. It can be used to only grab certain file types, or all files within a certain directory, or even to take a complete copy of a website! We'll discuss wget more in module 2 (visit also  the Programming Historian on wget ).    Select Command Line in your DH Box. You will have to login again with your DH Box username and password.   NB In many tutorials or how-tos, you will see the  $  sign at the beginning of some text you are meant to type. This is a convention to show that what follows the  $  is to be typed at the command prompt.  For example, when you type or copy the command  $ wget https://github.com/jgm/pandoc/releases/download/1.19.2.1/pandoc-1.19.2.1-1-amd64.deb  you omit the  $  and type or copy just the command  wget https://github.com/jgm/pandoc/releases/download/1.19.2.1/pandoc-1.19.2.1-1-amd64.deb    Type  $ wget https://github.com/jgm/pandoc/releases/download/1.19.2.1/pandoc-1.19.2.1-1-amd64.deb  This wget command gets a copy of the Pandoc program that will work in DH Box. The .deb file extension tells us that this is a Linux file. The next step is to unpack that file.    Type  $ sudo dpkg -i pandoc-1.19.2.1-1-amd64.deb  Some commands can have nasty side effects, and the operating system won't let you do them unless you specify that you  really  want to do them. That's what the  sudo command  achieves. The next part, dpkg with the  -i  'flag' is a 'package manager' for installing software. The last part is the file that we used wget to copy to our own machine.     Type  $ pandoc -v  to test that Pandoc is installed  If all has gone well, DH Box will tell you what version of Pandoc you have installed, who it was built by, and a copyright notice.    Type  $ history  This creates a file to keep a record of what commands we have been typing.  DH Box returns a list of every command that you've typed.     Type  $ history   dhbox-work-today.md  This will  pipe  that information into a new file.  When you hit enter, the computer seems to pause for a moment, and then it shows you the command prompt again. How do we know if anything happened? Generally, when working at the command prompt, no news is good news. There are two ways you can check to see if the new file  dhbox-work-today.md  was created.  You can click on the File Manager  (the first folder will have your username; click on that, and then you'll see a list of files) and there it is!  Click on the file name,  and it will download to your computer where you can open it with a text editor.     Alterntively, type  $ ls  This  lists    ls   all the files in the directory. How do you know what directory you're in? You can use the  pwd  command, which prints the working directory.  Let's take a look inside that new file you created. There is a text editor that you can use, called  Nano .     Type  $ nano dhbox-work-today.md  This opens the text editor, and you can see that the history command has copied its output into the editor.  If you got an error:  Sometimes, tools or commands we want to use are not present in the system. DH Box uses the  Ubuntu  operating system (an operating system is the underlying code that makes a PC a PC, a Mac a Mac, etc). We can install all sorts of useful things by using the  apt-get  command.  To get and install Nano, try re-installing Nano with the following command:  $ sudo apt-get install nano .  The computer makes things a little more difficult sometimes when you're asking it to do something that changes or adds capabilities. Simply typing  apt-get  wouldn't work.  sudo  tells the computer that I  really  do want to execute the command (it stands for 'super-user do'). Any  sudo  command will make the computer ask for your password to confirm that you really do want to run that command.  Read the beginner guide for the Nano text editor on How to Geek now .    In Nano, with your file open, make a header (remember to use  #  to indicate a header) at the start of the file which has today's date in it, and add some text explaining what you're trying to do with this exercise.     Hit ctrl+x to exit Nano.    Nano will ask you if you want to 'Save modified buffer?'.  Hit  y , then when it asks you for the file name, hit enter.  The entire key sequence for saving and exiting Nano is  ctrl-x, y, enter .       Use the File Manager to save a copy of  dhbox-work-today.md  onto your own computer.  One last thing: let's convert the Markdown file into both Word and HTML. Pandoc is capable of quite sophisticated transformations, but these are two of the easiest.     Type  $ pandoc -o todayscommands.docx dhbox-work-today.md  This says to Pandoc, create an output file ( the  -o ) called  todayscommands.docx  from  dhbox-work-today.md .     Type  $ ls  after running this command to see if you've made the file.   Any guesses how to create an HTML file? Pandoc is smart enough to know the kind of output you want from the file extension.    Retype the command but use .html instead of .docx this time. Use the file manager to save copies of the .docx and .html files to your own machine. (Incidentally, if you use the arrow up and arrow down keys on your keyboard when you're at the command line, you can page through commands that you've previously typed).       You've done some interesting work   you've installed software into a remote computer, you've copied all of the commands you typed into a new file, you've used Markdown and a text editor to add information and context to those commands, and you've used Pandoc to transform your basic text into the more complicated formats of Word or HTML.  As you do more work in this workbook, I want you to get in the habit of keeping these copies of what you've done. These are your actual lab notes, and they are invaluable for helping you keep track of what you've been doing and what you've been trying. If you remember the course manual, keeping a lab notebook is part of the assessment of the course.  In the next exercise, we learn to use another piece of software called 'git' which will enable you to set up a remote location for pushing copies of your work to, for safe keeping and for collaboration. For more on why we go to all this trouble, visit the Programming Historian on  'Preserving Your Research Data' .  Visit  Ubuntu's handy cheat-sheet of keyboard shortcuts and other useful commands  for your DH Box command-line work.", 
            "title": "Using the Command Line"
        }, 
        {
            "location": "/module-1/Exercises/#exercise-3-setting-up-your-github-space", 
            "text": "It's a familiar situation   you've been working on a paper. It's where you want it to be, and you're certain you're done. You save it as  final.doc . Then, you ask your friend to take a look at it. She spots several typos and that you flubbed an entire paragraph. You open it up, make the changes, and save as  final-w-changes.doc . Later that day it occurs to you that you don't like those changes, and you go back to the original  final.doc , make some changes, and just overwrite the previous version.   Soon, you have a folder resembling the following:  |-project\n    |-'finalfinal.doc'\n    |-'final-w-changes.doc'\n    |-'final-w-changes2.doc'\n    |-'isthisone-changes.doc'\n    |-'this.doc'  Things can get messy quite quickly. Imagine that you also have several spreadsheets in there as well, images, snippets of code... we don't want this. What we want is a way of managing the evolution of your files. We do this with a program called  git . Git is not a user-friendly piece of software, and it takes some work to get your head around. Git is also very powerful, but fortunately, the basic uses to which most of us put it to are more or less straightforward. There are many other programs that make use of git for version control; these programs weld a graphical user interface on top of the main git program. For now, we'll content ourselves with the  GitHub website , which does the same thing more or less, but from a browser.  Firstly, let's define some terms.   git  is a program that keeps snapshots of the contents of a designated folder; it watches for 'difs' or differences between one snapshot and the next. You 'commit' these snapshots to a repository, which lives on your computer (it's just a folder).  GitHub  is a webservice that allows you to share those repositories, and to keep track of those  versions  online, and what's more, to share the repositories and files with multiple collaborators, and  keep everyone's changes straight!  There are other services that work like GitHub, such as  Bitbucket . GitHub is just one of the better known services. You can browse GitHub repositories for code that solves a particular problem for you, software, and data.   NB Because we are going to use the free version, we cannot make the repositories we create private.  In this exercise, we're going to create a repository via the GitHub website and use it as a kind of back-up space for the files you created in the previous exercise. In the follow up exercise, you will learn how to do this from the command line.    Go to  GitHub  and sign up for an account. Remember, you don't have to use your real name. If you use a pseudonym, please communicate to me privately what your account is called.    Once you're logged in, we will create a new repository called  hist3814o . Click on the  +  at the top right of the screen, beside your avatar image.    Write a short description in the 'description box', and tick off the 'initialize the repository with a readme'. You can also select a license from the drop down box   this will put some standard wording on your repository page about the conditions under which someone else might use (or cite) your code.    Click create repository.  At this point, you now have a folder   a repository   on the GitHub website into which you can deposit your files. It will be at  http://github.com/your-account-name/hist3814o . So let's put some materials into that repository.  Notice, when you're on your repository's page, that there is a button to 'create new file' and another for 'upload files'.    Click on upload files.    Drag the HTML file you created in the previous exercise (the one you made with Pandoc, and then saved to your computer via the DH Box File Manager) into the large grey box. This will upload the file. You can drag multiple files into the box to upload them into your repository.     Enter a brief commit message in the commit message box. Then hit the green commit changes button. Git   and GitHub   attach messages to any 'commits' you make. These messages are brief notes explaining why you were making the commit. This way, if you ever had to roll back (go back to an earlier version) you can understand the evolution of the repository and find the spot you want.        Instead of creating multiple versions of a file, you have a single file that has a version history. Neat, eh?  This is perhaps the simplest use case for GitHub. You can create files directly in the repository as well, by hitting the 'create new file' button, and following the prompts. GitHub has a  brief tutorial on using the website to collaborate with other people on a repository .  For the remainder of the course, use your hist3814o repository as your scratch pad, your fail log, and your open notebook for showing your work across these modules  Check out  my example 'fail log' as a model . 'Fail' is a pretty harsh word   I use it to point out that for everything that works perfectly, there's an awful lot of trial-and-error that happened first  upon which  our successes are built. We need to keep track of this! James Baker calls this,  de-wizardification .  Some useful vocabulary when discussing git, GitHub, and version control:   repository  a single folder that holds all of the files and subfolders of your project.  commit  this means, 'take a snapshot of the current state of my repository'.  publish  take a folder on my computer, and copy it and its contents to the web as a repository at  github.com/myusername/repositoryname .  sync  update the web repository with the latest commit from the folder on my computer.  branch  make a copy of my repository with a 'working name'.  merge  fold the changes I have made on a branch into another branch (typically, either  master  or  gh-pages ).  fork  to make a copy of someone else's repo.  clone  to copy a repo online onto your own computer.  pull  request to ask the original maker of a repo to 'pull' your changes into their master, original, repository.  push  to move your changes from your computer to the online repo.", 
            "title": "Exercise 3: Setting up your GitHub space"
        }, 
        {
            "location": "/module-1/Exercises/#an-aside", 
            "text": "Many websites   including this workbook   use a GitHub repository as a way of hosting a website. The video below by historian Jack Dougherty shows how this could be done. Note that the HTML code that he pastes into an  index.html  file (the first page of any website is usually called  index.html ) he got from a different service. You could write a document in Markdown, then use Pandoc to convert that into an  index.html , for example.", 
            "title": "An aside"
        }, 
        {
            "location": "/module-1/Exercises/#exercise-4-a-detailed-look-at-using-git-on-the-command-line", 
            "text": "At its heart, git is a way of taking 'snapshots' of the current state of a folder, and saving those snapshots in sequence. (For an excellent brief presentation on git, visit  Alice Bartlett's presentation on git ; Bartlett is a senior developer for the Financial Times). In git's lingo, as stated earlier, a folder on your computer is known as a repository or repo. This sequence of snapshots in total lets you see how your project unfolded over time. Each time you wish to take a snapshot, you make a  commit . A commit is a git command to take a snapshot of the entire repository. Thus, your folder we discussed above, with its proliferation of documents becomes:  |-project\n    |-'final.doc'  But  its commit history could be visualized like a string of pearls, where each pearl is a unique commit. Each one of those pearls represents a point in time when you the writer made a commit; git compared the state of the file to the earlier state, and saved a snapshot of the  differences . What is particularly useful about making a commit is that git requires two more pieces of information about the git: who is making it, and when. The final useful bit about a commit is that you can save a detailed message about  why  the commit is being made. In our hypothetical situation, your first commit message might look like this:  Fixed conclusion\n\nJulie pointed out that I had missed\nthe critical bit in the assignment\nregarding stratigraphy. This was\nadded in the concluding section.  This information is stored in the history of the commits. In this way, you can see exactly how the project evolved and why. Each one of these commits has what is called a  hash . This is a unique fingerprint that you can use to 'time travel' (in Bartlett's felicitous phrasing). If you want to see what your project looked like a few months ago, you  checkout  that commit. This has the effect of 'rewinding' the project. Once you've checked out a commit, don't be alarmed when you look at the folder: your folder (your repository) looks like how it once did all those weeks ago! Any files written after that commit seem as if they've disappeared. Don't worry, they still exist!  What would happen if you wanted to experiment or take your project in a new direction from that point forward? Git lets you do this. What you will do is create a new  branch  of your project from that point. You can think of a branch as like the branch of a tree, or perhaps better, a branch of a river that eventually merges back to the source. (Another way of thinking about branches is that it is a label that sticks with these particular commits.)   It is generally considered  best practice  to leave your  master  branch alone, in the sense that it represents the best version of your project. When you want to experiment or do something new, you create a  branch  and work there. If the work on the branch ultimately proves fruitless, you can discard it.  But , if you decide that you like how it's going, you can  merge  that branch back into your master. A merge is a commit that folds all of the commits from the branch with the commits from the master.  Git is also a powerful tool for backing up your work. You can work quite happily with git on your own machine, but when you store those files and the history of commits somewhere remote, you open up the possibility of collaboration  and  a safe place where your materials can be recalled if   perish the thought   something happened to your computer. In git-speak, the remote location is, well, the  remote . There are many different places on the web that can function as a remote for git repositories. You can even set one up on your own server, if you want. To get material  out  of GitHub and onto your own computer, you  clone  it. If that hypothetical paper you were writing was part of a group project, your partners could clone it from your GitHub space, and work on it as well!  Let us imagine a scenario.... You and Anna are working together on the project. You have made a new project repository in your GitHub space, and you have cloned it to your computer. Anna has cloned it to hers. Let's assume that you have a very productive weekend and you make some real headway on the project. You  commit  your changes, and then  push  them from your computer to the GitHub version of your repository. That repository is now one commit  ahead  of Anna's version. Anna  pulls  those changes from GitHub to her own version of the repository, which now looks  exactly  like your version. What happens if you make changes to the exact same part of the exact same file? This is called a  conflict . Git will make a version of the file that contains text clearly marking off the part of the file where the conflict occurs, with the conflicting information marked out as well. The way to  resolve  the conflict is to open the file (typically with a text editor) and to delete the added git text, making a decision on which information is the correct information.  Caution  what follows might take a bit of time. It walks you through setting up a git repository in your DH Box; making changes to it; making different branches; and publishing the repository to your space on GitHub's website.", 
            "title": "Exercise 4: A detailed look at using git on the command line"
        }, 
        {
            "location": "/module-1/Exercises/#41-git-init", 
            "text": "How do you turn a folder into a repository? With the  git init  command. At the command line (remember, the  $  just shows you the prompt; you don't have to type it!):   Type  $ mkdir first-repo  to make a new directory.  Type  $ ls  (list) to see that the directory exists. Then change directory into it:  cd first-repo . (Remember: if you're ever not sure what directory you're in, type  $ pwd , or print working directory.)  Make a new file called  readme.md . You do this by calling the text editor:  nano readme.md .   Type an explanation of what this exercise is about.   Hit ctrl+x to exit, then type y to save, leave the file name as it is. Hit enter.  If you get an error to the effect that Nano is not found  you just need to install it with  $ sudo apt-get install nano . DH Box will ask you for your password again. Once the dust settles, you can make the new file with  $ nano readme.md .  Type  $ ls  again to check that the file is there.  Type  $ git init  to tell the git program that this folder is to be tracked as a repository. If all goes correctly, you should see a variation on this message:  Initialized empty Git repository in /home/demonstration/first-repo/.git/ .   Type  $ ls  again. What do you (not) see?       The changes in your repo will now be stored in that  hidden  directory,  .git . Most of the time, you will never have reason to search that folder out. But know that the config file that describes your repo is in that folder. There might come a time in the future where you want to alter some of the default behaviour of the git program. You do that by opening the config file (which you can read with a text editor). Google 'show hidden files and folders' for your operating system when that time comes.", 
            "title": "4.1. git init"
        }, 
        {
            "location": "/module-1/Exercises/#42-git-status", 
            "text": "Open your readme.md file again with the Nano text editor, from the command line. Add some more information to it, then save and exit the text editor.    Type  $ git status  Git will respond with a couple of pieces of information. It will tell you which  branch  you are on. It will list any untracked files present or new changes that are unstaged.     We now will  stage  those changes to be added to our commit history by typing  $ git add -A . (the bit that says  -A  adds any new, modified, or deleted files to your commit when you make it. There are  other options or flags  where you add  only  the new and modified files,  or  only the modified and deleted files.)    Let's check our git status again: type  $ git status    You should see something like this:  On branch master\nInitial commit\nChanges to be committed:\n  (use \"git rm --cached  file ...\" to unstage)\n        new file:   readme.md    Let's take a snapshot: type  $ git commit -m \"My first commit\" . This command represents a bit of a shortcut for making commit messages by using the  -m  flag to associate the text in the quotation marks with the commit.   What happened? Remember, git keeps track not only of the changes, but  who  is making them. If this is your first time working with git in the DH Box, git will ask you for your name and email.     Helpfully, the Git error message tells you exactly what to do: type  $ git config --global user.email \"you\\@example.com\"  and then type  $ git config --global user.name \"Your Name\" . Now try making your first commit.    Open up your  readme.md  file again, and add some more text to it.     Save  readme.md  and exit the text editor.     Add the new changes to the snapshot that we will take.    Type  $ git commit . Git automatically opens up the text editor so you can type a longer, more substantive commit message. In this message (unlike in Markdown) the  #  indicates a line to be ignored. You'll see that there is already some default text in there telling you what to do.     Type a message indicating the nature of the changes you have made.     Save and exit the text editor.  DO NOT  change the filename!       Congratulations, you are now able to track your changes, and keep your materials under version control!", 
            "title": "4.2. git status"
        }, 
        {
            "location": "/module-1/Exercises/#43-git-merge", 
            "text": "Go ahead and make some more changes to your repository. Add some new files. Commit your changes after each new file is created.   Now we're going to view the history of your commits.     Type  $ git log . What do you notice about this list of changes? Look at the time stamps. You'll see that the entries are listed in reverse chronological order. Each entry has its own 'hash' or unique ID, the person who made the commit and time are listed, as well as the commit message eg:  commit 253506bc23070753c123accbe7c495af0e8b5a43\nAuthor: Shawn Graham  shawn.graham@carleton.ca \nDate:   Tue Feb 14 18:42:31 2017 +0000\n\nFixed the headings that were broken in the about section of readme.md    We're going to go back in time and create a new branch. You can escape the  git log  by typing  q . Here's how the command will look:  $ git checkout -b branchname  commit  where  branch  is the name you want the branch to be called, and  commit  is that unique ID. Make a new branch from your second last commit (don't use   or  ).   We typed  git checkout -b experiment 253506bc23070753c123accbe7c495af0e8b5a43 . The response:  Switched to a new branch 'experiment'    Check git status and then list the contents of your repository. What do you see? You should notice that some of the files you had created before seem to have disappeared   congratulations, you've time travelled! Those files are not missing; but they  are  on a different branch (the master branch) and you can't harm them now.   Add a number of new files, making commits after each one.    Check your git status, and check your git log as you go to make sure you're getting everything. Make sure there are no unstaged changes   everything's been committed.", 
            "title": "4.3. git merge"
        }, 
        {
            "location": "/module-1/Exercises/#44-git-merge-continued", 
            "text": "Now let's assume that your  experiment  branch was successful   everything you did there you were happy with and you want to integrate all of those changes back into your  master  branch. We're going to merge things. To merge, we have to go back to the master branch:  $ git checkout master . (Good practice is to keep separate branches for all major experiments or directions you go. In case you lose track of the names of the branches you've created, this command:  git branch -va  will list them for you.)   Now, we merge with  $ git merge experiment . Remember, a merge is a special kind of commit that rolls all previous commits from both branches into one   git will open your text editor and prompt you to add a message (it will have a default message already there if you want it).    Save and exit and ta da! Your changes have been merged together.", 
            "title": "4.4. git merge continued"
        }, 
        {
            "location": "/module-1/Exercises/#45-git-push", 
            "text": "One of the most powerful aspects of using git is the possibility of using it to manage collaborations. To do this, we have to make a copy of your repository available to others as a  remote . There are a variety of places on the web where this can be done; one of the most popular at the moment is  GitHub . GitHub allows a user to have an unlimited number of public repositories.  Public  repositories can be viewed and copied by anyone.  Private  repositories require a paid account, and access is controlled. If you are working on sensitive materials that can only be shared amongst the collaborators on a project, you should invest in an upgraded account (note that you can also control which files get included in commit;  visit GitHub for help on ignoring files . In essence, you simply list the file names you do not want committed;  visit GitHub for an example on listing ignored files ).  Let's assume that your materials are not sensitive .    Login to GitHub.    On the upper right part of the screen there is a large + sign. Click on that, and select  new repository .    On the following screen, give your repo a name.    Leave the repository set to 'Public'    DO NOT  'initialize this repo with a readme.md'. Leave  add .gitignore  and  add license  set to NONE.    Click the green 'Create Repository' button. You now have a space into which you will publish the repository on your machine.     At the command line, we now need to tell git the location of this space. We do that with the following command, where you will change  your-username  and  your-new-repo  appropriately:  $ git remote add origin https://github.com/YOUR-USERNAME/YOUR-NEW-REPO.git    Now we push your local copy of the repository onto the web, to the GitHub version of your repo:  $ git push -u origin master  NB If you wanted to push a  branch  to your repository on the web instead, do you see how you would do that? If your branch was called  experiment , the command would look like this:  $ git push origin experiment  The changes can sometimes take a few minutes to show up on the website. Now, the next time you make changes to this repository, you can push them to your GitHub account   which is the 'origin' in the command above.      Add a new text file. Commit the changes. Push the changes to your account.", 
            "title": "4.5 git push"
        }, 
        {
            "location": "/module-1/Exercises/#46-git-clone", 
            "text": "Imagine you are collaborating with one of your classmates. Your classmate is in charge of the project, and is keeping track of the 'official' folder of materials (i.e. the repo). You wish to make some changes to the files in that repository. You can manage that collaboration via GitHub by making a copy, what GitHub calls a  fork .    Make sure you're logged into your GitHub account on the GitHub website. We're going to fork an example repository right now by going to  GitHub's forking example page .     Click the 'fork' button at top-right. GitHub now makes a copy of the repository in your own GitHub account!    To make a copy of that repository on your own machine, you will now clone it with the  git clone  command. (Remember: a 'fork' copies someone's GitHub repo into a repo in your OWN GitHub account; a 'clone' makes a copy on your own MACHINE). Type:  $ cd..\n$ pwd    We do that to make sure you're not  inside  any other repo you've made! Make sure you're not inside the repository we used in Exercises 1 to 4, then proceed:  $ git clone https://github.com/YOUR-USERNAME/Spoon-Knife.git\n$ ls  You now have a folder called 'Spoon-Knife' on your machine! Any changes you make inside that folder can be tracked with commits. You can also  git push -u origin master  when you're inside it, and the changes will show up on your OWN copy (your fork) on  github.com .       Make a fork of, and then clone, one of your classmates' repositories. Create a new branch. Add a new file to the repository on your machine, and then push it to your fork on GitHub. Remember, your new file will appear on the new branch you created, NOT the master branch.", 
            "title": "4.6. git clone"
        }, 
        {
            "location": "/module-1/Exercises/#47-pull-request", 
            "text": "Now, you let your collaborator know that you've made a change that you want her to  merge  into the original repository. You do this by issuing a  pull request . But first, we have to tell git to keep an eye on that original repository, which we will call  upstream . You do this by adding that repository's location like so:    Type (but change the address appropriately):  $ git remote add upstream THE-FULL-URL-TO-THEIR-REPO-ENDING-WITH-.git    You can keep your version of the remote up-to-date by fetching any new changes your classmate has done:  $ git fetch upstream  Now let's make a  pull  request (you might want to  bookmark GitHub's help document for pull requests ).     Go to your copy of your classmate's repository at your GitHub account. Make sure you've selected the correct branch you pushed your changes to, by selecting it from the Branches menu drop down list.    Click the 'new pull request' button.    The new page that appears can be confusing, but it is trying to double check with you which changes you want to make, and where. Make sure these are set properly.    Base branch  is the branch where you want your changes to go (ie. your classmate's repository).     Head branch  is the branch where you made  your  changes.     Remember: the  Base  branch is the  TO , the  Head  branch is the  FROM    the place where you want your changes to go  TO ,  FROM  the place where you made the changes.   For example, say I clone Dr. Graham's  R  repository and create a new  experiment  branch:    If make changes to my  experiment  branch   that is the  Head  branch.   Then I push those changes to Dr. Graham's  master  branch   that is the  Base  branch.        A pull request has to have a message attached to it, so that your classmate knows what kind of change you're proposing. Fill in the message fields appropriately, then hit the 'create pull request' button.", 
            "title": "4.7. pull request"
        }, 
        {
            "location": "/module-1/Exercises/#48-git-merge-again", 
            "text": "Finally, the last bit of work to be done is to accept the pull request and  merge  the changes into the original repository.   Go to your repository on your GitHub account. Check to see if there are any 'pull requests'   these will be listed under the 'pull requests' tab. Click on that tab.  You can merge from the command line, but for now, you can simply click on the green 'merge pull request' button, and then the 'confirm merge' button. The changes your classmate has made have now been folded into your repository.  To get the updates on your local machine, go back to the command line and type   $ git pull origin master    Phew. You might want to do  $ history   recentcommands.md  just to remember what you've done. And then commit that to a repository.", 
            "title": "4.8. git merge again"
        }, 
        {
            "location": "/module-1/Exercises/#conclusion", 
            "text": "In the modules to come, we will be using DH Box as we find data, fetch data, wrangle data, analyze data, and visualize data. It becomes very important that you note the kinds of commands you use or try, the thinking that you were doing at that point, and so on. You want to leave yourself (and anybody who comes after) breadcrumbs so that you understand what you were doing. Quick notes written in Markdown, piping the history of what you've done to a file, and keeping those files in a repository alongside any other code or files that you may make will set you on the path to open access research and computational reproducibility. As Martha might say, 'and that's a good thing'.", 
            "title": "Conclusion"
        }, 
        {
            "location": "/module-2/Finding Data/", 
            "text": "How do we find data, anyway? July 17 - 23\n\n\nConcepts\n\n\n\n\nSomething given.\n That's a nice way of thinking about it. Of course, much of the data that we are 'given' wasn't really given \nwillingly\n. When we \ntopic model Martha Ballard's diary\n, did she \ngive\n this to us? Of course, she couldn't have imagined what we might try to do to it. Other kinds of data - census data, for instance - were compelled: folks had to answer the questions, on pain of punishment. This is all to suggest that there is a moral dimension to what we do with big data in history. Stop for a moment and read \nThe Joys of Big Data\n (if you haven't already) and then \nThe Third Wave of Computational History\n.\n\n\nDigitized data is not value-neutral; we need to think about, and talk about, what it means to collect, transform, analyze and visualize it. Who has the power here? (and you might also reflect on \n'the most profitable obsolete technology'\n ) Finally, you might also think about recent history - listen to Ian Milligan \ndiscuss how Yahoo's closure of Geocities represented a terrible blow to social history\n.\n\n\nAccepting that historical 'big' data is out there, that there's more material than one person can usefully digest and understand, and that a big-picture, macroscopic point of view is a useful perspective, means also thinking about the digital milieu that makes this possible. But see this piece by Tim Sherratt on \nSeams and edges: Dreams of aggregation, access \n discovery in a broken world\n. We interact with the data we find, and in the process, we alter both it and ourselves! As you do your projects and work through this workbook, think about the ethical, moral, and legal dimensions to what you are doing. \nAlways keep track of your thoughts in your notebook. Remember to put them up in your GitHub repo.\n\n\nFinding big data\n\n\nSo how can we find big data? The exercises in this module will teach you how some historical materials get online, and the work involved in doing that. They will show you how to use wget on the command line to grab webpages; and they will introduce you to the concept of APIs and what you might achieve with them as a historian. Additional exercises show you how to use some existing free and commercial tools for webscraping; and we will also learn how to grab social media data as well.\n\n\nFor future references consult \nthis list of historical data sources\n. You should also perhaps dip into the \n'Data Fundamentals'\n part of \nData + Design\n; think of it as another excellent textbook to help you when you need another perspective on the materials in this course.\n\n\nAnd don't forget serendipity\n\n\nFollow researchers and institutions in your field of study. Once on Twitter I saw something that struck me as an excellent find. Penn Libraries tweeted, and I retweeted, \na link to a traveller's diary from the 19th century\n - a woman who sailed from the US to Europe and thence the Nile, which she ascended and explored. \nMy tweet\n led to a flurry of activity amongst scholars, and even now, the transcription has begun. Indeed, I made an Android-only \ngame out of it\n.\n\n\nBut first... let's set a bit of framework.\n\n\nIf we're going to find data, we need to be able to access the power of our machines, to get them to do what we want. It's worth thinking about what Corey Doctorow has called \nthe war on general purpose computing\n as we begin...\n\n\n...and then thinking about what 'search' actually means. Check out Ted Underwood's piece on \n'Theorizing Research Practices We Forgot to Theorize Twenty Years Ago'\n.\n\n\nFinally, Cameron Blevins has some thoughts on the \n'perpetual sunrise of methodology'\n.\n\n\nWhat you need to do this week\n\n\n\n\nRespond to the readings and the reading questions through annotation (taking care to respond to others' annotations as well) - see the instructions below.  Remember to tag your annotations with 'hist3814o' so that we can find them \non the course Hypothes.is group\n\n\nDo the exercises for this module, pushing yourself as far as you can. Annotate the instructions where they might be unclear or confusing; see if others have annotated them as well, and respond to them with help if you can. Keep an eye on our Slack channel - you can always offer help or seek out help there. Note that one of the exercises shows you how to get the data that you will need for your final project! Write a blog post describing what happened as you went through the exercises (your successes, your failures, the help you may have found/received), and link to your 'faillog' (ie, the notes you upload to your GitHub account - for more on that, see the exercises!).\n\n\nSubmit your work to the course submission form\n\n\n\n\nReadings\n\n\nThis week, I want you to choose just \none\n of the articles linked to above to do your 'official' annotations on OR annotate one of these two articles regarding the 'Transcribing Bentham' project:\n\n\n\n\nCauser \n Wallace, \nBuilding A Volunteer Community: Results and Findings from Transcribe Bentham\n DHQ 6.2, 2012\n\n\nCauser, Tonra, \n Wallace \nTranscription maximized; expense minimized? Crowdsourcing and editing The Collected Works of Jeremy Bentham\n LLC 27.2, 2012\n\n\n\n\nAgain, I also want you to respond to at least one substantive annotation made by your peers. Remember, with Hypothes.is you can annotate pdfs that you have opened in your browser from a website.\n\n\nReading questions\n: Have you ever sat down with one of the librarians to get help finding something? Consider the knowledge and labour involved not just with finding materials, but in making materials findable in the first place. Make an entry in your blog that reflects on these questions in the light of your annotations.", 
            "title": "How do we find data?"
        }, 
        {
            "location": "/module-2/Finding Data/#how-do-we-find-data-anyway-july-17-23", 
            "text": "", 
            "title": "How do we find data, anyway? July 17 - 23"
        }, 
        {
            "location": "/module-2/Finding Data/#concepts", 
            "text": "Something given.  That's a nice way of thinking about it. Of course, much of the data that we are 'given' wasn't really given  willingly . When we  topic model Martha Ballard's diary , did she  give  this to us? Of course, she couldn't have imagined what we might try to do to it. Other kinds of data - census data, for instance - were compelled: folks had to answer the questions, on pain of punishment. This is all to suggest that there is a moral dimension to what we do with big data in history. Stop for a moment and read  The Joys of Big Data  (if you haven't already) and then  The Third Wave of Computational History .  Digitized data is not value-neutral; we need to think about, and talk about, what it means to collect, transform, analyze and visualize it. Who has the power here? (and you might also reflect on  'the most profitable obsolete technology'  ) Finally, you might also think about recent history - listen to Ian Milligan  discuss how Yahoo's closure of Geocities represented a terrible blow to social history .  Accepting that historical 'big' data is out there, that there's more material than one person can usefully digest and understand, and that a big-picture, macroscopic point of view is a useful perspective, means also thinking about the digital milieu that makes this possible. But see this piece by Tim Sherratt on  Seams and edges: Dreams of aggregation, access   discovery in a broken world . We interact with the data we find, and in the process, we alter both it and ourselves! As you do your projects and work through this workbook, think about the ethical, moral, and legal dimensions to what you are doing.  Always keep track of your thoughts in your notebook. Remember to put them up in your GitHub repo.", 
            "title": "Concepts"
        }, 
        {
            "location": "/module-2/Finding Data/#finding-big-data", 
            "text": "So how can we find big data? The exercises in this module will teach you how some historical materials get online, and the work involved in doing that. They will show you how to use wget on the command line to grab webpages; and they will introduce you to the concept of APIs and what you might achieve with them as a historian. Additional exercises show you how to use some existing free and commercial tools for webscraping; and we will also learn how to grab social media data as well.  For future references consult  this list of historical data sources . You should also perhaps dip into the  'Data Fundamentals'  part of  Data + Design ; think of it as another excellent textbook to help you when you need another perspective on the materials in this course.", 
            "title": "Finding big data"
        }, 
        {
            "location": "/module-2/Finding Data/#and-dont-forget-serendipity", 
            "text": "Follow researchers and institutions in your field of study. Once on Twitter I saw something that struck me as an excellent find. Penn Libraries tweeted, and I retweeted,  a link to a traveller's diary from the 19th century  - a woman who sailed from the US to Europe and thence the Nile, which she ascended and explored.  My tweet  led to a flurry of activity amongst scholars, and even now, the transcription has begun. Indeed, I made an Android-only  game out of it .", 
            "title": "And don't forget serendipity"
        }, 
        {
            "location": "/module-2/Finding Data/#but-first-lets-set-a-bit-of-framework", 
            "text": "If we're going to find data, we need to be able to access the power of our machines, to get them to do what we want. It's worth thinking about what Corey Doctorow has called  the war on general purpose computing  as we begin...  ...and then thinking about what 'search' actually means. Check out Ted Underwood's piece on  'Theorizing Research Practices We Forgot to Theorize Twenty Years Ago' .  Finally, Cameron Blevins has some thoughts on the  'perpetual sunrise of methodology' .", 
            "title": "But first... let's set a bit of framework."
        }, 
        {
            "location": "/module-2/Finding Data/#what-you-need-to-do-this-week", 
            "text": "Respond to the readings and the reading questions through annotation (taking care to respond to others' annotations as well) - see the instructions below.  Remember to tag your annotations with 'hist3814o' so that we can find them  on the course Hypothes.is group  Do the exercises for this module, pushing yourself as far as you can. Annotate the instructions where they might be unclear or confusing; see if others have annotated them as well, and respond to them with help if you can. Keep an eye on our Slack channel - you can always offer help or seek out help there. Note that one of the exercises shows you how to get the data that you will need for your final project! Write a blog post describing what happened as you went through the exercises (your successes, your failures, the help you may have found/received), and link to your 'faillog' (ie, the notes you upload to your GitHub account - for more on that, see the exercises!).  Submit your work to the course submission form", 
            "title": "What you need to do this week"
        }, 
        {
            "location": "/module-2/Finding Data/#readings", 
            "text": "This week, I want you to choose just  one  of the articles linked to above to do your 'official' annotations on OR annotate one of these two articles regarding the 'Transcribing Bentham' project:   Causer   Wallace,  Building A Volunteer Community: Results and Findings from Transcribe Bentham  DHQ 6.2, 2012  Causer, Tonra,   Wallace  Transcription maximized; expense minimized? Crowdsourcing and editing The Collected Works of Jeremy Bentham  LLC 27.2, 2012   Again, I also want you to respond to at least one substantive annotation made by your peers. Remember, with Hypothes.is you can annotate pdfs that you have opened in your browser from a website.  Reading questions : Have you ever sat down with one of the librarians to get help finding something? Consider the knowledge and labour involved not just with finding materials, but in making materials findable in the first place. Make an entry in your blog that reflects on these questions in the light of your annotations.", 
            "title": "Readings"
        }, 
        {
            "location": "/module-2/Exercises/", 
            "text": "Module 2 Exercises\n\n\nAll five exercises are on this page. Don't forget to scroll. If you have difficulties, or if the instructions need clarification, please click the 'issues' button and leave a note. Feel free to fork and improve these instructions, if you are so inclined. Remember, these exercises get progressively more difficult, and will require you to either download materials or read materials on other websites. Give yourself plenty of time. Try them all, and remember you can turn to your classmates for help. Work together!\n\n\nDO NOT suffer in silence as you try these exercises! Annotate, ask for help, set up an appointment, or find me in person.\n\n\nBackground\n\n\nWhere do we go to find data? Part of that problem is solved by knowing what question you are asking, and what \nkinds\n of data would help solve that question. Let's assume that you have a pretty good question you want an answer to \n say, something concerning social and household history in early Ottawa, like what was the role of 'corner' stores (if such things exist?) in fostering a sense of neighbourhood \n and begin thinking about how you'd find data to explore that question.\n\n\nThe exercises in this module cover:\n\n\n\n\nThe Dream Case\n\n\nWget\n\n\nWriting a program to extract data from a webpage\n\n\nEncoding transcribed text\n\n\nCollecting data from Twitter\n\n\nCoverting images to text with Tesseract \n\n\n\n\nThere is so much data available; with these methods, we can gather enormous amounts that will let us see large-scale macroscopic patterns. At the same time, it allows us to dive into the details with comparative ease. The thing is, not all digital data are created equally. Google has spent millions digitizing \neverything\n; newspapers have digitized their own collections. Genealogists and local historical societies upload yoinks of digitized photographs, wills, local tax records, \nyou-name-it\n, \nevery day\n. But, consider what Milligan has to say about \n'illusionary order'\n:\n\n\n\n\n[...] poor and misunderstood use of online newspapers can skew historical research. In a conference presentation or a lecture, it\u2019s not uknown to see the familiar yellow highlighting of found searchwords on projected images: indicative of how the original primary material was obtained. But this historical approach generally usually remains unspoken, without a critical methodological reflection. As I hope I\u2019ll show here, using Pages of the Past uncritically for historical research is akin to using a volume of the Canadian Historical Review with 10% or so of the pages ripped out. Historians, journalists, policy researchers, genealogists, and amateur researchers need to at least have a basic understanding of what goes on behind the black box.\n\n\n\n\nAsk yourself: what are some of the key dangers? Reflect: how have you used digitized resources uncritically in the past? Remember: \nTo digitize\n doesn't \n or shouldn't \n mean uploading a photograph of a document. There's a lot more going on that that. We'll get to that in a moment.\n\n\n\n\nExercise 1: The Dream Case\n\n\nIn the dream case, your data are not just images, but are actually sorted and structured into some kind of pre-existing database. There are choices made in the \ncreation\n of the database, but a good database, a good project, will lay out for you their decision making, their corpora, and how they've dealt with ambiguity and so on. You search using a robust interface, and you get a well-formed spreadsheet of data in return. Two examples of 'dream case' data:\n\n\n\n\nEpigraphic Database Heidelberg\n\n\nCommwealth War Graves Commission, Find War Dead\n\n\nExplore both databases. Perform a search of interest to you. In the case of the epigraphic database, if you've done any Latin, try searching for terms related to occupations; or you could search \nFiglina\n. \n\n\nIn the CWGC database, search your own surname. Download your results. You now have data that you can explore! \n\n\nUsing the Nano text editor in your DH Box, make a record (or records) of what you searched, the URL for your search \n its results, and where you're keeping your data. \n\n\nLodge a copy of this record in your repository.\n\n\n\n\n\n\nExercise 2: Wget\n\n\nYou've already encountered wget in the introduction to this workbook, when you were setting up your DH Box to use Pandoc. \n\n\n\n\n\n\nIn this exercise, I want you to do \nIan Milligan's wget tutorial at the Programming Historian\n to learn more about the power of this command, and how to wield that power properly. Skip ahead to step 2, since your DH Box already has wget installed. (If you want to continue to use wget after this course is over, you will have to install it on your own machine, obviously).\n\n\nOnce you've completed Milligan's tutorial, remember to put your history into a new Markdown file, and to lodge a copy of it in your repository.\n\n\nNow that you're \nau fait\n with wget, we will do part of \nKellen Kurschinski's wget tutorial at the Programming Historian\n. I want you to use wget to download the Library and Archives Canada \n14th Canadian General Hospital war diaries\n in a responsible and respectful manner. Otherwise, you will look like a bot attacking their site.\n\n\nThe URLs of this diary go from \nhttp://data2.archives.ca/e/e061/e001518029.jpg\n to \nhttp://data2.archives.ca/e/e061/e001518109.jpg\n. This is a total of 80 pages \n note the last part of the URL goes from \ne001518029\n to \ne001518109\n for a total of 80 images.\n\n\n\n\n\n\nMake a new directory: \n$ mkdir war-diary\n and then cd into it: \n$ cd war-diary\n. Make sure you're in the directory by typing \n$ pwd\n. \n\n\nNB Make sure you are in your parent directory \n~\n. To get there directly from any subdirectory, type \n$ cd ~\n. If you want to check your file structure quickly, go to the File Manager.\n\n\nWe will use a simple Python script to gather all the URLs of the diary images from the Library and Archives. Python is a general purpose programming language.\n\n\n\n\n\n\nType \n$ nano urls.py\n to open a new Python file called \nurls\n.\n\n\n\n\n\n\nPaste the script below. This script grabs the URLs from \ne001518029\n to \ne001518110\n and puts them in a file called \nurls.txt\n: \n\n\nurls = '';\nf=open('urls.txt','w')\nfor x in range(8029, 8110):\n    urls = 'http://data2.collectionscanada.ca/e/e061/e00151%d.jpg\\n' % (x)\n    f.write(urls)\nf.close\n\n\n\n\n\n\n\nHit ctrl+x, Y, enter to save and exit Nano.\n\n\n\n\n\n\nType \n$ python urls.py\n to run the Python script. \n\n\n\n\n\n\nType \n$ ls\n and notice the \nurls.txt\n file.\n\n\n\n\n\n\nType \n$ nano urls.txt\n to examine the file. Exit Nano.\n\n\n\n\n\n\nType \n$ wget -i urls.txt -r --no-parent -nd -w 2 --limit-rate=100k\n to download all the URLs from the \nurls.txt\n file.\n\n\n\n\n\n\nType \n$ ls\n to verify the files were downloaded.\n\n\n\n\n\n\nAdd your command to your history file, and lodge it in your repository. For reference, visit \nModule 1, Exercise 2\n.\n\n\n\n\n\n\n\n\n\n\n\n\nIn \nExercise 6\n, we will open some of the text files in Nano to judge the 'object character recognition'. Part of the point of working with these files is to show that even with horrible 'digitization', we can still extract useful insights. Digitization is more than simply throwing automatically generated files online. Good digitization requires scholarly work and effort! We will learn how to do this properly in the next exercise.\n\n\nThere's no one right way to do things, digitally. There are many paths. The crucial thing is that you find a way that makes sense for your own workflow, and that doesn't make you a drain on someone else's resources.\n\n\n\n\nExercise 3: TEI\n\n\nDigitization requires human intervention. This can be as straightforward as correcting errors or adjusting the scanner settings when we do OCR, or it can be the rather more involved work of adding a layer of semantic information to the text. When we mark up a text with the semantic hooks and signs that explain we are talking about \nLondon, Ontario\n rather than \nLondon, UK\n, we've made the text a whole lot more useful for other scholars or tools. In this exercise, you will do some basic marking up of a text using standards from the \nText Encoding Initiative\n. (Some of the earliest digital history work was along these lines). \n\n\nThe TEI exercise requires carefully attention to detail. Read through it before you try it. In this exercise, you'll transcribe a page from an abolitionist's pamphlet. You'll also think about ways of transforming the resulting XML into other formats. Make notes in a file to upload to your repository, and upload your XML and your XSL file to your own repository as well. (As an added optional challenge, create a gh-pages branch and figure out the direct URL to your XML file, and email that URL to me).\n\n\nFor this exercise, do the following: \n\n\n\n\nThe TEI exercise found \nin our supporting materials\n.\n\n\n\n\nI will note that a perfectly fine final project for HIST3814 might be to use this exercise as a model to markup the war diary and suggest ways this material might be explored. Remember to make (and lodge in your repository) a file detailing the work you've done and any issues you've run into.\n\n\n\n\nExercise 4: APIs\n\n\nSometimes, a website will have what is called an \nApplication Programming Interface\n or API. In essence, this lets a program on your computer talk to the computer serving the website you're interested in, such that the website gives you the data that you're looking for.\n\n\nThat is, instead of \nyou\n punching in the search terms, and copying and pasting the results, you get the computer to do it. More or less. The thing is, the results come back to you in a machine-readable format \n often, JSON, which is a kind of text format. It looks like the following:\n\n\n\n\nThe \nCanadiana Discovery Portal\n has tonnes of materials related to Canada's history, from a wide variety of sources.\n\n\n\n\n\n\nGo to the \nCanadiana Discovery Portal\n, and search \"ottawa\".\n\n\n\n\n\n\nSet the date range to 1800 to 1900 and hit enter. You are presented with a page of results -56 249 results! That's a lot of data. But do you notice the address bar of your browser? It'll say something like this:\n\n\nhttp://search.canadiana.ca/search?q=ottawa\nfield=\ndf=1900\ndt=1900\n\n\n\nYour search query has been put into the URL. You're looking at the API! Everything after \n/search\n is a command that you are sending to the Canadiana server.\n\n\n\n\n\n\nScroll through the results, and you'll see a number just before the question mark \n?\n\n\nhttp://search.canadiana.ca/search/2?df=1800\ndt=1900\nq=ottawa\nfield=\nhttp://search.canadiana.ca/search/3?df=1800\ndt=1900\nq=ottawa\nfield=\nhttp://search.canadiana.ca/search/4?df=1800\ndt=1900\nq=ottawa\nfield=\n\n\n\n....all the way up to 5625 (ie. 10 results per page, so 56249 / 10).\n\n\nIf you go to the \nCanadiana API support page\n you can see the full list of options. What we are particularly interested in now is the bit that says \nfmt=json\n.\n\n\n\n\n\n\nAdd that \nfmt=json\n to your query URL. How different the results now look! What's nice here is that the data is formatted in a way that makes sense to a machine \n which we'll learn more about in due course.\n\n\n\n\n\n\n\n\nIf you look back at the full list of API options, you'll see at the bottom that one of the options is 'retrieving individual item records'; the key for that is a field called \noocihm\n. If you look at your page of JSON results, and scroll through them, you'll see that each individual record has its own oocihm number. If we could get a list of those, we'd be able to programmatically slot them into the commands for retrieving individual item records:\n\n\nhttp://search.canadiana.ca/view/oocihm.16278/?r=0\ns=1\nfmt=json\napi_text=1\n\n\n\nThe problem is: how to retrieve those oocihm numbers. The answer is, 'we write a program'. And the program that you want can be \nfound on Ian Milligan's website\n. Study that program carefully. There are a number of useful things happening in there, notably \ncurl\n, \njq\n, \nsed\n, and \nawk\n. \ncurl\n  is a program for downloading webpages, \njq\n for dealing with JSON, and \nsed\n and \nawk\n for searching within and cleaning up text. If this all sounds Greek to you, there is an excellent gentle introduction over at \nWilliam Turkel's blog\n.\n\n\n\n\n\n\nWe need the command line program jq. We install it into our DH Box with \n$ sudo apt-get install jq -y\n\n\n\n\n\n\nWe need to create a program. Make a new directory for this exercise like so: \n$ mkdir m2e4\n. Then, change into that directory by typing \n$ cd m2e4\n. \n\n\n\n\n\n\nMake sure that's where you are by typing \n$ pwd\n. Now, make an empty file for our program with \n$ touch canadiana.sh\n. Touch makes an empty file; the .sh in the filename indicates that this is a shell script.\n\n\n\n\n\n\nOpen the empty file with \n$ nano canadiana.sh\n. Now, the program that Ian Milligan wrote makes calls to the API that \nused to live\n at eco.canadiana.ca. But note the \nerror message on Canadiana's website\n. So we have to change Milligan's script so that it points to the API at \nsearch.canadiana.ca\n. Copy the script below into your empty \ncanadiana.sh\n. If you want, adjust the search parameters (in the line starting with \npages\n) for material you're more interested in.\n\n\n#! /bin/bash\npages=$(curl 'http://search.canadiana.ca/search?q=ottawa*\nfield=\nso=score\ndf=1800\ndt=1900\nfmt=json' | jq '.pages')\n# this goes into the results and reads the value of 'pages' in each one of them.\n# it then tells us how many pages we're going to have.\necho \"Pages:\"$pages\n# this says 'for each one of these pages, download the 'key' value on each page'\nfor i in $(seq 1 $pages)\ndo\n        curl 'http://search.canadiana.ca/search/'${i}'?q=ottawa*\nfield=\nso=score\ndf=1800\ndt=1900\nfmt=json' | jq '.docs[] | {key}' \n results.txt\ndone\n# the next two lines take the results.txt file that is quite messy and clean it up. I'll try to explain what they all mean. Basically, tr deletes a given character - so we delete quotation marks \"\\\"\" (the slash tells the computer not to treat the quotation mark as a computer code, but as a quotation mark itself), to erase spaces, and to find the phrase \"key:\" and delete it too.\nsed -e 's/\\\"key\\\": \\\"//g' results.txt | tr -d \"\\\"\" | tr -d \"{\" | tr -d \"}\" | tr -s \" \" | sed '/^\\s*$/d' | tr -d ' ' \n cleanlist.txt\n# this adds a prefix and a suffix.\nawk '$0=\"search.canadiana.ca/view/\"$0' cleanlist.txt| awk '{print $0 \"/1?r=0\ns=1\nfmt=json\napi_text=1\"}' \n urlstograb.txt\n# then if we want we can take those URLs and output them all to a big text file for analysis.\nwget -i urlstograb.txt -O output.txt\n\n\n\n\n\n\n\nHit ctrl+x to exit Nano, and save the changes.\n\n\n\n\n\n\n\n\n\n\n\n\nBefore we can run this program, we have to tell DH Box that it is alright to run it. To change the 'permissions' on the file, type \n$ chmod 755 canadiana.sh\n\n\nThe \n$ chmod\n command means change mode. Each number represents a user permission for reading, writing, and executing files on your computer.  \n\n\n\n\n\n\nAnd now we can run the program by typing \n$ ./canadiana.sh\n (the ./ is important!)\n\n\nTa da! You now have a pretty powerful tool now for grabbing data from one of the largest portals for Canadian history! \n\n\n\n\n\n\nDownload your \noutput.txt\n file to your computer via the file manager and have a look at it. \n\n\n\n\n\n\n\n\n\n\n\n\nMake sure to make a file noting what you've done, commands you've made, etc, and upload it in your GitHub repository.\n\n\n\n\nExercise 5: Mining Twitter\n\n\nEd Summers is part of a project called '\nDocumenting the Now\n' which is developing tools to collect and understand the historical materials being shared (and lost) on social media. One component of Documenting the Now is the Twitter Archiving Tool, '\nTwarc\n'. In this exercise, you are going to use Twarc to create an archive of Tweets relevant to a current trending news topic.\n\n\n\n\n\n\nFirst of all, you need to set up a Twitter account, if you haven't already got one. Do so, but make sure to minimize any personal information that is exposed. For instance, don't make your handle the same as your real name. \n\n\n\n\n\n\nTurn off geolocation. Do not give your actual location in the profile. \n\n\n\n\n\n\nView the settings, and make sure all of the privacy settings are dialed down. For the time being, you \ndo\n have to associate a cell phone number with your account. You can delete that once you've done the next step.\n\n\n\n\n\n\nGo to the \nTwitter apps page\n and click on \nnew app\n. \n\n\n\n\n\n\nOn the \nnew application\n page, just give your app a name like \nmy-twarc\n or similar. For website, use the \nCrafting Digital History site URL\n (although for our purposes any website will do). You don\u2019t need to fill in any of the rest of the fields. \n\n\n\n\n\n\nContinue on to the next page (tick off the box saying you\u2019ve read the developer code of behaviour). This next page shows you all the details about your new application.\n\n\n\n\n\n\nClick on the \u2018Keys and Access Tokens\u2019 tab. \n\n\n\n\n\n\nCopy the consumer key, the consumer secret to a text file.\n\n\n\n\n\n\nClick on the \u2018create access tokens\u2019 button at the bottom of the page. This generates an access token and an access secret. \n\n\n\n\n\n\nCopy those to your text file, save it. \nDo not put this file in your repo or leave it online anywhere\n \n\n\n\n\n\n\n\n\nWe need to download an older version of Twarc to work with the DH Box. In your DH Box, at the command line, type the following:\n\n\n$ sudo pip install https://github.com/DocNow/twarc/archive/v1.2.0.tar.gz\n\n\n\nTwarc is written in Python, which is already installed in DH Box. 'Pip' is a package manager for installing new Python modules and packages. \n\n\n\n\n\n\nNow type \n$ twarc configure\n and give it the information it asks for (your consumer secret etc).\n\n\nYou're now ready to search. For instance, \n$ twarc search canada150 \n search.json\n will search Twitter for posts using the canada150 hashtag. \n\n\nWait! Don't run that command! (Force-stop the search by hitting ctrl+c.)\n \n\n\nIf you search for \ncanada150\n , there are, what, 36 million Canadians? How many tweets is that likely to be? Quite a lot \n and the command will run quietly for days grabbing that information, writing it to file, and you'll be sitting looking at the screen wondering if anything is happening. \n\n\n\n\n\n\n\n\n\n\n\n\nTry something smaller and more contained for now: \n$ twarc search hist3814o \n search.json\n. \n\n\nNote that Twitter only gives access to the last two weeks or so via search.\n For grabbing the stream \nas an event happens\n you'd use the \ntwarc stream\n command \n see the Twarc documentation for more.\n\n\nIt might take some time for the search to happen. \nYou can always force-stop the search by hitting ctrl+c.\n If you do that though there could be an error in the formatting of the file which will throw an error when you get to step 15. You can still open the JSON in a text editor though, but you will have to go to the end of the file and fix the formatting.\n\n\nThe data being collected is in JSON format. That is, a list of 'keys' and 'values'. This is a handy format for computers, and some data visualization platforms require data in this format. For our purposes we might want to transform the JSON into a CSV (comma separated) table \n a spreadsheet.\n\n\n\n\n\n\nType \n$ sudo npm install json2csv --save -g\n. This installs a command that can convert the JSON to CSV format. Full details about the command can be found on \njson2csv's GitHub repository\n.\n\n\n\n\n\n\nConvert your \nsearch.json\n to CSV by typing \njson2csv -i search.json -o out.csv\n\n\n\n\n\n\nExamine your data either in a text editor or in a spreadsheet.\n\n\n\n\n\n\nUse Twarc to create a file with a list of IDs. \n\n\n\n\n\n\nLodge this list and your history and notes in your repository.\n\n\n\n\n\n\n\n\n\n\n\n\nNB Twitter forbids the sharing of the full metadata of a collection of tweets. You may however share a list of tweet IDs. See the Twarc documentation for the instructions on how to do that.\n\n\nWhat can you do with this data?\n\n\n\n\n\n\nExamine the Twarc repository, especially its utilities. You could extract the geolocated ones and map them. You could examine the difference between 'male' and 'female' tweeters (and how problematic might that be?). \n\n\n\n\n\n\nIn your CSV, save the text of the posts to a new file and upload it to something like \nVoyant Tools\n to visualize trends over time. \n\n\n\n\n\n\nGoogle for analysis of Twitter data to get some ideas.\n\n\n\n\n\n\n\n\nExercise 6: Using Tesseract to turn an image into text\n\n\nWe've all used image files like JPGs and PNGs. Images always look the same on whatever machine they are displayed on, because they contain within themselves the complete description of what the 'page' should look like. You're likley familiar with the fact that you cannot select text within an image. When we digitize documens, the image that results only contains the image layer, not the text. \n\n\nTo turn that image into text, we have to do what's called 'object character recognition', or OCR. An OCR algorithm looks at the pattern of pixels in the image, and maps these against the shapes it 'knows' to be an A, or an a, or a B, or a \n, and so on. Cleaner, sharper printing gives better results as do high resolution images free from noise. People who have a lot of material to OCR use some very powerful tools to identify blocks of text within the newspaper page, and then train the machine to identify these, a process beyond us just now (but visit \nthis Tesseract q \n a on stackoverflow\n if you're interested).\n\n\nIn this exercise, you'll:\n\n\n\n\nInstall the Tesseract OCR engine into your DH Box\n\n\nInstall and use ImageMagick to convert the JPG into TIFF image format\n\n\nUse Tesseract to OCR the resulting pages.\n\n\nUse Tesseract in R to OCR the resulting pages.\n\n\nCompare the resulting OCRd texts.\n\n\n\n\nConverting images in the command line\n\n\n\n\n\n\nBegin by making a new director for this exercise: \n$ mkdir ocr-test\n. \n\n\n\n\n\n\nType \n$ cd ocr-test\n to change directories into ocr-test.\n\n\n\n\n\n\nType \n$ sudo apt-get install tesseract-ocr\n to grab the latest version of Tesseract and install it into your DH Box. Enter your password when the computer asks for it.\n\n\n\n\n\n\nType \n$ sudo apt-get install imagemagick\n to install ImageMagick.\n\n\n\n\n\n\nLet's convert the first file to TIFF with ImageMagick's convert command\n\n\n$ convert -density 300 ~/war-diary/e001518087.jpg -depth 8 -strip -background white -alpha off e001518087.tiff\n\n\n\nYou want a high density image, which is what the -density and the -depth flags do; the rest of the command formats the image in a way that Tesseract expects to encounter text. This command might take a while. Just wait, be patient.\n\n\n\n\n\n\nExtract text with \n$ tesseract e001518087.tiff output.txt\n. This might also take some time.\n\n\n\n\n\n\nDownload the \noutput.txt\n file to your own machine via DH Box's filemanager. \n\n\n\n\n\n\nOpen the file with a text editor. \n\n\n\n\n\n\n\n\n\n\n\n\nConverting images in R\n\n\n\n\n\n\nNow we will convert the file using R. Navigate to RStudio in the DH Box.\n\n\n\n\n\n\nOn the upper left side, click the green plus button \n R Script to open a new blank script file.\n\n\n\n\n\n\nPaste in the following script and save it as \nocr\n in RStudio.\n\n\ninstall.packages('magick')\ninstall.packages('magrittr')\ninstall.packages('tesseract')\nlibrary(magick) \nlibrary(magrittr) \nlibrary(tesseract) \ntext \n- image_read(\"~/war-diary/e001518087.jpg\") %\n% \n  image_resize(\"2000\") %\n% \n  image_convert(colorspace = 'gray') %\n% \n  image_trim() %\n% \n  image_ocr()\nwrite.table(text, \"~/ocr-test/R.txt\")\n\n\n\nBefore installing any packages in RStudio, we need to install some dependencies in the command line. The reason is that since DH Box runs an older version of RStudio, not everything installs as planned.\n\n\n\n\n\n\nType \n$ sudo apt-get install libmagick++-dev\n in the command line to install the libmagick library.\n\n\n\n\n\n\nType \n$ sudo apt-get install libtesseract-dev\n in the command line to install the libtesseract library.\n\n\n\n\n\n\nType \n$ sudo apt-get install libleptonica-dev\n in the command line to install the libleptonic library.\n\n\n\n\n\n\nType \n$ sudo apt-get install tesseract-ocr-eng\n in the command line to install the English Tesseract library.\n\n\n\n\n\n\nNavigate to RStudio and run each \ninstall.packages\n line in our script. This will take some time.\n\n\n\n\n\n\nRun each \nlibrary()\n line to load the libraries.\n\n\n\n\n\n\nRun each line up to \nimage_ocr()\n. This may take some time to complete.\n\n\n\n\n\n\nRun the last line \nwrite.table()\n to export the OCR to a text file with the same name.\n\n\n\n\n\n\nNavigate to your file manager and download both the \noutput.txt\n file and the \nR.txt\n file.\n\n\n\n\n\n\nCompare the two text files in your desktop. How is the OCR in the command line versus within R? Note that they both use Tesseract just with different settings and in different environments. \n\n\n\n\n\n\n\n\n\n\n\n\nProgressively converting our files with Tesseract\n\n\n\n\n\n\nNow take a screen shot of both text files (just the text area) and name them \noutput_1.png\n and \nR_1.png\n respectively.\n\n\n\n\n\n\nUpload both files into DH Box via the File Manager.\n\n\n\n\n\n\nIn the command line, type \n$ tesseract output_1.png output_1.txt\n. \n\n\n\n\n\n\nIn RStudio, change the file paths in your script to the following:\n\n\ninstall.packages('magick')\ninstall.packages('magrittr')\ninstall.packages('tesseract')\nlibrary(magick) \nlibrary(magrittr) \nlibrary(tesseract) \ntext \n- image_read(\"~/ocr-test/R_1.png\") %\n% \n  image_resize(\"2000\") %\n% \n  image_convert(colorspace = 'gray') %\n% \n  image_trim() %\n% \n  image_ocr()\nwrite.table(text, \"~/ocr-test/R_1.txt\")\n\n\n\n\n\n\n\nRun each script line again. Except this time \nDO NOT\n run the \ninstall.packages()\n lines since we already installed them. Simply load the libraries again and run each line.\n\n\n\n\n\n\nNavigate to the File Manager and download \nouput_1.txt\n and \nR_1.txt\n.\n\n\n\n\n\n\nCompare these two files. Did the OCR conversion get progressively worse? How do they compare to each other, to the first attempt at conversion, and then to the originals?\n\n\n\n\n\n\n\n\n\n\n\n\nChoose either the command line or the R method to convert more of the war diary files to text. Save these files into a new directory called \nwar-diary-text\n. We will use these text files for future work in topic modeling and text analysis. How might your decision on which method to use change the results you would get in, say, a topic modeling tool? \n\n\n\n\n\n\n\n\n\n\n\n\nThink about how these conversions can change based on the image being run through Tesseract. Does Tesseract have an easier time converting computer text even though it's in an image format? How might OCR conversions affect the way historians work on batch files? How does the context of the text change how historians analyse it? \n\n\nLook up the \nTesseract wiki\n. What other options could you use with the Tesseract command to improve the results? When you decide to download Tesseract to you own computer, use the following two guides to automating bulk OCR (multiple files) with Tesseract: \nPeirson's\n and \nSchmidt's\n.", 
            "title": "Exercises"
        }, 
        {
            "location": "/module-2/Exercises/#module-2-exercises", 
            "text": "All five exercises are on this page. Don't forget to scroll. If you have difficulties, or if the instructions need clarification, please click the 'issues' button and leave a note. Feel free to fork and improve these instructions, if you are so inclined. Remember, these exercises get progressively more difficult, and will require you to either download materials or read materials on other websites. Give yourself plenty of time. Try them all, and remember you can turn to your classmates for help. Work together!  DO NOT suffer in silence as you try these exercises! Annotate, ask for help, set up an appointment, or find me in person.", 
            "title": "Module 2 Exercises"
        }, 
        {
            "location": "/module-2/Exercises/#background", 
            "text": "Where do we go to find data? Part of that problem is solved by knowing what question you are asking, and what  kinds  of data would help solve that question. Let's assume that you have a pretty good question you want an answer to   say, something concerning social and household history in early Ottawa, like what was the role of 'corner' stores (if such things exist?) in fostering a sense of neighbourhood   and begin thinking about how you'd find data to explore that question.  The exercises in this module cover:   The Dream Case  Wget  Writing a program to extract data from a webpage  Encoding transcribed text  Collecting data from Twitter  Coverting images to text with Tesseract    There is so much data available; with these methods, we can gather enormous amounts that will let us see large-scale macroscopic patterns. At the same time, it allows us to dive into the details with comparative ease. The thing is, not all digital data are created equally. Google has spent millions digitizing  everything ; newspapers have digitized their own collections. Genealogists and local historical societies upload yoinks of digitized photographs, wills, local tax records,  you-name-it ,  every day . But, consider what Milligan has to say about  'illusionary order' :   [...] poor and misunderstood use of online newspapers can skew historical research. In a conference presentation or a lecture, it\u2019s not uknown to see the familiar yellow highlighting of found searchwords on projected images: indicative of how the original primary material was obtained. But this historical approach generally usually remains unspoken, without a critical methodological reflection. As I hope I\u2019ll show here, using Pages of the Past uncritically for historical research is akin to using a volume of the Canadian Historical Review with 10% or so of the pages ripped out. Historians, journalists, policy researchers, genealogists, and amateur researchers need to at least have a basic understanding of what goes on behind the black box.   Ask yourself: what are some of the key dangers? Reflect: how have you used digitized resources uncritically in the past? Remember:  To digitize  doesn't   or shouldn't   mean uploading a photograph of a document. There's a lot more going on that that. We'll get to that in a moment.", 
            "title": "Background"
        }, 
        {
            "location": "/module-2/Exercises/#exercise-1-the-dream-case", 
            "text": "In the dream case, your data are not just images, but are actually sorted and structured into some kind of pre-existing database. There are choices made in the  creation  of the database, but a good database, a good project, will lay out for you their decision making, their corpora, and how they've dealt with ambiguity and so on. You search using a robust interface, and you get a well-formed spreadsheet of data in return. Two examples of 'dream case' data:   Epigraphic Database Heidelberg  Commwealth War Graves Commission, Find War Dead  Explore both databases. Perform a search of interest to you. In the case of the epigraphic database, if you've done any Latin, try searching for terms related to occupations; or you could search  Figlina .   In the CWGC database, search your own surname. Download your results. You now have data that you can explore!   Using the Nano text editor in your DH Box, make a record (or records) of what you searched, the URL for your search   its results, and where you're keeping your data.   Lodge a copy of this record in your repository.", 
            "title": "Exercise 1: The Dream Case"
        }, 
        {
            "location": "/module-2/Exercises/#exercise-2-wget", 
            "text": "You've already encountered wget in the introduction to this workbook, when you were setting up your DH Box to use Pandoc.     In this exercise, I want you to do  Ian Milligan's wget tutorial at the Programming Historian  to learn more about the power of this command, and how to wield that power properly. Skip ahead to step 2, since your DH Box already has wget installed. (If you want to continue to use wget after this course is over, you will have to install it on your own machine, obviously).  Once you've completed Milligan's tutorial, remember to put your history into a new Markdown file, and to lodge a copy of it in your repository.  Now that you're  au fait  with wget, we will do part of  Kellen Kurschinski's wget tutorial at the Programming Historian . I want you to use wget to download the Library and Archives Canada  14th Canadian General Hospital war diaries  in a responsible and respectful manner. Otherwise, you will look like a bot attacking their site.  The URLs of this diary go from  http://data2.archives.ca/e/e061/e001518029.jpg  to  http://data2.archives.ca/e/e061/e001518109.jpg . This is a total of 80 pages   note the last part of the URL goes from  e001518029  to  e001518109  for a total of 80 images.    Make a new directory:  $ mkdir war-diary  and then cd into it:  $ cd war-diary . Make sure you're in the directory by typing  $ pwd .   NB Make sure you are in your parent directory  ~ . To get there directly from any subdirectory, type  $ cd ~ . If you want to check your file structure quickly, go to the File Manager.  We will use a simple Python script to gather all the URLs of the diary images from the Library and Archives. Python is a general purpose programming language.    Type  $ nano urls.py  to open a new Python file called  urls .    Paste the script below. This script grabs the URLs from  e001518029  to  e001518110  and puts them in a file called  urls.txt :   urls = '';\nf=open('urls.txt','w')\nfor x in range(8029, 8110):\n    urls = 'http://data2.collectionscanada.ca/e/e061/e00151%d.jpg\\n' % (x)\n    f.write(urls)\nf.close    Hit ctrl+x, Y, enter to save and exit Nano.    Type  $ python urls.py  to run the Python script.     Type  $ ls  and notice the  urls.txt  file.    Type  $ nano urls.txt  to examine the file. Exit Nano.    Type  $ wget -i urls.txt -r --no-parent -nd -w 2 --limit-rate=100k  to download all the URLs from the  urls.txt  file.    Type  $ ls  to verify the files were downloaded.    Add your command to your history file, and lodge it in your repository. For reference, visit  Module 1, Exercise 2 .       In  Exercise 6 , we will open some of the text files in Nano to judge the 'object character recognition'. Part of the point of working with these files is to show that even with horrible 'digitization', we can still extract useful insights. Digitization is more than simply throwing automatically generated files online. Good digitization requires scholarly work and effort! We will learn how to do this properly in the next exercise.  There's no one right way to do things, digitally. There are many paths. The crucial thing is that you find a way that makes sense for your own workflow, and that doesn't make you a drain on someone else's resources.", 
            "title": "Exercise 2: Wget"
        }, 
        {
            "location": "/module-2/Exercises/#exercise-3-tei", 
            "text": "Digitization requires human intervention. This can be as straightforward as correcting errors or adjusting the scanner settings when we do OCR, or it can be the rather more involved work of adding a layer of semantic information to the text. When we mark up a text with the semantic hooks and signs that explain we are talking about  London, Ontario  rather than  London, UK , we've made the text a whole lot more useful for other scholars or tools. In this exercise, you will do some basic marking up of a text using standards from the  Text Encoding Initiative . (Some of the earliest digital history work was along these lines).   The TEI exercise requires carefully attention to detail. Read through it before you try it. In this exercise, you'll transcribe a page from an abolitionist's pamphlet. You'll also think about ways of transforming the resulting XML into other formats. Make notes in a file to upload to your repository, and upload your XML and your XSL file to your own repository as well. (As an added optional challenge, create a gh-pages branch and figure out the direct URL to your XML file, and email that URL to me).  For this exercise, do the following:    The TEI exercise found  in our supporting materials .   I will note that a perfectly fine final project for HIST3814 might be to use this exercise as a model to markup the war diary and suggest ways this material might be explored. Remember to make (and lodge in your repository) a file detailing the work you've done and any issues you've run into.", 
            "title": "Exercise 3: TEI"
        }, 
        {
            "location": "/module-2/Exercises/#exercise-4-apis", 
            "text": "Sometimes, a website will have what is called an  Application Programming Interface  or API. In essence, this lets a program on your computer talk to the computer serving the website you're interested in, such that the website gives you the data that you're looking for.  That is, instead of  you  punching in the search terms, and copying and pasting the results, you get the computer to do it. More or less. The thing is, the results come back to you in a machine-readable format   often, JSON, which is a kind of text format. It looks like the following:   The  Canadiana Discovery Portal  has tonnes of materials related to Canada's history, from a wide variety of sources.    Go to the  Canadiana Discovery Portal , and search \"ottawa\".    Set the date range to 1800 to 1900 and hit enter. You are presented with a page of results -56 249 results! That's a lot of data. But do you notice the address bar of your browser? It'll say something like this:  http://search.canadiana.ca/search?q=ottawa field= df=1900 dt=1900  Your search query has been put into the URL. You're looking at the API! Everything after  /search  is a command that you are sending to the Canadiana server.    Scroll through the results, and you'll see a number just before the question mark  ?  http://search.canadiana.ca/search/2?df=1800 dt=1900 q=ottawa field=\nhttp://search.canadiana.ca/search/3?df=1800 dt=1900 q=ottawa field=\nhttp://search.canadiana.ca/search/4?df=1800 dt=1900 q=ottawa field=  ....all the way up to 5625 (ie. 10 results per page, so 56249 / 10).  If you go to the  Canadiana API support page  you can see the full list of options. What we are particularly interested in now is the bit that says  fmt=json .    Add that  fmt=json  to your query URL. How different the results now look! What's nice here is that the data is formatted in a way that makes sense to a machine   which we'll learn more about in due course.     If you look back at the full list of API options, you'll see at the bottom that one of the options is 'retrieving individual item records'; the key for that is a field called  oocihm . If you look at your page of JSON results, and scroll through them, you'll see that each individual record has its own oocihm number. If we could get a list of those, we'd be able to programmatically slot them into the commands for retrieving individual item records:  http://search.canadiana.ca/view/oocihm.16278/?r=0 s=1 fmt=json api_text=1  The problem is: how to retrieve those oocihm numbers. The answer is, 'we write a program'. And the program that you want can be  found on Ian Milligan's website . Study that program carefully. There are a number of useful things happening in there, notably  curl ,  jq ,  sed , and  awk .  curl   is a program for downloading webpages,  jq  for dealing with JSON, and  sed  and  awk  for searching within and cleaning up text. If this all sounds Greek to you, there is an excellent gentle introduction over at  William Turkel's blog .    We need the command line program jq. We install it into our DH Box with  $ sudo apt-get install jq -y    We need to create a program. Make a new directory for this exercise like so:  $ mkdir m2e4 . Then, change into that directory by typing  $ cd m2e4 .     Make sure that's where you are by typing  $ pwd . Now, make an empty file for our program with  $ touch canadiana.sh . Touch makes an empty file; the .sh in the filename indicates that this is a shell script.    Open the empty file with  $ nano canadiana.sh . Now, the program that Ian Milligan wrote makes calls to the API that  used to live  at eco.canadiana.ca. But note the  error message on Canadiana's website . So we have to change Milligan's script so that it points to the API at  search.canadiana.ca . Copy the script below into your empty  canadiana.sh . If you want, adjust the search parameters (in the line starting with  pages ) for material you're more interested in.  #! /bin/bash\npages=$(curl 'http://search.canadiana.ca/search?q=ottawa* field= so=score df=1800 dt=1900 fmt=json' | jq '.pages')\n# this goes into the results and reads the value of 'pages' in each one of them.\n# it then tells us how many pages we're going to have.\necho \"Pages:\"$pages\n# this says 'for each one of these pages, download the 'key' value on each page'\nfor i in $(seq 1 $pages)\ndo\n        curl 'http://search.canadiana.ca/search/'${i}'?q=ottawa* field= so=score df=1800 dt=1900 fmt=json' | jq '.docs[] | {key}'   results.txt\ndone\n# the next two lines take the results.txt file that is quite messy and clean it up. I'll try to explain what they all mean. Basically, tr deletes a given character - so we delete quotation marks \"\\\"\" (the slash tells the computer not to treat the quotation mark as a computer code, but as a quotation mark itself), to erase spaces, and to find the phrase \"key:\" and delete it too.\nsed -e 's/\\\"key\\\": \\\"//g' results.txt | tr -d \"\\\"\" | tr -d \"{\" | tr -d \"}\" | tr -s \" \" | sed '/^\\s*$/d' | tr -d ' '   cleanlist.txt\n# this adds a prefix and a suffix.\nawk '$0=\"search.canadiana.ca/view/\"$0' cleanlist.txt| awk '{print $0 \"/1?r=0 s=1 fmt=json api_text=1\"}'   urlstograb.txt\n# then if we want we can take those URLs and output them all to a big text file for analysis.\nwget -i urlstograb.txt -O output.txt    Hit ctrl+x to exit Nano, and save the changes.       Before we can run this program, we have to tell DH Box that it is alright to run it. To change the 'permissions' on the file, type  $ chmod 755 canadiana.sh  The  $ chmod  command means change mode. Each number represents a user permission for reading, writing, and executing files on your computer.      And now we can run the program by typing  $ ./canadiana.sh  (the ./ is important!)  Ta da! You now have a pretty powerful tool now for grabbing data from one of the largest portals for Canadian history!     Download your  output.txt  file to your computer via the file manager and have a look at it.        Make sure to make a file noting what you've done, commands you've made, etc, and upload it in your GitHub repository.", 
            "title": "Exercise 4: APIs"
        }, 
        {
            "location": "/module-2/Exercises/#exercise-5-mining-twitter", 
            "text": "Ed Summers is part of a project called ' Documenting the Now ' which is developing tools to collect and understand the historical materials being shared (and lost) on social media. One component of Documenting the Now is the Twitter Archiving Tool, ' Twarc '. In this exercise, you are going to use Twarc to create an archive of Tweets relevant to a current trending news topic.    First of all, you need to set up a Twitter account, if you haven't already got one. Do so, but make sure to minimize any personal information that is exposed. For instance, don't make your handle the same as your real name.     Turn off geolocation. Do not give your actual location in the profile.     View the settings, and make sure all of the privacy settings are dialed down. For the time being, you  do  have to associate a cell phone number with your account. You can delete that once you've done the next step.    Go to the  Twitter apps page  and click on  new app .     On the  new application  page, just give your app a name like  my-twarc  or similar. For website, use the  Crafting Digital History site URL  (although for our purposes any website will do). You don\u2019t need to fill in any of the rest of the fields.     Continue on to the next page (tick off the box saying you\u2019ve read the developer code of behaviour). This next page shows you all the details about your new application.    Click on the \u2018Keys and Access Tokens\u2019 tab.     Copy the consumer key, the consumer secret to a text file.    Click on the \u2018create access tokens\u2019 button at the bottom of the page. This generates an access token and an access secret.     Copy those to your text file, save it.  Do not put this file in your repo or leave it online anywhere       We need to download an older version of Twarc to work with the DH Box. In your DH Box, at the command line, type the following:  $ sudo pip install https://github.com/DocNow/twarc/archive/v1.2.0.tar.gz  Twarc is written in Python, which is already installed in DH Box. 'Pip' is a package manager for installing new Python modules and packages.     Now type  $ twarc configure  and give it the information it asks for (your consumer secret etc).  You're now ready to search. For instance,  $ twarc search canada150   search.json  will search Twitter for posts using the canada150 hashtag.   Wait! Don't run that command! (Force-stop the search by hitting ctrl+c.)    If you search for  canada150  , there are, what, 36 million Canadians? How many tweets is that likely to be? Quite a lot   and the command will run quietly for days grabbing that information, writing it to file, and you'll be sitting looking at the screen wondering if anything is happening.        Try something smaller and more contained for now:  $ twarc search hist3814o   search.json .   Note that Twitter only gives access to the last two weeks or so via search.  For grabbing the stream  as an event happens  you'd use the  twarc stream  command   see the Twarc documentation for more.  It might take some time for the search to happen.  You can always force-stop the search by hitting ctrl+c.  If you do that though there could be an error in the formatting of the file which will throw an error when you get to step 15. You can still open the JSON in a text editor though, but you will have to go to the end of the file and fix the formatting.  The data being collected is in JSON format. That is, a list of 'keys' and 'values'. This is a handy format for computers, and some data visualization platforms require data in this format. For our purposes we might want to transform the JSON into a CSV (comma separated) table   a spreadsheet.    Type  $ sudo npm install json2csv --save -g . This installs a command that can convert the JSON to CSV format. Full details about the command can be found on  json2csv's GitHub repository .    Convert your  search.json  to CSV by typing  json2csv -i search.json -o out.csv    Examine your data either in a text editor or in a spreadsheet.    Use Twarc to create a file with a list of IDs.     Lodge this list and your history and notes in your repository.       NB Twitter forbids the sharing of the full metadata of a collection of tweets. You may however share a list of tweet IDs. See the Twarc documentation for the instructions on how to do that.", 
            "title": "Exercise 5: Mining Twitter"
        }, 
        {
            "location": "/module-2/Exercises/#what-can-you-do-with-this-data", 
            "text": "Examine the Twarc repository, especially its utilities. You could extract the geolocated ones and map them. You could examine the difference between 'male' and 'female' tweeters (and how problematic might that be?).     In your CSV, save the text of the posts to a new file and upload it to something like  Voyant Tools  to visualize trends over time.     Google for analysis of Twitter data to get some ideas.", 
            "title": "What can you do with this data?"
        }, 
        {
            "location": "/module-2/Exercises/#exercise-6-using-tesseract-to-turn-an-image-into-text", 
            "text": "We've all used image files like JPGs and PNGs. Images always look the same on whatever machine they are displayed on, because they contain within themselves the complete description of what the 'page' should look like. You're likley familiar with the fact that you cannot select text within an image. When we digitize documens, the image that results only contains the image layer, not the text.   To turn that image into text, we have to do what's called 'object character recognition', or OCR. An OCR algorithm looks at the pattern of pixels in the image, and maps these against the shapes it 'knows' to be an A, or an a, or a B, or a  , and so on. Cleaner, sharper printing gives better results as do high resolution images free from noise. People who have a lot of material to OCR use some very powerful tools to identify blocks of text within the newspaper page, and then train the machine to identify these, a process beyond us just now (but visit  this Tesseract q   a on stackoverflow  if you're interested).  In this exercise, you'll:   Install the Tesseract OCR engine into your DH Box  Install and use ImageMagick to convert the JPG into TIFF image format  Use Tesseract to OCR the resulting pages.  Use Tesseract in R to OCR the resulting pages.  Compare the resulting OCRd texts.", 
            "title": "Exercise 6: Using Tesseract to turn an image into text"
        }, 
        {
            "location": "/module-2/Exercises/#converting-images-in-the-command-line", 
            "text": "Begin by making a new director for this exercise:  $ mkdir ocr-test .     Type  $ cd ocr-test  to change directories into ocr-test.    Type  $ sudo apt-get install tesseract-ocr  to grab the latest version of Tesseract and install it into your DH Box. Enter your password when the computer asks for it.    Type  $ sudo apt-get install imagemagick  to install ImageMagick.    Let's convert the first file to TIFF with ImageMagick's convert command  $ convert -density 300 ~/war-diary/e001518087.jpg -depth 8 -strip -background white -alpha off e001518087.tiff  You want a high density image, which is what the -density and the -depth flags do; the rest of the command formats the image in a way that Tesseract expects to encounter text. This command might take a while. Just wait, be patient.    Extract text with  $ tesseract e001518087.tiff output.txt . This might also take some time.    Download the  output.txt  file to your own machine via DH Box's filemanager.     Open the file with a text editor.", 
            "title": "Converting images in the command line"
        }, 
        {
            "location": "/module-2/Exercises/#converting-images-in-r", 
            "text": "Now we will convert the file using R. Navigate to RStudio in the DH Box.    On the upper left side, click the green plus button   R Script to open a new blank script file.    Paste in the following script and save it as  ocr  in RStudio.  install.packages('magick')\ninstall.packages('magrittr')\ninstall.packages('tesseract')\nlibrary(magick) \nlibrary(magrittr) \nlibrary(tesseract) \ntext  - image_read(\"~/war-diary/e001518087.jpg\") % % \n  image_resize(\"2000\") % % \n  image_convert(colorspace = 'gray') % % \n  image_trim() % % \n  image_ocr()\nwrite.table(text, \"~/ocr-test/R.txt\")  Before installing any packages in RStudio, we need to install some dependencies in the command line. The reason is that since DH Box runs an older version of RStudio, not everything installs as planned.    Type  $ sudo apt-get install libmagick++-dev  in the command line to install the libmagick library.    Type  $ sudo apt-get install libtesseract-dev  in the command line to install the libtesseract library.    Type  $ sudo apt-get install libleptonica-dev  in the command line to install the libleptonic library.    Type  $ sudo apt-get install tesseract-ocr-eng  in the command line to install the English Tesseract library.    Navigate to RStudio and run each  install.packages  line in our script. This will take some time.    Run each  library()  line to load the libraries.    Run each line up to  image_ocr() . This may take some time to complete.    Run the last line  write.table()  to export the OCR to a text file with the same name.    Navigate to your file manager and download both the  output.txt  file and the  R.txt  file.    Compare the two text files in your desktop. How is the OCR in the command line versus within R? Note that they both use Tesseract just with different settings and in different environments.", 
            "title": "Converting images in R"
        }, 
        {
            "location": "/module-2/Exercises/#progressively-converting-our-files-with-tesseract", 
            "text": "Now take a screen shot of both text files (just the text area) and name them  output_1.png  and  R_1.png  respectively.    Upload both files into DH Box via the File Manager.    In the command line, type  $ tesseract output_1.png output_1.txt .     In RStudio, change the file paths in your script to the following:  install.packages('magick')\ninstall.packages('magrittr')\ninstall.packages('tesseract')\nlibrary(magick) \nlibrary(magrittr) \nlibrary(tesseract) \ntext  - image_read(\"~/ocr-test/R_1.png\") % % \n  image_resize(\"2000\") % % \n  image_convert(colorspace = 'gray') % % \n  image_trim() % % \n  image_ocr()\nwrite.table(text, \"~/ocr-test/R_1.txt\")    Run each script line again. Except this time  DO NOT  run the  install.packages()  lines since we already installed them. Simply load the libraries again and run each line.    Navigate to the File Manager and download  ouput_1.txt  and  R_1.txt .    Compare these two files. Did the OCR conversion get progressively worse? How do they compare to each other, to the first attempt at conversion, and then to the originals?       Choose either the command line or the R method to convert more of the war diary files to text. Save these files into a new directory called  war-diary-text . We will use these text files for future work in topic modeling and text analysis. How might your decision on which method to use change the results you would get in, say, a topic modeling tool?        Think about how these conversions can change based on the image being run through Tesseract. Does Tesseract have an easier time converting computer text even though it's in an image format? How might OCR conversions affect the way historians work on batch files? How does the context of the text change how historians analyse it?   Look up the  Tesseract wiki . What other options could you use with the Tesseract command to improve the results? When you decide to download Tesseract to you own computer, use the following two guides to automating bulk OCR (multiple files) with Tesseract:  Peirson's  and  Schmidt's .", 
            "title": "Progressively converting our files with Tesseract"
        }, 
        {
            "location": "/module-3/Wrangling Data/", 
            "text": "Wrangling Data July 24 - 30\n\n\nConcepts\n\n\nIn the previous module, we successfully grabbed \na lot\n of data from various online repositories. Some of it was already in well-structured tables; much of it was not. All of it was text though. Initially, it (or most of it) was just scanned images of documents. At some point, \nobject character recognition\n was used to identify the black dots from the white dots in those images, to recognize the patterns that make up letters, numbers, and punctuation. There are commercial products that can do this (and we have some installed in the Underhill Research Room that you can use), and there are \nfree products that you can install\n on your computer to do it yourself.\n\n\nIt all looks so neat and tidy. \nIan Milligan discusses this 'illusionary order'\n and its implications for historians (the previous link will not work unless you are connected to Carleton's VPN):\n\n\n\n\nIn this article, I make two arguments. Firstly, online historical databases have profoundly shaped Canadian historiography. In a shift that is rarely \u2013 if ever \u2013 made explicit, Canadian historians have profoundly reacted to the availability of online databases. Secondly, historians need to understand how OCR works, in order to bring a level of methodological rigor to their work that use these sources.\n\n\n\n\nJust as we saw with \nTed Underwood's article on theorizing search\n, these 'simple' steps in the research process are anything but. They are also profoundly theoretical in how they change what it is we can know. In archaeology, every step of the method, every stage in the process, has a profound impact on the stories we eventually tell about the past. Decisions we make destroy data, and create new data. Historians aren't used to thinking about these kinds of issues!\n\n\nThere are also \nmanual\n ways of doing the same thing as OCR does - we call these things 'humans', and we organize their work through 'crowdsourcing'. We break the process up into wee manageable steps, and make these available over the net. Sometimes we gamify these steps, to make them more 'fun'. If several people all work on the same piece of text, the thinking is that errors will cancel each other out: a proper transcription will emerge from the work of the crowd.  While transcriptions might've provided the earliest examples of crowdsourcing research (but read also \nThe HeritageCrowd Project\n and the subsequent \n'How I Lost the Crowd'\n), other tasks are now finding their way into the crowdsourced world - see the archaeological applications within the \nMicroPasts\n platform. These include things like 'masking' artefact photographs in order to develop 3d photogrammetric models.\n\n\nBut often, we don't have a whole crowd. We're just one person, alone, with a computer, at the archive. Or working with someone else's \ndigitized image that we found online\n. How do we wrangle that data? Let's start with \nM. H. Beal's account of how she 'xml'd her way to data management'\n and then consider a few more of the nuts and bolts of her work in \nOA TEI-XML DH on the WWW; or, My Guide to Acronymic Success\n.\n\n\nThis kind of work is extraordinarily important!\n You already had a taste of it in the TEI exercise in the last module. (Now, if we had a \nseriously\n big project where we were transcribing lots of text, we'd invest in a dedicated XML editor like \nOxygen\n - there are \nplugins available and frameworks for doing historical transcription\n on this platform. There is a 30 day free trial license if you want to give it a try. But for now, Notepad++, Textwrangler, Komodo Edit, Sublime text, or any of a number of good text editors will do all that we need to do). \n\n\nAlso, check out the \nTEI\n. Take 15 minutes and read through \nWhat is XML and Why Should Humanists Care?\n by David Birnbaum. Annotate! If you didn't do the TEI exercise in module 2, you could pause right now and give that a try - or at least, read it over and talk to some of your peers who DID do it.\n\n\nIn this module we're going to do some other kinds of wrangling.\n\n\nExercises\n\n\nIn the exercises for this week we are going to focus on some bare-bones wrangling of data. First, we are going to do some activities and exercises to get in the right frame of mind. Then, we'll switch gears and we'll use \nregular expressions\n to search and extract information from the Diplomatic Correspondence of the Republic of Texas, which you'll find at the Internet Archive. \n\n\nWe'll conclude by using 'Open Refine' to tidy up the information we extracted from the Texan correspondence.\n\n\nThings you will learn in this module:\n\n\n\n\nRhe power of regular expressions. \nSearch\n and \nReplace\n in Word just won't cut it any more for you! (Another reason why you should write in Markdown in the first place and then convert to Word for the final typesetting.\n\n\nOpen Refine as a powerful engine for tidying up the messiness that is ocr'd text.\n\n\n\n\nWhat you need to do this week\n\n\n\n\nRespond to the readings and the reading questions through annotation (taking care to respond to others' annotations as well) - see the instructions below.  Remember to tag your annotations with 'hist3814o' so that we can find them \non the course Hypothes.is group\n\n\nDo the exercises for this module, pushing yourself as far as you can. Annotate the instructions where they might be unclear or confusing; see if others have annotated them as well, and respond to them with help if you can. Keep an eye on our Slack channel - you can always offer help or seek out help there.  Write a blog post describing what happened as you went through the exercises (your successes, your failures, the help you may have found/received), and link to your 'faillog' (ie, the notes you upload to your GitHub account - for more on that, see the exercises!).\n\n\nSubmit your work to the course submission form\n\n\n\n\nReadings\n\n\nSelect one of the articles behind the links above OR select one of the articles below to annotate.\n\n\nBlevins, \nMining and Mapping the Production of Space A View of the World from Houston\n\n\nBlevins, \nSpace, Nation, and the Triumph of Region: A View of the World from Houston\n\n\nIan Milligan on Imageplot\n and \nimages in web archives\n\n\nRyan Cordell, \nQitjb-the-raven\n\n\nReading questions\n: On your blog, reflect on any data cleaning you've had to do in other classes. \n\n\n\n\nWhy don't historians discuss this kind of work? \n\n\nWhat gets hidden, what gets lost, how is the ultimate argument \nweaker\n as a result? \n\n\nOr does it matter? Make reference (or link to) key annotations, whether by your or one of your peers, to support your points.", 
            "title": "Data is messy"
        }, 
        {
            "location": "/module-3/Wrangling Data/#wrangling-data-july-24-30", 
            "text": "", 
            "title": "Wrangling Data July 24 - 30"
        }, 
        {
            "location": "/module-3/Wrangling Data/#concepts", 
            "text": "In the previous module, we successfully grabbed  a lot  of data from various online repositories. Some of it was already in well-structured tables; much of it was not. All of it was text though. Initially, it (or most of it) was just scanned images of documents. At some point,  object character recognition  was used to identify the black dots from the white dots in those images, to recognize the patterns that make up letters, numbers, and punctuation. There are commercial products that can do this (and we have some installed in the Underhill Research Room that you can use), and there are  free products that you can install  on your computer to do it yourself.  It all looks so neat and tidy.  Ian Milligan discusses this 'illusionary order'  and its implications for historians (the previous link will not work unless you are connected to Carleton's VPN):   In this article, I make two arguments. Firstly, online historical databases have profoundly shaped Canadian historiography. In a shift that is rarely \u2013 if ever \u2013 made explicit, Canadian historians have profoundly reacted to the availability of online databases. Secondly, historians need to understand how OCR works, in order to bring a level of methodological rigor to their work that use these sources.   Just as we saw with  Ted Underwood's article on theorizing search , these 'simple' steps in the research process are anything but. They are also profoundly theoretical in how they change what it is we can know. In archaeology, every step of the method, every stage in the process, has a profound impact on the stories we eventually tell about the past. Decisions we make destroy data, and create new data. Historians aren't used to thinking about these kinds of issues!  There are also  manual  ways of doing the same thing as OCR does - we call these things 'humans', and we organize their work through 'crowdsourcing'. We break the process up into wee manageable steps, and make these available over the net. Sometimes we gamify these steps, to make them more 'fun'. If several people all work on the same piece of text, the thinking is that errors will cancel each other out: a proper transcription will emerge from the work of the crowd.  While transcriptions might've provided the earliest examples of crowdsourcing research (but read also  The HeritageCrowd Project  and the subsequent  'How I Lost the Crowd' ), other tasks are now finding their way into the crowdsourced world - see the archaeological applications within the  MicroPasts  platform. These include things like 'masking' artefact photographs in order to develop 3d photogrammetric models.  But often, we don't have a whole crowd. We're just one person, alone, with a computer, at the archive. Or working with someone else's  digitized image that we found online . How do we wrangle that data? Let's start with  M. H. Beal's account of how she 'xml'd her way to data management'  and then consider a few more of the nuts and bolts of her work in  OA TEI-XML DH on the WWW; or, My Guide to Acronymic Success .  This kind of work is extraordinarily important!  You already had a taste of it in the TEI exercise in the last module. (Now, if we had a  seriously  big project where we were transcribing lots of text, we'd invest in a dedicated XML editor like  Oxygen  - there are  plugins available and frameworks for doing historical transcription  on this platform. There is a 30 day free trial license if you want to give it a try. But for now, Notepad++, Textwrangler, Komodo Edit, Sublime text, or any of a number of good text editors will do all that we need to do).   Also, check out the  TEI . Take 15 minutes and read through  What is XML and Why Should Humanists Care?  by David Birnbaum. Annotate! If you didn't do the TEI exercise in module 2, you could pause right now and give that a try - or at least, read it over and talk to some of your peers who DID do it.  In this module we're going to do some other kinds of wrangling.", 
            "title": "Concepts"
        }, 
        {
            "location": "/module-3/Wrangling Data/#exercises", 
            "text": "In the exercises for this week we are going to focus on some bare-bones wrangling of data. First, we are going to do some activities and exercises to get in the right frame of mind. Then, we'll switch gears and we'll use  regular expressions  to search and extract information from the Diplomatic Correspondence of the Republic of Texas, which you'll find at the Internet Archive.   We'll conclude by using 'Open Refine' to tidy up the information we extracted from the Texan correspondence.", 
            "title": "Exercises"
        }, 
        {
            "location": "/module-3/Wrangling Data/#things-you-will-learn-in-this-module", 
            "text": "Rhe power of regular expressions.  Search  and  Replace  in Word just won't cut it any more for you! (Another reason why you should write in Markdown in the first place and then convert to Word for the final typesetting.  Open Refine as a powerful engine for tidying up the messiness that is ocr'd text.", 
            "title": "Things you will learn in this module:"
        }, 
        {
            "location": "/module-3/Wrangling Data/#what-you-need-to-do-this-week", 
            "text": "Respond to the readings and the reading questions through annotation (taking care to respond to others' annotations as well) - see the instructions below.  Remember to tag your annotations with 'hist3814o' so that we can find them  on the course Hypothes.is group  Do the exercises for this module, pushing yourself as far as you can. Annotate the instructions where they might be unclear or confusing; see if others have annotated them as well, and respond to them with help if you can. Keep an eye on our Slack channel - you can always offer help or seek out help there.  Write a blog post describing what happened as you went through the exercises (your successes, your failures, the help you may have found/received), and link to your 'faillog' (ie, the notes you upload to your GitHub account - for more on that, see the exercises!).  Submit your work to the course submission form", 
            "title": "What you need to do this week"
        }, 
        {
            "location": "/module-3/Wrangling Data/#readings", 
            "text": "Select one of the articles behind the links above OR select one of the articles below to annotate.  Blevins,  Mining and Mapping the Production of Space A View of the World from Houston  Blevins,  Space, Nation, and the Triumph of Region: A View of the World from Houston  Ian Milligan on Imageplot  and  images in web archives  Ryan Cordell,  Qitjb-the-raven  Reading questions : On your blog, reflect on any data cleaning you've had to do in other classes.    Why don't historians discuss this kind of work?   What gets hidden, what gets lost, how is the ultimate argument  weaker  as a result?   Or does it matter? Make reference (or link to) key annotations, whether by your or one of your peers, to support your points.", 
            "title": "Readings"
        }, 
        {
            "location": "/module-3/Exercises/", 
            "text": "Module 3 Exercises\n\n\nThe exercises in this module cover:\n\n\n\n\nCleaning text with regular expressions\n\n\nCleaning files with Open Refine\n\n\n\n\n\n\nExercise 1: Regular Expressions\n\n\nWhen we have text that has been marked up, we can do interesting things with it. In the previous module, you saw that XSL can be used to add styles to your XML (which a browser can interpret and display). You can see how having those tags makes for easier searching for information as well, right? Sometimes things are not well marked up. Sometimes, it's all a real mess. In this exercise, we'll explore 'regular expressions', (aka 'Regex') or ways of asking the computer to search for \npatterns\n rather than matching exact text.\n\n\nThis exercise follows a tutorial written for \nThe Macroscope\n. Another good reference is the \nregular expressions website\n.\n\n\nYou should open \nRegeXr\n, an interactive tutorial that shows us what various regular expressions can do, in a browser window so you can test your regular expressions out \nbefore\n applying them to your data! We will use the power of regular expressions to search for particular patterns in a published volume of the correspondence of the Republic of Texas. We'll use regex to massage the information into a format that we can then use to generate a social network of letter writers over time. (You might also want to try the \nProgramming Historian tutorial Understanding Regular Expressions\n).\n\n\nWhat if you had \na lot\n of documents that you needed to clean up? One way of doing it would be to write a python program that applied your regex automatically, across all the files in a folder. \nOptional advanced exercise\n: \nCleaning OCR'd Text with Regular Expressions\n.\n\n\nFor this exercise, do the following: \n\n\n\n\n\n\nStart with \na gentle introduction to regex\n.\n\n\n\n\n\n\nThen begin the \nregex exercise\n.\n\n\n\n\n\n\nRemember to copy your history file and any other information, observations, or thoughts to your GitHub repo.\n\n\n\n\nExercise 2: Open Refine\n\n\nOpen Refine\n is the final tool we'll explore in this module. This engine allows us to clean up our messy data. We will feed it the results from Exercise 2 in order to consolidate individuals (ie. 'Shawn' and 'S4awn' are probably the same person, so Open Refine will consolidate that information for us). This exercise also follows a tutorial originally written for \nThe Macroscope\n.\n\n\nOpen Refine does not run in DH Box, so use the File Manager in DH Box to move your \ncleaned-correspondence\n file to somewhere safe on your computer.\n\n\nFor this exercise, do the following: \n\n\n\n\nThe \nOpen Refine exercise in our supporting materials\n.\n\n\n\n\nFor more advanced usage of Open Refine, as an optional exercise you can also try \nThe Programming Historian's Tutorial on Open Refine\n.\n\n\nRemember to copy your notes and any other information/observations/thoughts to your GitHub repo.\n\n\nYour final project\n\n\nYou can use what you've been learning here to do some clean-up on the Canadian war diary files (or a subset of them, of course, as suits your interests \n you have explored them, haven't you?). Google for \nsed patterns\n and see if you can combine what you find with what you've learned in this module in order to clean up the text. For instance, a common error in OCR is to confuse the letter \ni\n with the letter \nl\n and the numeral \n1\n. Shawville becomes Shawvllle. You could use grep to see if that error is present, and then sed to correct it. The file names are long, and there are several hundred; you might give this kind of thing a try too:\n\n\n$ find . -type f -exec sed -i.bak \"s/foo/bar/g\" {} \\;\n\n\nThis command finds all files in a folder and creates a backup for each one in turn before searching for \nfoo\n and replacing it with \nbar\n.\n\n\nThe command \n$ grep \"[b-df-hj-np-tv-xz]\\{5\\}\" filename\n will find all instances of strings of five consonants or more, which can be useful to give you and idea of what kinds of sed patterns to write.\n\n\nMaybe, if you inspect the PDF and the txt files together, you can figure out patterns that set off interesting things in say the classified ads or the editorials \n and then write some grep and sed to create new files with just that information. Then you could use Open Refine to further clean things up. Maybe the messiness of the data is \nexactly the point\n (\nand my workup on bad OCR\n) you want to explore. Nevertheless:\n\n\nCleaning data is 80% of the work in digital history.\n\n\nRemember to copy your notes and any other information/observations/thoughts to your GitHub repo.\n\n\nGoing Further\n\n\n\n\n\n\nSee some of the suggestions at the end of the \nOpen Refine exercise\n.\n\n\n\n\n\n\nThe \nStanford Named Entity Recognizer\n is a program that enables you to automatically tag words in your corpus according to whether or not they are place names, individuals, and so on. The output can then be subsequently extracted and visualized. Try the \nNER exercise in our supporting materials\n.\n\n\n\n\n\n\nCounting and Mining Data with Unix", 
            "title": "Exercises"
        }, 
        {
            "location": "/module-3/Exercises/#module-3-exercises", 
            "text": "The exercises in this module cover:   Cleaning text with regular expressions  Cleaning files with Open Refine", 
            "title": "Module 3 Exercises"
        }, 
        {
            "location": "/module-3/Exercises/#exercise-1-regular-expressions", 
            "text": "When we have text that has been marked up, we can do interesting things with it. In the previous module, you saw that XSL can be used to add styles to your XML (which a browser can interpret and display). You can see how having those tags makes for easier searching for information as well, right? Sometimes things are not well marked up. Sometimes, it's all a real mess. In this exercise, we'll explore 'regular expressions', (aka 'Regex') or ways of asking the computer to search for  patterns  rather than matching exact text.  This exercise follows a tutorial written for  The Macroscope . Another good reference is the  regular expressions website .  You should open  RegeXr , an interactive tutorial that shows us what various regular expressions can do, in a browser window so you can test your regular expressions out  before  applying them to your data! We will use the power of regular expressions to search for particular patterns in a published volume of the correspondence of the Republic of Texas. We'll use regex to massage the information into a format that we can then use to generate a social network of letter writers over time. (You might also want to try the  Programming Historian tutorial Understanding Regular Expressions ).  What if you had  a lot  of documents that you needed to clean up? One way of doing it would be to write a python program that applied your regex automatically, across all the files in a folder.  Optional advanced exercise :  Cleaning OCR'd Text with Regular Expressions .  For this exercise, do the following:     Start with  a gentle introduction to regex .    Then begin the  regex exercise .    Remember to copy your history file and any other information, observations, or thoughts to your GitHub repo.", 
            "title": "Exercise 1: Regular Expressions"
        }, 
        {
            "location": "/module-3/Exercises/#exercise-2-open-refine", 
            "text": "Open Refine  is the final tool we'll explore in this module. This engine allows us to clean up our messy data. We will feed it the results from Exercise 2 in order to consolidate individuals (ie. 'Shawn' and 'S4awn' are probably the same person, so Open Refine will consolidate that information for us). This exercise also follows a tutorial originally written for  The Macroscope .  Open Refine does not run in DH Box, so use the File Manager in DH Box to move your  cleaned-correspondence  file to somewhere safe on your computer.  For this exercise, do the following:    The  Open Refine exercise in our supporting materials .   For more advanced usage of Open Refine, as an optional exercise you can also try  The Programming Historian's Tutorial on Open Refine .  Remember to copy your notes and any other information/observations/thoughts to your GitHub repo.", 
            "title": "Exercise 2: Open Refine"
        }, 
        {
            "location": "/module-3/Exercises/#your-final-project", 
            "text": "You can use what you've been learning here to do some clean-up on the Canadian war diary files (or a subset of them, of course, as suits your interests   you have explored them, haven't you?). Google for  sed patterns  and see if you can combine what you find with what you've learned in this module in order to clean up the text. For instance, a common error in OCR is to confuse the letter  i  with the letter  l  and the numeral  1 . Shawville becomes Shawvllle. You could use grep to see if that error is present, and then sed to correct it. The file names are long, and there are several hundred; you might give this kind of thing a try too:  $ find . -type f -exec sed -i.bak \"s/foo/bar/g\" {} \\;  This command finds all files in a folder and creates a backup for each one in turn before searching for  foo  and replacing it with  bar .  The command  $ grep \"[b-df-hj-np-tv-xz]\\{5\\}\" filename  will find all instances of strings of five consonants or more, which can be useful to give you and idea of what kinds of sed patterns to write.  Maybe, if you inspect the PDF and the txt files together, you can figure out patterns that set off interesting things in say the classified ads or the editorials   and then write some grep and sed to create new files with just that information. Then you could use Open Refine to further clean things up. Maybe the messiness of the data is  exactly the point  ( and my workup on bad OCR ) you want to explore. Nevertheless:  Cleaning data is 80% of the work in digital history.  Remember to copy your notes and any other information/observations/thoughts to your GitHub repo.", 
            "title": "Your final project"
        }, 
        {
            "location": "/module-3/Exercises/#going-further", 
            "text": "See some of the suggestions at the end of the  Open Refine exercise .    The  Stanford Named Entity Recognizer  is a program that enables you to automatically tag words in your corpus according to whether or not they are place names, individuals, and so on. The output can then be subsequently extracted and visualized. Try the  NER exercise in our supporting materials .    Counting and Mining Data with Unix", 
            "title": "Going Further"
        }, 
        {
            "location": "/module-4/Seeing Patterns/", 
            "text": "Seeing Patterns July 31 - Aug 6\n\n\nConcepts\n\n\nIn the previous module, we collected and cleaned vast amounts of historical data. Let's see what patterns emerge. Part exploration, part reflection, and part argument, these two stages are not linear but feed into one another. Exploration and preliminary visualizations are often about finding the holes. Sometimes, a quick visualization helps you understand what is missing, or what is a glitch in your method, or what is just plain mistaken in your assumptions. I once spent quite a lot of time staring at a network graph before I realized that I'd neglected to import the entire file - I was trying to interpret just a subset of it! Back to my wrangling I went, and in the process of cleaning that data, I realized there were patterns that would be better visualized as simple line plots. I visualized these - and found other errors. Back to wrangling. In the end, a combination of network graph and simple line plots allowed me to identify a story about influence and control of landholding (I was working with archaeological data).\n\n\nIn this module, let us begin by looking at a couple of pieces that explore this cyclical dynamic between data wrangling and exploration. In our Slack space for this module or in your notebooks, I want you to explicitly discuss the work of the following scholars. Make explicit connections with things you've read/explored/studied/developed in other history classes!\n\n\nIn essence: what would your last essay last term have looked like, if you'd been thinking along these lines?\n\n\n\n\nBegin by taking visiting the \nMapping Texts (pdf opens in new tab)\n project. \n\n\nLook at the work of Michelle Moravec who is using \ncorpus linguistics tools to understand the history of women's suffrage\n (and also the \nvisualizing gender\n). \n\n\nListen to Micki Kaufmann on \nQuantifying Kissinger\n. (Her \nmethods are detailed on her blog\n.) \n\n\nTalk about \nnetwork analysis\n. \n\n\nKeep an \neye on Paul Revere\n. Talk about \ntopic modeling\n (and also \nthis blog post on Mallet\n). \n\n\nTalk about \nprinciples of visualization\n. \n\n\nYou should probably talk about \nEdward Tufte\n, too.\n\n\n\n\nTalk. What could history be like if more of our materials went through these mills?\n\n\nThings you will learn in this module\n\n\n\n\nImporting, querying, and visualizing networks with \nGephi\n \n- igraph, R\n\n\nTopic modeling with the GUI topic modeling tool, and MALLET at the command line, and in R\n\n\nSimple maps with CartoDB (which may include georectifying and displaying historical maps as base layers)\n\n\nCorpus linguistics with AntConc\n\n\nTF-IDF with Overview\n\n\nQuick visualizations using \nRAW, an open source data visualization framework\n\n\n\n\nWe will be busy in this module. \nDo not be afraid\n to ask myself, your peers, or the wider DH community for help and advice! It's the only way we grow.\n\n\nAnd finally...\n\n\nJust because there is a package, or a routine, or an approach to doing \nx\n with your data \ndoes not mean\n that you park your critical apparatus at the door. Consider Matthew Jockers who has done some amazing work putting together \na package for the R statistical language that helps one analyze plot arcs in thousands of novels at once\n. Jockers describes how it works on \nhis blog\n. Annie Swafford \nexplores this package\n in a comprehensive blog post. She highlights important issues in the way the package deals with the complexity of the world, and how that might have an impact on results and conclusions drawn from using the package. The interplay of the work that Jockers has done, and the dialogue that Swafford has started with Jockers, is an important feature (and perhaps, a defining feature) of digital humanities scholarship. Jocker builds; and Swafford approaches the code from a humanistic perspective; and in the dialectic of their exchange, the code and our ability to understand the code's limitations and potentials will improve. And so we understand the world better.\n\n\nWhen you choose your exploratory method, you need to consider that the method has its own agenda!\n\n\nWhat you need to do this week\n\n\n\n\nRespond to the readings and the reading questions through annotation (taking care to respond to others' annotations as well) - see the instructions below.  Remember to tag your annotations with 'hist3814o' so that we can find them \non the course Hypothes.is group\n\n\nDo the exercises for this module, pushing yourself as far as you can. Annotate the instructions where they might be unclear or confusing; see if others have annotated them as well, and respond to them with help if you can. Keep an eye on our Slack channel - you can always offer help or seek out help there. The \nexercises\n in this module will pick up where we left off, with the Texan Correspondence. You'll represent the exchange of letters as a network. We'll try to extract place names from it. We'll topic model the letters themselves. We'll explore various ways of visualizing those results. Write a blog post describing what happened as you went through the exercises (your successes, your failures, the help you may have found/received), and link to your 'faillog' (ie, the notes you upload to your GitHub account - for more on that, see the exercises!).\n\n\nSubmit your work to the course submission form\n\n\n\n\nReadings\n\n\nSelect one of the articles behind the links above (and/or in the exercises) to annotate. Ask yourself: \nwho benefits from this? Who is hurt from this?\n Make an entry in your blog on this theme.", 
            "title": "Seeing Patterns"
        }, 
        {
            "location": "/module-4/Seeing Patterns/#seeing-patterns-july-31-aug-6", 
            "text": "", 
            "title": "Seeing Patterns July 31 - Aug 6"
        }, 
        {
            "location": "/module-4/Seeing Patterns/#concepts", 
            "text": "In the previous module, we collected and cleaned vast amounts of historical data. Let's see what patterns emerge. Part exploration, part reflection, and part argument, these two stages are not linear but feed into one another. Exploration and preliminary visualizations are often about finding the holes. Sometimes, a quick visualization helps you understand what is missing, or what is a glitch in your method, or what is just plain mistaken in your assumptions. I once spent quite a lot of time staring at a network graph before I realized that I'd neglected to import the entire file - I was trying to interpret just a subset of it! Back to my wrangling I went, and in the process of cleaning that data, I realized there were patterns that would be better visualized as simple line plots. I visualized these - and found other errors. Back to wrangling. In the end, a combination of network graph and simple line plots allowed me to identify a story about influence and control of landholding (I was working with archaeological data).  In this module, let us begin by looking at a couple of pieces that explore this cyclical dynamic between data wrangling and exploration. In our Slack space for this module or in your notebooks, I want you to explicitly discuss the work of the following scholars. Make explicit connections with things you've read/explored/studied/developed in other history classes!  In essence: what would your last essay last term have looked like, if you'd been thinking along these lines?   Begin by taking visiting the  Mapping Texts (pdf opens in new tab)  project.   Look at the work of Michelle Moravec who is using  corpus linguistics tools to understand the history of women's suffrage  (and also the  visualizing gender ).   Listen to Micki Kaufmann on  Quantifying Kissinger . (Her  methods are detailed on her blog .)   Talk about  network analysis .   Keep an  eye on Paul Revere . Talk about  topic modeling  (and also  this blog post on Mallet ).   Talk about  principles of visualization .   You should probably talk about  Edward Tufte , too.   Talk. What could history be like if more of our materials went through these mills?", 
            "title": "Concepts"
        }, 
        {
            "location": "/module-4/Seeing Patterns/#things-you-will-learn-in-this-module", 
            "text": "Importing, querying, and visualizing networks with  Gephi   - igraph, R  Topic modeling with the GUI topic modeling tool, and MALLET at the command line, and in R  Simple maps with CartoDB (which may include georectifying and displaying historical maps as base layers)  Corpus linguistics with AntConc  TF-IDF with Overview  Quick visualizations using  RAW, an open source data visualization framework   We will be busy in this module.  Do not be afraid  to ask myself, your peers, or the wider DH community for help and advice! It's the only way we grow.", 
            "title": "Things you will learn in this module"
        }, 
        {
            "location": "/module-4/Seeing Patterns/#and-finally", 
            "text": "Just because there is a package, or a routine, or an approach to doing  x  with your data  does not mean  that you park your critical apparatus at the door. Consider Matthew Jockers who has done some amazing work putting together  a package for the R statistical language that helps one analyze plot arcs in thousands of novels at once . Jockers describes how it works on  his blog . Annie Swafford  explores this package  in a comprehensive blog post. She highlights important issues in the way the package deals with the complexity of the world, and how that might have an impact on results and conclusions drawn from using the package. The interplay of the work that Jockers has done, and the dialogue that Swafford has started with Jockers, is an important feature (and perhaps, a defining feature) of digital humanities scholarship. Jocker builds; and Swafford approaches the code from a humanistic perspective; and in the dialectic of their exchange, the code and our ability to understand the code's limitations and potentials will improve. And so we understand the world better.  When you choose your exploratory method, you need to consider that the method has its own agenda!", 
            "title": "And finally..."
        }, 
        {
            "location": "/module-4/Seeing Patterns/#what-you-need-to-do-this-week", 
            "text": "Respond to the readings and the reading questions through annotation (taking care to respond to others' annotations as well) - see the instructions below.  Remember to tag your annotations with 'hist3814o' so that we can find them  on the course Hypothes.is group  Do the exercises for this module, pushing yourself as far as you can. Annotate the instructions where they might be unclear or confusing; see if others have annotated them as well, and respond to them with help if you can. Keep an eye on our Slack channel - you can always offer help or seek out help there. The  exercises  in this module will pick up where we left off, with the Texan Correspondence. You'll represent the exchange of letters as a network. We'll try to extract place names from it. We'll topic model the letters themselves. We'll explore various ways of visualizing those results. Write a blog post describing what happened as you went through the exercises (your successes, your failures, the help you may have found/received), and link to your 'faillog' (ie, the notes you upload to your GitHub account - for more on that, see the exercises!).  Submit your work to the course submission form", 
            "title": "What you need to do this week"
        }, 
        {
            "location": "/module-4/Seeing Patterns/#readings", 
            "text": "Select one of the articles behind the links above (and/or in the exercises) to annotate. Ask yourself:  who benefits from this? Who is hurt from this?  Make an entry in your blog on this theme.", 
            "title": "Readings"
        }, 
        {
            "location": "/module-4/Exercises/", 
            "text": "Module 4 Exercises\n\n\nThere are \nmany\n \ndifferent tools and approaches\n you could use to visualize your data, both as a preliminary pass to spot the holes and also for more formal analysis. In which case, for this module, I would like you to select \ntwo\n of these exercises which seem most germane for your final project.\n\n\nYou are welcome to work through more of them, of course, but I want the exercises to move your own research forward. Some of these I wrote and some are adapted from \nThe Macroscope\n; others are adapted or used holus-bolus from scholars like \nMiriam Posner\n, \nFred Gibbs\n, and \nHeather Froehlich\n (and I'm grateful that they shared their materials!). Finally, you are welcome to explore the lessons and tutorials at \nThe Programming Historian\n if they seem appropriate to what you want to do for your project.\n\n\nBut what if I haven't any idea of what to do for the final project?\n Then read through the various tutorials for inspiration. Find something that strikes you as interesting, and then talk to me about how you might employ the ideas or concepts with regard to the Canadian war diary data.\n\n\nThings to install?\n\n\nMany of these involve having to install more software on your own machine. In those exercises that involve using R and RStudio, you are welcome to install RStudio on your own machine OR to use it in DH Box. Please read \nthis quick introduction to R and Rstudio carefully\n.\n\n\nIf you decide to install R and RStudio on your own machine,\n I would suggest you read the introductory bits from Lincoln Mullen's book-in-progress, \nComputational Historical Thinking\n, especially the 'setup' part under 'getting started' (pay attention to the bit on installing packages and dependencies). If you spot any exercises in Mullen's book that seem relevant to your project, you may do those as an alternative to the ones here. \nAlternatively\n, go to \nSwirl\n and \nlearn the basics of R within R\n. \nDHNow\n links to a new \nBasic Text Mining in R\n tutorial which is worth checking out as well.\n\n\nNB It is always very important to record in your own notebooks what version of R you used for your analysis, what version of any R packages you installed and used, and so on because packages can go out of date.\n\n\nIn the table below I've gathered the exercises together under the headings of \nText\n, \nNetworks\n, \nMaps\n, and \nCharts\n. I've also added some entries that I am categorizing under \nArt\n The first is a series of exercises on \nthe sonification of data\n and the second is a guide \nto making twitterbots\n; the third is about glitching digital imagery. These approaches can provide surprising and novel insights into history, as they move from representing history digitally to \nperforming\n it. Visit, for instance, \nthe final project in an undergraduate digital history class at the University of Saskatchewan by Daniel Ruten\n. Daniel translated simple wordclouds of a First World War diary into a profound auditory performance. I would be very interested indeed to see if any final projects in HIST3814o gave sonification or twitterbots or glitch a try.\n\n\nThe exercises in this module are covered in the chart below:\n\n\n\n\n\n\n\n\nTexts\n\n\nNetworks\n\n\nMaps\n\n\nCharts\n\n\nArt\n\n\n\n\n\n\n\n\n\n\nTopic Modeling Tool\n\n\nNetwork analysis in Gephi\n\n\nSimple mapping \n georectifying\n\n\nQuick charts using RAW\n\n\nSonification\n\n\n\n\n\n\nTopic Modeling in R\n\n\nConverting 2-mode to 1-mode\n\n\nQGIS (tutorials by Fred Gibbs)\n\n\n\n\nTwitterbots\n\n\n\n\n\n\nText Analysis with OverviewProject\n\n\nNetwork Analysis in R\n\n\nGeoparsing with Python\n\n\n\n\nGlitching Photos\n\n\n\n\n\n\nCorpus Linguistics with AntConc\n\n\nNetwork Analysis in Cytoscape\n\n\nPalladio with Posner\n\n\n\n\n\n\n\n\n\n\nText Analysis with Voyant\n\n\nChoose your own adventure\n\n\nLeaflet.js Maps\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 1: Network Visualization\n\n\nThis exercise uses the open source program \nGephi\n which you install on your own computer. If you'd rather not install anything, please see \nNetwork Analysis in R\n instead.\n\n\nRecall that the index of the collected letters of the Republic of Texas was just a list of letters from so-and-so to so-and-so. We haven't looked at the content of those letters, but the shape of network \n the meta data of that correspondence \n can be revealing (remember \nPaul Revere!\n) When we stitch that together into a network of people connected because they exchanged letters, we end up with a shard of their social network. Networks can be queried for things like power, position, and role, and so used judiciously, we can begin to suss something of the social structures in which their history took place. I would recommend that you also take a long look at Scott Weingart's series, \nNetworks Demystified\n. Finally, \nheed our warning\n.\n\n\nFor this exercise, do the following: \n\n\n\n\nTransform your Texan Correspondence data into a network, which you will then visualize with the open source program \nGephi\n. The detailed instructions are \nin our supporting materials\n.\n\n\n\n\n\n\nExercise 2: Topic Modeling Tool\n\n\nIn this exercise you will use the \nTopic Modeling Tool\n to create a simple topic model and a webpage that allows you to browse the results.\n\n\n\n\n\n\nDownload the \nTopic Modeling Tool from GitHub\n.\n\n\n\n\n\n\nMake sure you have some content on your own machine; the Colonial Newspaper Database is a handy corpus. (Created by Melodee Beals, it's a series of late 18th, early 19th century cleanly transcribed newspaper articles from Scotland and Northern England; You can grab \nmy copy from GitHub\n). Or perhaps you might move your copy of the Canadian war diary out of DH Box onto your computer. \n\n\n\n\n\n\nAt the command prompt in DH Box, type \n$ ls\n to make sure you can see your Canadian war diary text folder (ie. you can't zip a folder from the command line if you are \nin\n that folder, so \n$ cd\n out of it if necessary). \n\n\n\n\n\n\nAssuming your files are in \nwar-diary-text\n, zip the folder up with this command, \n$ zip -r wardiaryfiles.zip war-diary-text\n. \n\n\n\n\n\n\nUse the File Manager to download the zip file. \n\n\n\n\n\n\nUnzip the folder on your machine.\n\n\n\n\n\n\n \n\n\n\n\n\n\nDouble-click on the file you downloaded in Step 1. This will open a java-based graphical user interface with one of the most common topic modeling approaches, 'Latent Dirichlet Allocation'.\n\n\n\n\n\n\nSet the input to be the Colonial Newspaper Database \nor\n the Canadian war diary.\n\n\n\n\n\n\nSet the output to be somewhere neat and tidy on your computer.\n\n\n\n\n\n\nSet the number of topics you'd like to model.\n\n\n\n\n\n\nClick 'train topics' to run the algorithm.\n\n\n\n\n\n\nWhen it finishes, go to the folder you selected for output, and find the file \nall_topics.html\n in the \noutput_html\n folder. \n\n\n\n\n\n\nClick on \nall_topics.html\n. You now have a browser-based way of navigating your topics and documents. In the \noutput_csv\n folder created, you will find the same information as CSV, which you could then input into a spreadsheet for other kinds of visualizations (which we'll talk about in class.)\n\n\n\n\n\n\n \n\n\n\n\n\n\nMake a note in your open notebook about your process and your observations. How does reading this material in this way change/challenge/or focus your understanding of the material?\n\n\n\n\nExercise 3: Topic Modeling in R\n\n\nExercise 2 was quite a simple way to do topic modeling. In this exercise, we are going to use a package for the R statistical language called 'Mallet' to do our topic modeling. One way isn't necessarily better than the other, although doing our analysis within R allows the potential for extending the analysis or combining it with other data. First, read \nthis introduction to R\n so what follows isn't a complete shock!\n\n\nFor this exercise, do \nONE\n of the following: \n\n\n\n\nGuidance for doing this in RStudio \nin the DH Box\n\n\nGuidance for doing this in RStudio \ninstalled on your own computer\n\n\n\n\n\n\nExercise 4: Text Analysis with Overview\n\n\nIn this exercise, we're going to look at the Colonial Newspaper Database again, but this time using a tool called 'Overview'. Overview uses a different approach that the topic models we've been discussing. In essence, it looks at word frequencies and their distributions within a document, and within a corpus, to organize the documents into folders of progressively similar word use.\n\n\n\n\nYou can download Overview to run on your own machine, but for our purposes, the hosted version on the \nOverview Docs website\n is sufficient. Go to that page, watch the video, create an account, and then log in (\nyou must create an account to use overview\n). (More help about how Overview works \nmay be found on their blog\n, including helpful videos.)\n\n\nOnce you're inside, click 'import from a CSV file', and upload the \nCND.csv\n (which you can download and save to your own machine from \nmy GitHub\n \n- right-click and save as. \n\n\nOn the 'UPLOAD A CSV FILE' page in Overview click 'browse' and select the \nCND.csv\n. It will give you a preview. There are a number of options here \n you can tell Overview which words to ignore, and which words to give added importance to. What words will you select? Make a note in your notebook. \n\n\nHit 'upload'.\n\n\nA new page appears, called \nYOUR DOCUMENT SETS\n. Click on the one you just uploaded. A file folder tree showing documents of progressively greater similarity will open; on the right hand side will be the list of documents within each box (the box in question will be greyed out when you click on it, so you know where you are). You can search for words in your document, and Overview will tell you where they are; you can tag documents that you find interesting. The Overview system allows you to jump between a distant, macroscopic view and a close, document level view. \n\n\nJump back and forth, see what you can find. For suggestions about how to use Overview effectively, try \ntheir blog\n. Make notes about what you observe in your notebook. Also, you can export your tagged document set from Overview, so that you could visualize the patterns of tagging in a spreadsheet (for instance).\n\n\n\n\n\n\n\n\n \n\n\nGoing further:\n Do you see how you could upload your documents that you collected during Module 2?\n\n\n\n\nExercise 5: Corpus Linguistics with AntConc\n\n\nHeather Froelich has put together an excellent step-by-step with using AntConc for exploring textual patterns within, and across, corpora of texts. Work your way through her \ntutorial\n.\n\n\nCan you get our example materials (from the Colonial Newspaper Database) into AntConc? \nOur instructions in The Macroscope might help you\n to split the CSV into individual txt files. Alternatively, do you have any materials of your own, already collected? Feed them into AntConc. What patterns do you see? What if you compare your materials against other corpora of texts?\n\n\nFor your information, \nCoRD has a collection of corpora that you can explore\n.\n\n\n\n\nExercise 6: Text Analysis with Voyant\n\n\nIn \nModule 2\n, if you recall, we worked through how to transform XML using stylesheets. Melodee Beals used a \nstylesheet \n to transform her database into a series of individual txt files. In the exercises above, a transformer was used to make the database into a single CSV file. In this exercise, we are going to use \nVoyant Tools\n to visualize patterns in word use in the database. Voyant can read either a CSV \nor\n text files. The advantage of uploading a folder of text files is that, if the files are in chronological order, Voyant's default visualizations will also be arranged in chronological order and thus we can see change over time.\n\n\n\n\n\n\nGo to \nVoyant Tools\n. Paste the following URL to the CSV of the CND database: \nhttps://raw.githubusercontent.com/shawngraham/exercise/gh-pages/CND.csv\n.\n\n\n\n\n\n\nNow, open a new browser window, and go to this \ncolonial newspaper file on Voyant tools\n.\n\n\nDo you see the difference? In the latter window, the individual articles have been uploaded individually, and thus are treated as individual documents in chronological order.\n\n\n\n\n\n\nExplore the corpus, comparing terms over time, looking at keywords in context, and using the RezoViz tool to create a graph where people, places, and organizations that appear in the same documents (and across documents) are connected (you can find 'rezoviz' under the cogwheel icon at the top right of the panel). \n\n\n\n\n\n\nGoogle these terms and tools for what they mean and how others have used them. You can embed any of the tools in your blogs by using the 'save' icon and getting the iframe or embed code. You can apply \nstopwords\n by clicking on the cogwheel in any of the different tools, and selecting stopwords. \n\n\n\n\n\n\nApply the stopwords globally, and you'll only have to do this once! What patterns do you see? What do different tools highlight? Which ones are useful? What patterns do you see that strike you as interesting? Note this all down.\n\n\n\n\n\n\n \n\n\n\n\n\n\nGoing further:\n Upload materials you collected in module 2 and explore them.\n\n\n\n\nExercise 7: RAW\n\n\nQuick Charts Using RAW\n\n\nA quick chart can be a handy thing to have. Google spreadsheets, Microsoft Excel, and a host of other programs can make excellent charts quickly with their wizard functions. Never hesitate to turn to these. However, they are not always good with non-numeric data. In \nModule 3\n, you used the Stanford Named Entity Recognizer to extract place names from a text. After some further munging with regex, you might have ended up with a CSV that looks like \nthe Texas CSV\n. Can we do a quick visualization of this information? One useful tool is \nRAW\n. \n\n\n\n\nOpen RAW in a new window.\n\n\nCopy the table of data of places mentioned in the Texan correspondence, and paste it into the data input box at the top of the RAW screen.\n\n\n\n\nOh no, an error!\n\n\nA quick data munge\n\n\nYou should get an error message, to the effect that you need to check 'line 2'. What's gone wrong? RAW has checked the number of values you have in that row, and compared it to the number of columns in row 1 (which contains all the column names). It sees that the two don't match. What we need to do is add a default null value in those cells.\n\n\n\n\n\n\nGo to \nGoogle Sheets\n.\n\n\n\n\n\n\nClick the 'Go to Google Sheets' button, and then click on the big green plus sign to start a new sheet. \n\n\n\n\n\n\nPaste the following into the top-left cell (cell A1):\n\n\n=IMPORTDATA(\"https://raw.githubusercontent.com/hist3907b-winter2015/module4-holes/master/texas.csv\")\n\n\n\nPretty neat, eh? Now, here's the thing: even though your sheet \nlooks\n like it is filled with information, it's not (at least, as far as the script we are about to run is concerned). That is to say, the sheet itself only has one cell of data, and that one cell is grabbing info from elsewhere on the web and dynamically filling the sheet. The script we're going to run works only on static values (more or less).\n\n\n\n\n\n\nPlace your cursor in cell B1. On a Mac, hit \nshift+cmnd+downarrow\n. On a Windows machine, hit \nshift+ctrl+downarrow\n. Then on Mac \nshift+cmnd+rightarrow\n, on Windows \nshift+ctrl+rightarrow\n.\n\n\n\n\n\n\nCopy all of that data (\ncmnd+c\n or \nctrl+c\n).\n\n\n\n\n\n\nUnder 'Edit' select 'paste special' -\n 'paste VALUES only'.\n\n\nThe formula you put in cell A1 now says \n#REF!\n. You can delete this now. This mucking about is necessary so that the add on script we are about to run will work.\n\n\n\n\n\n\nWe now need to fill those empty values. In the tool bar, click \nadd ons\n -\n \nget add ons\n. Search for \nblanks\n. You want to add \nBlank Detector\n.\n\n\n\n\n\n\nNow, click somewhere in your data. On Mac, hit \ncmnd+a\n. On Windows, hit \nctrl+a\n. This highlights all of your data. \n\n\n\n\n\n\nClick \nAdd ons\n -\n \nblank detector\n -\n \ndetect cells\n. A dialogue panel will open on the right hand side of your screen. \n\n\n\n\n\n\nClick the button beside \nset value\n and type in \nnull\n. \n\n\n\n\n\n\nHit \nrun\n. All of the blank cells will fill with the word \nnull\n. \n\n\n\n\n\n\nDelete column A (which formerly had record numbers, but is now just filled with the word \nnull\n. We don't need it). \nIf you get the error, 'run exceeded maximum time'\n just hit the run button again. This script might take a few minutes.\n\n\nYou can now copy and paste your table of data into the data input box in RAW, and you should get the green thumbs up saying x records have been successfully parsed!\n\n\nNB The Blank Detector add on may take a long time. Try it out on the CSV. You can copy the parsed data from \nmy Google Sheets file\n.\n\n\n\n\n\n\n \n\n\n\n\n\n\nPlaying with RAW\n\n\nRAW takes your data, and depending on your choices, passes it into chart templates built on the D3.js code library. D3.js is a powerful library for making all sorts of charts (including interactive ones). If this sort of thing interests you, you can follow the tutorials in \nElijah Meeks' excellent new book\n.\n\n\n\n\n\n\nWith your data pasted in, you can now experiment with a number of different visualizations that are all built on the D3.js code library.  \n\n\n\n\n\n\nTry the \u2018alluvial\u2019 diagram.  Pick place1 and place2 as your dimensions \n you click and drag the green boxes under 'map your data' into the 'steps' box. \n\n\n\n\n\n\nLeave the 'size' box empty. \n\n\n\n\n\n\nUnder 'customize your visualization' you can click inside the 'width' box to make the diagram wider and more legible.\n\n\nDoes anything jump out? \n\n\n\n\n\n\nTry place3 and place 4. Try place1, place2, place3, and place4 in a single alluvial diagram. \n\n\nWhen we look at the original letters, we see that the writer often identified the town in which he was writing, and the town of the addressee. Why choose the third and fourth places? Perhaps it makes sense, for a given research question, to assume that with the pleasantries out of the way the writers will discuss the places important to their message. Experiment! This is one of the joys of working with data, experimenting to see how you can deform your materials to see them in a new light.\n\n\n\n\n\n\nYou can export your visualization under the 'download' box at the bottom of the RAW page \n your choices are as a simple raster image (PNG), a vector image (SVG) or a data representation (json).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 8: Simple Mapping and Georectifying\n\n\nIn this exercise, you will find a historical map online, upload a copy to a mapwarper service, georectify it, and then display the map online, via a hosted service like CartoDB, and also through a map you will build yourself using Leaflet.js. Finally, we will also convert CSV to geojson using \nMapbox's geojson converter\n, and we'll map that as a GitHub gist. We'll also grab a geojson file hosted on GitHub gist and import it into cartodb.\n\n\nGeorectifying\n\n\nGeorectifying is the process of taking an image (whether it is of a historical map, chart, airphoto, or whatever) and manipulating its geometry so that it matches a geographic projection. Think of it like this: you take your handdrawn map, and use pushpins to pin down known locations on your map to a globe. As you pin, your image stretches and warps. Traditionally, this has not been an easy thing to do, if you are new to GIS. In recent years, the curve has flattened significantly. In this exercise, we'll grab an image, upload it to the Map Warper website, and then export it as a tileset which can be used in other mapping programs.\n\n\n\n\n\n\nGet a historical map. I like the Fire Insurance plans from the \nGatineau Valley Historical Society\n; I'm sure you can find others to suit your interests.\n\n\n\n\n\n\nRight-click and save as to grab a copy. Save it somewhere easily accessible.\n\n\n\n\n\n\nGo to \nMap Warper\n and sign up for an account. Then login.\n\n\n\n\n\n\nGo to the upload screen: \n \n\n\n\n\n\n\n\n\n\n\n\n\nFill in as much of the metadata as you can. Then select your map from your computer, and upload it.\n\n\n\n\n\n\n\n\n\n\n\n\nOn the next page, click 'rectify'. \n \n\n\n\n\n\n\nPan and zoom both maps until you're sure you're looking at the same area in both. Double click in a map, select the location icon, and click on a point (location) you are sure you can match in the other window.\n\n\n\n\n\n\nClick on the other map window, select the location icon, and then click on the same point. \n\n\n\n\n\n\nThe 'add control point' button below and between both maps will light up. Click on this to confirm that this is a control point you want. Do this at least three times; the more times you can do it, the better the map warp.\n\n\n\n\n\n\nHaving selected your control points, click on 'warp image'.\n\n\n\n\n\n\n\n\n\n\n\n\nYou can now click on the 'export' panel, and get the URL for your georectified image in a few different formats. If you clicked on the KML option, a new Google Map window will open. For many webmapping applications, the Tiles (Google/OSM scheme): Tiles Based URL is what you want. You'll get a URL like this: \nhttp://mapwarper.net/maps/tile/27421/{z}/{x}/{y}.png\n Save that info. You'll need it later.\n\n\nYou have now georectified a map. Let's use that map as a base layer in \nPalladio\n.\n\n\n\n\n\n\nGo to \nPalladio\n. Hit 'start'. You will see 'Load .csv or spreadsheet'. \n\n\n\n\n\n\nIn the box, paste in your data. \nYou can progress to the next step without having any real data: just paste or type something in and hit enter so you have two lines.\n Obviously, you won't have any points on your map, but if you were having trouble with that step, this allows you to bypass it to continue on with this tutorial.\n\n\n\n\n\n\nClick the 'Map' tab at the top of the screen.\n\n\n\n\n\n\nClick 'New layer' on the right side menu.\n\n\n\n\n\n\nClick the Tiles tab and in the right side menu.\n\n\n\n\n\n\nSelect Custom tiles.\n\n\n\n\n\n\nPaste your Map Warper URL into the Tileset URL field. Your URL will look like \nhttp://mapwarper.net/maps/tile/27421/{z}/{x}/{y}.png\n.\n\n\n\n\n\n\nClick 'Add layer'.\n\n\n\n\n\n\n\n\n\n\n\n\nCongratulations! You've georectified a map, and used it as a base layer for a visualization of some point data. References these \nnotes on using a georectified map with the CartoDB service\n.\n\n\n\n\n\n\nExercise 9: Network Analysis in R\n\n\nEarlier, we took the index from the Texan Correspondence, a list of letters from so-and-so to so-and-so. When we stitch that together into a network of people connected because they exchanged letters, we end up with a shard of their social network. Networks can be queried for things like power, position, and role, and so used judiciously, we can begin to suss something of the social structures in which their history took place. Before you go any further, make sure you also take a long look at Scott Weingart's series, \nNetworks Demystified\n. Finally, \nheed our warning\n.\n\n\nThis exercise uses the R language to do our analysis, which in DH Box we access via RStudio, a programming environment. \n\n\nFor this exercise, do the following: \n\n\n\n\nPlease read \nthe introduction to R in our supporting materials\n.\n\n\nThen progress to the \nnetviz exercise\n.\n\n\n\n\n\n\nExercise 10: QGIS\n\n\nThere are many excellent tutorials around concerning how to get started with GIS. Our own library, in the \nMADGIC centre\n has tremendous resources and I would encourage you to speak with the map librarians before embarking on any \nserious\n mapping projects. In the short term, the historian \nFred Gibbs\n has an excellent series on using the open source GIS platform \nQGIS\n to make and map historical data.\n\n\nFor this exercise, do the following: \n\n\n\n\n\n\nTry Gibbs' first tutorial, \n'Making a map with QGIS'\n.\n\n\nInstalling QGIS\n\n\n\n\n\n\n\n\nDownloading geographic data\n\n\n\n\n\n\n\n\nDisplaying data in QGIS\n\n\n\n\n\n\n\n\n\n\n\n\nNext, try georectifying a historical map and adding it to your GIS following Gibbs' other tutorial \n'Using Historical maps with QGIS'\n.\n\n\n\n\n\n\nGoing Further\n\n\nThere are many tutorials at \nThe Programming Historian\n that are appropriate here. Try some under the 'data manipulation' or 'distant reading' headings.", 
            "title": "Exercises"
        }, 
        {
            "location": "/module-4/Exercises/#module-4-exercises", 
            "text": "There are  many   different tools and approaches  you could use to visualize your data, both as a preliminary pass to spot the holes and also for more formal analysis. In which case, for this module, I would like you to select  two  of these exercises which seem most germane for your final project.  You are welcome to work through more of them, of course, but I want the exercises to move your own research forward. Some of these I wrote and some are adapted from  The Macroscope ; others are adapted or used holus-bolus from scholars like  Miriam Posner ,  Fred Gibbs , and  Heather Froehlich  (and I'm grateful that they shared their materials!). Finally, you are welcome to explore the lessons and tutorials at  The Programming Historian  if they seem appropriate to what you want to do for your project.  But what if I haven't any idea of what to do for the final project?  Then read through the various tutorials for inspiration. Find something that strikes you as interesting, and then talk to me about how you might employ the ideas or concepts with regard to the Canadian war diary data.", 
            "title": "Module 4 Exercises"
        }, 
        {
            "location": "/module-4/Exercises/#things-to-install", 
            "text": "Many of these involve having to install more software on your own machine. In those exercises that involve using R and RStudio, you are welcome to install RStudio on your own machine OR to use it in DH Box. Please read  this quick introduction to R and Rstudio carefully .  If you decide to install R and RStudio on your own machine,  I would suggest you read the introductory bits from Lincoln Mullen's book-in-progress,  Computational Historical Thinking , especially the 'setup' part under 'getting started' (pay attention to the bit on installing packages and dependencies). If you spot any exercises in Mullen's book that seem relevant to your project, you may do those as an alternative to the ones here.  Alternatively , go to  Swirl  and  learn the basics of R within R .  DHNow  links to a new  Basic Text Mining in R  tutorial which is worth checking out as well.  NB It is always very important to record in your own notebooks what version of R you used for your analysis, what version of any R packages you installed and used, and so on because packages can go out of date.  In the table below I've gathered the exercises together under the headings of  Text ,  Networks ,  Maps , and  Charts . I've also added some entries that I am categorizing under  Art  The first is a series of exercises on  the sonification of data  and the second is a guide  to making twitterbots ; the third is about glitching digital imagery. These approaches can provide surprising and novel insights into history, as they move from representing history digitally to  performing  it. Visit, for instance,  the final project in an undergraduate digital history class at the University of Saskatchewan by Daniel Ruten . Daniel translated simple wordclouds of a First World War diary into a profound auditory performance. I would be very interested indeed to see if any final projects in HIST3814o gave sonification or twitterbots or glitch a try.  The exercises in this module are covered in the chart below:     Texts  Networks  Maps  Charts  Art      Topic Modeling Tool  Network analysis in Gephi  Simple mapping   georectifying  Quick charts using RAW  Sonification    Topic Modeling in R  Converting 2-mode to 1-mode  QGIS (tutorials by Fred Gibbs)   Twitterbots    Text Analysis with OverviewProject  Network Analysis in R  Geoparsing with Python   Glitching Photos    Corpus Linguistics with AntConc  Network Analysis in Cytoscape  Palladio with Posner      Text Analysis with Voyant  Choose your own adventure  Leaflet.js Maps", 
            "title": "Things to install?"
        }, 
        {
            "location": "/module-4/Exercises/#exercise-1-network-visualization", 
            "text": "This exercise uses the open source program  Gephi  which you install on your own computer. If you'd rather not install anything, please see  Network Analysis in R  instead.  Recall that the index of the collected letters of the Republic of Texas was just a list of letters from so-and-so to so-and-so. We haven't looked at the content of those letters, but the shape of network   the meta data of that correspondence   can be revealing (remember  Paul Revere! ) When we stitch that together into a network of people connected because they exchanged letters, we end up with a shard of their social network. Networks can be queried for things like power, position, and role, and so used judiciously, we can begin to suss something of the social structures in which their history took place. I would recommend that you also take a long look at Scott Weingart's series,  Networks Demystified . Finally,  heed our warning .  For this exercise, do the following:    Transform your Texan Correspondence data into a network, which you will then visualize with the open source program  Gephi . The detailed instructions are  in our supporting materials .", 
            "title": "Exercise 1: Network Visualization"
        }, 
        {
            "location": "/module-4/Exercises/#exercise-2-topic-modeling-tool", 
            "text": "In this exercise you will use the  Topic Modeling Tool  to create a simple topic model and a webpage that allows you to browse the results.    Download the  Topic Modeling Tool from GitHub .    Make sure you have some content on your own machine; the Colonial Newspaper Database is a handy corpus. (Created by Melodee Beals, it's a series of late 18th, early 19th century cleanly transcribed newspaper articles from Scotland and Northern England; You can grab  my copy from GitHub ). Or perhaps you might move your copy of the Canadian war diary out of DH Box onto your computer.     At the command prompt in DH Box, type  $ ls  to make sure you can see your Canadian war diary text folder (ie. you can't zip a folder from the command line if you are  in  that folder, so  $ cd  out of it if necessary).     Assuming your files are in  war-diary-text , zip the folder up with this command,  $ zip -r wardiaryfiles.zip war-diary-text .     Use the File Manager to download the zip file.     Unzip the folder on your machine.         Double-click on the file you downloaded in Step 1. This will open a java-based graphical user interface with one of the most common topic modeling approaches, 'Latent Dirichlet Allocation'.    Set the input to be the Colonial Newspaper Database  or  the Canadian war diary.    Set the output to be somewhere neat and tidy on your computer.    Set the number of topics you'd like to model.    Click 'train topics' to run the algorithm.    When it finishes, go to the folder you selected for output, and find the file  all_topics.html  in the  output_html  folder.     Click on  all_topics.html . You now have a browser-based way of navigating your topics and documents. In the  output_csv  folder created, you will find the same information as CSV, which you could then input into a spreadsheet for other kinds of visualizations (which we'll talk about in class.)         Make a note in your open notebook about your process and your observations. How does reading this material in this way change/challenge/or focus your understanding of the material?", 
            "title": "Exercise 2: Topic Modeling Tool"
        }, 
        {
            "location": "/module-4/Exercises/#exercise-3-topic-modeling-in-r", 
            "text": "Exercise 2 was quite a simple way to do topic modeling. In this exercise, we are going to use a package for the R statistical language called 'Mallet' to do our topic modeling. One way isn't necessarily better than the other, although doing our analysis within R allows the potential for extending the analysis or combining it with other data. First, read  this introduction to R  so what follows isn't a complete shock!  For this exercise, do  ONE  of the following:    Guidance for doing this in RStudio  in the DH Box  Guidance for doing this in RStudio  installed on your own computer", 
            "title": "Exercise 3: Topic Modeling in R"
        }, 
        {
            "location": "/module-4/Exercises/#exercise-4-text-analysis-with-overview", 
            "text": "In this exercise, we're going to look at the Colonial Newspaper Database again, but this time using a tool called 'Overview'. Overview uses a different approach that the topic models we've been discussing. In essence, it looks at word frequencies and their distributions within a document, and within a corpus, to organize the documents into folders of progressively similar word use.   You can download Overview to run on your own machine, but for our purposes, the hosted version on the  Overview Docs website  is sufficient. Go to that page, watch the video, create an account, and then log in ( you must create an account to use overview ). (More help about how Overview works  may be found on their blog , including helpful videos.)  Once you're inside, click 'import from a CSV file', and upload the  CND.csv  (which you can download and save to your own machine from  my GitHub   - right-click and save as.   On the 'UPLOAD A CSV FILE' page in Overview click 'browse' and select the  CND.csv . It will give you a preview. There are a number of options here   you can tell Overview which words to ignore, and which words to give added importance to. What words will you select? Make a note in your notebook.   Hit 'upload'.  A new page appears, called  YOUR DOCUMENT SETS . Click on the one you just uploaded. A file folder tree showing documents of progressively greater similarity will open; on the right hand side will be the list of documents within each box (the box in question will be greyed out when you click on it, so you know where you are). You can search for words in your document, and Overview will tell you where they are; you can tag documents that you find interesting. The Overview system allows you to jump between a distant, macroscopic view and a close, document level view.   Jump back and forth, see what you can find. For suggestions about how to use Overview effectively, try  their blog . Make notes about what you observe in your notebook. Also, you can export your tagged document set from Overview, so that you could visualize the patterns of tagging in a spreadsheet (for instance).        Going further:  Do you see how you could upload your documents that you collected during Module 2?", 
            "title": "Exercise 4: Text Analysis with Overview"
        }, 
        {
            "location": "/module-4/Exercises/#exercise-5-corpus-linguistics-with-antconc", 
            "text": "Heather Froelich has put together an excellent step-by-step with using AntConc for exploring textual patterns within, and across, corpora of texts. Work your way through her  tutorial .  Can you get our example materials (from the Colonial Newspaper Database) into AntConc?  Our instructions in The Macroscope might help you  to split the CSV into individual txt files. Alternatively, do you have any materials of your own, already collected? Feed them into AntConc. What patterns do you see? What if you compare your materials against other corpora of texts?  For your information,  CoRD has a collection of corpora that you can explore .", 
            "title": "Exercise 5: Corpus Linguistics with AntConc"
        }, 
        {
            "location": "/module-4/Exercises/#exercise-6-text-analysis-with-voyant", 
            "text": "In  Module 2 , if you recall, we worked through how to transform XML using stylesheets. Melodee Beals used a  stylesheet   to transform her database into a series of individual txt files. In the exercises above, a transformer was used to make the database into a single CSV file. In this exercise, we are going to use  Voyant Tools  to visualize patterns in word use in the database. Voyant can read either a CSV  or  text files. The advantage of uploading a folder of text files is that, if the files are in chronological order, Voyant's default visualizations will also be arranged in chronological order and thus we can see change over time.    Go to  Voyant Tools . Paste the following URL to the CSV of the CND database:  https://raw.githubusercontent.com/shawngraham/exercise/gh-pages/CND.csv .    Now, open a new browser window, and go to this  colonial newspaper file on Voyant tools .  Do you see the difference? In the latter window, the individual articles have been uploaded individually, and thus are treated as individual documents in chronological order.    Explore the corpus, comparing terms over time, looking at keywords in context, and using the RezoViz tool to create a graph where people, places, and organizations that appear in the same documents (and across documents) are connected (you can find 'rezoviz' under the cogwheel icon at the top right of the panel).     Google these terms and tools for what they mean and how others have used them. You can embed any of the tools in your blogs by using the 'save' icon and getting the iframe or embed code. You can apply  stopwords  by clicking on the cogwheel in any of the different tools, and selecting stopwords.     Apply the stopwords globally, and you'll only have to do this once! What patterns do you see? What do different tools highlight? Which ones are useful? What patterns do you see that strike you as interesting? Note this all down.         Going further:  Upload materials you collected in module 2 and explore them.", 
            "title": "Exercise 6: Text Analysis with Voyant"
        }, 
        {
            "location": "/module-4/Exercises/#exercise-7-raw", 
            "text": "", 
            "title": "Exercise 7: RAW"
        }, 
        {
            "location": "/module-4/Exercises/#quick-charts-using-raw", 
            "text": "A quick chart can be a handy thing to have. Google spreadsheets, Microsoft Excel, and a host of other programs can make excellent charts quickly with their wizard functions. Never hesitate to turn to these. However, they are not always good with non-numeric data. In  Module 3 , you used the Stanford Named Entity Recognizer to extract place names from a text. After some further munging with regex, you might have ended up with a CSV that looks like  the Texas CSV . Can we do a quick visualization of this information? One useful tool is  RAW .    Open RAW in a new window.  Copy the table of data of places mentioned in the Texan correspondence, and paste it into the data input box at the top of the RAW screen.   Oh no, an error!", 
            "title": "Quick Charts Using RAW"
        }, 
        {
            "location": "/module-4/Exercises/#a-quick-data-munge", 
            "text": "You should get an error message, to the effect that you need to check 'line 2'. What's gone wrong? RAW has checked the number of values you have in that row, and compared it to the number of columns in row 1 (which contains all the column names). It sees that the two don't match. What we need to do is add a default null value in those cells.    Go to  Google Sheets .    Click the 'Go to Google Sheets' button, and then click on the big green plus sign to start a new sheet.     Paste the following into the top-left cell (cell A1):  =IMPORTDATA(\"https://raw.githubusercontent.com/hist3907b-winter2015/module4-holes/master/texas.csv\")  Pretty neat, eh? Now, here's the thing: even though your sheet  looks  like it is filled with information, it's not (at least, as far as the script we are about to run is concerned). That is to say, the sheet itself only has one cell of data, and that one cell is grabbing info from elsewhere on the web and dynamically filling the sheet. The script we're going to run works only on static values (more or less).    Place your cursor in cell B1. On a Mac, hit  shift+cmnd+downarrow . On a Windows machine, hit  shift+ctrl+downarrow . Then on Mac  shift+cmnd+rightarrow , on Windows  shift+ctrl+rightarrow .    Copy all of that data ( cmnd+c  or  ctrl+c ).    Under 'Edit' select 'paste special' -  'paste VALUES only'.  The formula you put in cell A1 now says  #REF! . You can delete this now. This mucking about is necessary so that the add on script we are about to run will work.    We now need to fill those empty values. In the tool bar, click  add ons  -   get add ons . Search for  blanks . You want to add  Blank Detector .    Now, click somewhere in your data. On Mac, hit  cmnd+a . On Windows, hit  ctrl+a . This highlights all of your data.     Click  Add ons  -   blank detector  -   detect cells . A dialogue panel will open on the right hand side of your screen.     Click the button beside  set value  and type in  null .     Hit  run . All of the blank cells will fill with the word  null .     Delete column A (which formerly had record numbers, but is now just filled with the word  null . We don't need it).  If you get the error, 'run exceeded maximum time'  just hit the run button again. This script might take a few minutes.  You can now copy and paste your table of data into the data input box in RAW, and you should get the green thumbs up saying x records have been successfully parsed!  NB The Blank Detector add on may take a long time. Try it out on the CSV. You can copy the parsed data from  my Google Sheets file .", 
            "title": "A quick data munge"
        }, 
        {
            "location": "/module-4/Exercises/#playing-with-raw", 
            "text": "RAW takes your data, and depending on your choices, passes it into chart templates built on the D3.js code library. D3.js is a powerful library for making all sorts of charts (including interactive ones). If this sort of thing interests you, you can follow the tutorials in  Elijah Meeks' excellent new book .    With your data pasted in, you can now experiment with a number of different visualizations that are all built on the D3.js code library.      Try the \u2018alluvial\u2019 diagram.  Pick place1 and place2 as your dimensions   you click and drag the green boxes under 'map your data' into the 'steps' box.     Leave the 'size' box empty.     Under 'customize your visualization' you can click inside the 'width' box to make the diagram wider and more legible.  Does anything jump out?     Try place3 and place 4. Try place1, place2, place3, and place4 in a single alluvial diagram.   When we look at the original letters, we see that the writer often identified the town in which he was writing, and the town of the addressee. Why choose the third and fourth places? Perhaps it makes sense, for a given research question, to assume that with the pleasantries out of the way the writers will discuss the places important to their message. Experiment! This is one of the joys of working with data, experimenting to see how you can deform your materials to see them in a new light.    You can export your visualization under the 'download' box at the bottom of the RAW page   your choices are as a simple raster image (PNG), a vector image (SVG) or a data representation (json).", 
            "title": "Playing with RAW"
        }, 
        {
            "location": "/module-4/Exercises/#exercise-8-simple-mapping-and-georectifying", 
            "text": "In this exercise, you will find a historical map online, upload a copy to a mapwarper service, georectify it, and then display the map online, via a hosted service like CartoDB, and also through a map you will build yourself using Leaflet.js. Finally, we will also convert CSV to geojson using  Mapbox's geojson converter , and we'll map that as a GitHub gist. We'll also grab a geojson file hosted on GitHub gist and import it into cartodb.", 
            "title": "Exercise 8: Simple Mapping and Georectifying"
        }, 
        {
            "location": "/module-4/Exercises/#georectifying", 
            "text": "Georectifying is the process of taking an image (whether it is of a historical map, chart, airphoto, or whatever) and manipulating its geometry so that it matches a geographic projection. Think of it like this: you take your handdrawn map, and use pushpins to pin down known locations on your map to a globe. As you pin, your image stretches and warps. Traditionally, this has not been an easy thing to do, if you are new to GIS. In recent years, the curve has flattened significantly. In this exercise, we'll grab an image, upload it to the Map Warper website, and then export it as a tileset which can be used in other mapping programs.    Get a historical map. I like the Fire Insurance plans from the  Gatineau Valley Historical Society ; I'm sure you can find others to suit your interests.    Right-click and save as to grab a copy. Save it somewhere easily accessible.    Go to  Map Warper  and sign up for an account. Then login.    Go to the upload screen:          Fill in as much of the metadata as you can. Then select your map from your computer, and upload it.       On the next page, click 'rectify'.       Pan and zoom both maps until you're sure you're looking at the same area in both. Double click in a map, select the location icon, and click on a point (location) you are sure you can match in the other window.    Click on the other map window, select the location icon, and then click on the same point.     The 'add control point' button below and between both maps will light up. Click on this to confirm that this is a control point you want. Do this at least three times; the more times you can do it, the better the map warp.    Having selected your control points, click on 'warp image'.       You can now click on the 'export' panel, and get the URL for your georectified image in a few different formats. If you clicked on the KML option, a new Google Map window will open. For many webmapping applications, the Tiles (Google/OSM scheme): Tiles Based URL is what you want. You'll get a URL like this:  http://mapwarper.net/maps/tile/27421/{z}/{x}/{y}.png  Save that info. You'll need it later.  You have now georectified a map. Let's use that map as a base layer in  Palladio .    Go to  Palladio . Hit 'start'. You will see 'Load .csv or spreadsheet'.     In the box, paste in your data.  You can progress to the next step without having any real data: just paste or type something in and hit enter so you have two lines.  Obviously, you won't have any points on your map, but if you were having trouble with that step, this allows you to bypass it to continue on with this tutorial.    Click the 'Map' tab at the top of the screen.    Click 'New layer' on the right side menu.    Click the Tiles tab and in the right side menu.    Select Custom tiles.    Paste your Map Warper URL into the Tileset URL field. Your URL will look like  http://mapwarper.net/maps/tile/27421/{z}/{x}/{y}.png .    Click 'Add layer'.       Congratulations! You've georectified a map, and used it as a base layer for a visualization of some point data. References these  notes on using a georectified map with the CartoDB service .", 
            "title": "Georectifying"
        }, 
        {
            "location": "/module-4/Exercises/#exercise-9-network-analysis-in-r", 
            "text": "Earlier, we took the index from the Texan Correspondence, a list of letters from so-and-so to so-and-so. When we stitch that together into a network of people connected because they exchanged letters, we end up with a shard of their social network. Networks can be queried for things like power, position, and role, and so used judiciously, we can begin to suss something of the social structures in which their history took place. Before you go any further, make sure you also take a long look at Scott Weingart's series,  Networks Demystified . Finally,  heed our warning .  This exercise uses the R language to do our analysis, which in DH Box we access via RStudio, a programming environment.   For this exercise, do the following:    Please read  the introduction to R in our supporting materials .  Then progress to the  netviz exercise .", 
            "title": "Exercise 9: Network Analysis in R"
        }, 
        {
            "location": "/module-4/Exercises/#exercise-10-qgis", 
            "text": "There are many excellent tutorials around concerning how to get started with GIS. Our own library, in the  MADGIC centre  has tremendous resources and I would encourage you to speak with the map librarians before embarking on any  serious  mapping projects. In the short term, the historian  Fred Gibbs  has an excellent series on using the open source GIS platform  QGIS  to make and map historical data.  For this exercise, do the following:     Try Gibbs' first tutorial,  'Making a map with QGIS' .  Installing QGIS     Downloading geographic data     Displaying data in QGIS       Next, try georectifying a historical map and adding it to your GIS following Gibbs' other tutorial  'Using Historical maps with QGIS' .", 
            "title": "Exercise 10: QGIS"
        }, 
        {
            "location": "/module-4/Exercises/#going-further", 
            "text": "There are many tutorials at  The Programming Historian  that are appropriate here. Try some under the 'data manipulation' or 'distant reading' headings.", 
            "title": "Going Further"
        }, 
        {
            "location": "/module-5/Humanities Visualization/", 
            "text": "Humanities Visualization Aug 7 - Aug 13\n\n\nIn this week, I want you to focus on making your final project. You do not need to read or annotate any pieces this week, nor to work through the exercises that are included in module 5. They are there for you to dip into as you finish your project. YOU DO have to write a blog post explaining what you've been up to this week.\n\n\nConcepts\n\n\nIn this module, we will be exploring the nuts and bolts of visualization. However, we will also be thinking about what it means to visualize 'data' from a \nhumanities\n perspective. Following Drucker, we're going to imagine what it means to think about our \ndata\n not as things received (ie. empirically observed) but rather as \ncapta\n, as things taken/transformed.\n\n\nIt means visualizing and dealing with the intepretive process that got us to this point. What's more, we need to be aware of 'screen essentialism' and how it might be blinkering us to the possibilities of what humanities visualization could be. Finally, we need to be aware of the ways our digital 'templates' that we use reproduce ways of thinking and being that are antithetical to humanities' perspectives.\n\n\nThe following are worth reading on these issues:\n\n\n\n\nDrucker, J. \"Humanities approaches to graphical display\". \nDHQ\n 2011.5 \nhttp://www.digitalhumanities.org/dhq/vol/5/1/000091/000091.html\n\n\nWilliams, G. \"Disability, Universal Design, and the Digital Humanities\" \nDebates in the Digital Humanities\n 2012. \nhttp://dhdebates.gc.cuny.edu/debates/text/44\n\n\nOwens, T. \"Discovery and Justification Are Different: Notes on Science-ing the Humanities\" \nhttp://www.trevorowens.org/2012/11/discovery-and-justification-are-different-notes-on-sciencing-the-humanities/\n\n\nOwens, T. \"Defining Data for Humanists: Text, Artifact, Information, or Evidence?\" \nJournal of Digital Humanities\n 2011 1.1. \nhttp://journalofdigitalhumanities.org/1-1/defining-data-for-humanists-by-trevor-owens/\n\n\nWatters, Audrey. \n\"Men (Still) Explain Technology to Me: Gender and Education Technology\" \nHackeducation\n\n\n\n\nI also have a \nnumber of pieces of my own archaeological work\n that I think provide examples of how humanistic visualization can be a driver of interpretation and understanding. For instance, one thing I am currently working on is the possibility for \nsound\n \nto be a better representation of humanistic data\n. Oh, and by the way: maps are great, but sometimes, maps of ideas are even better; check out this \nlandscape of Last.fm Folksonomy (pdf opens in new tab)\n. (If you have any facility with Python, you might like \nthis library that allows you to generate similar self-organizing maps\n). Since Python 3 is installed in your DHBox, you're all set! (By the way, I find \ndabapps piece on python\n very helpful anytime I set out to do any Python on my own computer.)\n\n\nWhat you need to do this week\n\n\n\n\nWork on your project. You have until midnight August 16th to submit it. See the \nFinal Project requirements\n. Remember that all supporting files need to be in their own GitHub repository (it is not necessary to share the Equity files, unless you have created some sort of dataset from them), while the final project itself has to be mounted on your own domain.\n\n\nTalk to me, talk to each other in Slack. Feel free to collaborate, but keep a record of who does what and how much.\n\n\nIf you missed completing a module, now might also be a good time to finish it (see \n2.4 of the course manual\n)\n\n\nUse the materials in this module to help make your project.\n\n\nWrite a blog post describing what you've been up to THIS WEEK re your project (your successes, your failures, the help you may have found/received), and link to your 'faillog' (ie, the notes on your process that you upload to your GitHub. If you did any of the exercises, mention that).\n\n\n\n\nReadings\n\n\nNo formal readings are assigned this week. Below you can watch a video of me sonifying a topic model of John Adam's mind.", 
            "title": "Communicating your Findings"
        }, 
        {
            "location": "/module-5/Humanities Visualization/#humanities-visualization-aug-7-aug-13", 
            "text": "In this week, I want you to focus on making your final project. You do not need to read or annotate any pieces this week, nor to work through the exercises that are included in module 5. They are there for you to dip into as you finish your project. YOU DO have to write a blog post explaining what you've been up to this week.", 
            "title": "Humanities Visualization Aug 7 - Aug 13"
        }, 
        {
            "location": "/module-5/Humanities Visualization/#concepts", 
            "text": "In this module, we will be exploring the nuts and bolts of visualization. However, we will also be thinking about what it means to visualize 'data' from a  humanities  perspective. Following Drucker, we're going to imagine what it means to think about our  data  not as things received (ie. empirically observed) but rather as  capta , as things taken/transformed.  It means visualizing and dealing with the intepretive process that got us to this point. What's more, we need to be aware of 'screen essentialism' and how it might be blinkering us to the possibilities of what humanities visualization could be. Finally, we need to be aware of the ways our digital 'templates' that we use reproduce ways of thinking and being that are antithetical to humanities' perspectives.  The following are worth reading on these issues:   Drucker, J. \"Humanities approaches to graphical display\".  DHQ  2011.5  http://www.digitalhumanities.org/dhq/vol/5/1/000091/000091.html  Williams, G. \"Disability, Universal Design, and the Digital Humanities\"  Debates in the Digital Humanities  2012.  http://dhdebates.gc.cuny.edu/debates/text/44  Owens, T. \"Discovery and Justification Are Different: Notes on Science-ing the Humanities\"  http://www.trevorowens.org/2012/11/discovery-and-justification-are-different-notes-on-sciencing-the-humanities/  Owens, T. \"Defining Data for Humanists: Text, Artifact, Information, or Evidence?\"  Journal of Digital Humanities  2011 1.1.  http://journalofdigitalhumanities.org/1-1/defining-data-for-humanists-by-trevor-owens/  Watters, Audrey.  \"Men (Still) Explain Technology to Me: Gender and Education Technology\"  Hackeducation   I also have a  number of pieces of my own archaeological work  that I think provide examples of how humanistic visualization can be a driver of interpretation and understanding. For instance, one thing I am currently working on is the possibility for  sound   to be a better representation of humanistic data . Oh, and by the way: maps are great, but sometimes, maps of ideas are even better; check out this  landscape of Last.fm Folksonomy (pdf opens in new tab) . (If you have any facility with Python, you might like  this library that allows you to generate similar self-organizing maps ). Since Python 3 is installed in your DHBox, you're all set! (By the way, I find  dabapps piece on python  very helpful anytime I set out to do any Python on my own computer.)", 
            "title": "Concepts"
        }, 
        {
            "location": "/module-5/Humanities Visualization/#what-you-need-to-do-this-week", 
            "text": "Work on your project. You have until midnight August 16th to submit it. See the  Final Project requirements . Remember that all supporting files need to be in their own GitHub repository (it is not necessary to share the Equity files, unless you have created some sort of dataset from them), while the final project itself has to be mounted on your own domain.  Talk to me, talk to each other in Slack. Feel free to collaborate, but keep a record of who does what and how much.  If you missed completing a module, now might also be a good time to finish it (see  2.4 of the course manual )  Use the materials in this module to help make your project.  Write a blog post describing what you've been up to THIS WEEK re your project (your successes, your failures, the help you may have found/received), and link to your 'faillog' (ie, the notes on your process that you upload to your GitHub. If you did any of the exercises, mention that).", 
            "title": "What you need to do this week"
        }, 
        {
            "location": "/module-5/Humanities Visualization/#readings", 
            "text": "No formal readings are assigned this week. Below you can watch a video of me sonifying a topic model of John Adam's mind.", 
            "title": "Readings"
        }, 
        {
            "location": "/module-5/Exercises/", 
            "text": "Module 5 Exercises\n\n\nIn this module, you will begin with an introduction to web design concepts and finish by creating your own online exhibit. While we will not explore every possibility here, I do not want to leave you thinking that all digital history work is necessarily text based. To that end, if you are looking for a challenge or for exposure to something rather different, I suggest you at least bookmark my \nseries of tutorials on augmented reality, games, and 3d models for history and archaeology\n.\n\n\n\n\n'Accidental Renaissance: the photos that look like Italian paintings'\n.\n\n\nThe exercises in this module cover:\n\n\n\n\nDesign with colour and font\n\n\nLayout \n\n\nManipulating graphics\n\n\nCreating digital exhibits\n\n\n\n\n\n\nExercise 1: Layout\n\n\n'Layout' can mean different things in different contexts. A general overview on issues in layout is covered by \n'What makes a design great, Lynda.com\n and \n'Exploring principles of layout and composition'\n. The \nSlideshare on effective layouts\n gives you a sense of things to watch out for as well.\n\n\nFor academic posters in particular, consider \nthese suggestions from the APA\n.\n\n\nIn essence, good layout makes your argument legible, intuitive, and reinforces the rhetorical points you are trying to make. You should take into account 'principles of universal design' \n consider design issues with \nPowerPoint\n and \nwebsites\n (although some of the technical solutions proposed in those two documents are a bit out of date, the general principles hold!)\n\n\nIn this exercise, you will design a new poster \nor\n modify an existing poster. You can use either Inkscape or PowerPoint.\n\n\nInkscape\n\n\nView the \nInkscape exercise in our supporting materials for a gentle introduction to Inkscape\n.\n\n\n\n\nDownload one of the scientific poster templates from \nUgo Sangiorgi\n (These are developed from \nFelix Breuer's blog post\n; note his discussion on design). \n\n\nOpen it in Inkscape. \n\n\nMake notes in your open notebook \nfrom the point of view of layout:\n what elements of the design work? What aren't working? How would you repurpose this poster to fit the requirements of the assessment exercise (\nremember, details here\n)?\n\n\nVisit Inkscape's website for help with the basics of Inkscape\n. \n\n\nModify the poster, and upload the SVG, PDF, or PNG version to your repository.\n\n\n\n\nPowerPoint\n\n\nThere's a lot of information out there on making posters with PowerPoint. \n\n\n\n\nRead \nparts 1, 2, and 3 of the Make Signs tutorial\n and then consider \nColin Purrington's advice\n. Once you've read and digested, pick a poster from \nPimp My Poster\n that sticks out to you. \n\n\nMake notes in your open notebook: \nfrom the point of view of layout\n what elements of the design work? What aren't working? How would you repurpose this posert to fit the requirements ot he assessment exercises (\nremember to visit GitHub for the details\n)? \n\n\nGrab a template from \nfrom Colin Purrington's\n, and use it to prototype a poster that works. \n\n\nUpload the poster as a PDF to your repository.\n\n\n\n\nIf you want to explore layout in the context of webpage creation, I would point you to the the roster of lessons at \nCodeacademy\n. Same instructions: find an existing site that you will consider from the point of view of what works, what doesn't, and use that reflection to guide the construction of your own.\n\n\n\n\nExercise 2: Typography\n\n\nTypographic plays an important role in visual communication. It can do everything from reduce eyestrain (do a search for 'easiest fonts to read on screen') to communicate power and authority. Is it any wonder that strong bold capitals come to be known as 'Roman'? But first: \nare you a comic sans criminal?\n\n\nNB\n Serif fonts are generally not as accessible as sans-serif fonts. Older screens and monitors can have a difficult time rendering serif fonts. Sans-serif fonts can be more readable across different screens. \nHowever\n, this does not mean you should throw away serif fonts \n many serif fonts can be quite accessible and add to your design goals if used properly. For more information on accessible fonts, check out \nWebAIM's article on fonts\n.\n\n\nIn this exercise,\n\n\n\n\nI want you to read and understand the section on \nfont choices from the Owl at Purdue\n.\n\n\nThen, play some rounds of \nTypeconnection\n. Pay attention to why \n or why not \n various pairings work.\n\n\nThen, I want to consider the document you will be preparing for me that accounts for your learning in this course \n the document where you choose your best exercises from the modules and explain how your approach to history, data, the digital, etc, has changed over this course. What typographic pair would work best for you?\n\n\nFinally, you'll make a webpage that uses those fonts and explains why they work.\n\n\n\n\nThe first part of this exercise then is to find a pair of fonts and to understand why they work best for you. \n\n\n\n\n\n\nRead the materials above, and once you're done with the \nTypeconnection site\n, go to \nGoogle Fonts\n and search for a pair that are \nsuitable for your purpose\n. \n\n\n\n\n\n\nWhen you find a font you like, click the 'add to collection' button.\n\n\n\n\n\n\nAt the bottom of the screen, you'll see a link for 'use'. Click on this \n Google will ask you if you want to use any of the variants of the font. Tick off accordingly. \n\n\n\n\n\n\nDo you see the embed code that Google provides, and the code to integrate the font into your CSS (stylesheet)? Leave this window open \n we're going to use it in a moment.\n\n\n\n\n\n\nMake a new repository for this exercise. \n\n\n\n\n\n\nIn your repository, click the button beside 'branch'. \n\n\n\n\n\n\nIn the search box that opens, type in \ngh-pages\n. This will create a version of your repository that can be served as a website. You're now in the gh-pages branch.\n\n\n\n\n\n\nClick on the + sign beside the repository name. This will create a new file in your gh-branch of your repository. Call it \nmyfontchoice.html\n \n- the .html is important to specify; otherwise your browser will not know how to render your page.\n\n\n\n\n\n\nYou now need to put some HTML in there. I've written a simple webpage that will use two fonts from Google Fonts, and then applies the font to my text depending on whether or not it is a header, which you specify like this: \nh1\n this is a header in between header tags \n/h1\n or a paragraph, which you specify like this: \np\nblah blah blah a paragraph of text blah blah blah\n/p\n. Right-click and open in a new tab \nmy webpage on GitHub\n. Copy the HTML into your new HTML document. Commit your changes (ie. save).\n\n\n\n\n\n\n\n\n\n\n\n\nLet's see what we've got. To see the website version of your gh-pages branch, you go to \nyourusername\n.github.io/\nreponame\n/myfontchoice.html\n \n- ie. the final part of the URL is the name of the document in your repo. Do that now. You should see a simple webpage, with two very distinctive fonts.\n\n\n\n\n\n\nNow let's slide your font choices into the HTML. Go to your HTML page in your gh-pages repo (ie, not the \ngithub.io\n version, but the \ngithub.com/\nyourusername\n/\nrepo\n/myfontchoice.html\n version. \n\n\n\n\n\n\nHit the edit button. Look closely at line 6. Do you see my two fonts? Do you see the pipe character (the \n|\n symbol) between them? That tells Google you want \nboth\n fonts. \n\n\n\n\n\n\nNavigate to the \nGoogle Fonts page\n again to grab the exact name for your fonts (uppercase letters make a difference!) and paste them into line 5 appropriately.\n\n\n\n\n\n\nLines 8 and 14 specify which font to use for headers, and which font to use for body text. Change appropriately.\n\n\n\n\n\n\nChange my silly text for a paragraph that explains what the fonts are, and why they work for this purpose. Commit your changes.\n\n\n\n\n\n\nGo to the \ngithub.io\n version of your repository (if you forget the address, you can find it under the 'Settings' tab on your normal repository page when you're in the gh-pages branch). \n\n\n\n\n\n\nReload the page several times to clear the older version you've already looked at and to replace it with your version. Ta da! Not only have you thought carefully about typography and fonts, you've also learned how to serve a webpage from a GitHub repository.\n\n\n\n\n\n\n\n\n\n\n\n\nHint\n You could use this as the basics of your submission for assessment, for your exercises. Build a webpage, link to your evidence, embed your images. For basic HTML \nW3schools has a really good guide to keep around\n.\n\n\nGoing further with GitHub pages\n\n\nNow that you have learned the basics of creating a simple website using GitHub, you have the ability to go further. Using the DH Box, you can build a simple website through GitHub pages using the MkDocs static site generator. Navigate to the \nstatic site generator exercise using GitHub pages in the supporting materials\n.\n\n\n\n\nExercise 3: Colour\n\n\nThere's a lot of bumpf out there on the 'pyschology of colour'. Google it to see what I mean (\nYouth Designer has a good example\n). A lot of what you see here isn't so much psychology as it is associations (and indeed, western associations, at that). Associations are important of course; you don't want to undermine your message by making some unfortunate connections.\n\n\nIn this exercise, I want you to take the webpage you developed in the previous exercise, and make two versions of it: \n\n\n\n\n\n\nOne where the colours \nsupport\n the broader message of the page (a prototype for your exercises assessment piece, remember?)\n\n\nAnd the other where the colours \nundermine\n that broader message. \n\n\n\n\n\n\n\n\nExplain, in both cases, how your colour choices enhance and/or detract.\n\n\nAlternatively, you can make a one page slide in PowerPoint doing the same thing.\n\n\n\n\n\n\n\n\n\n\nResources\n\n\nBelow is a graphic and a movie to help with the theoretical side of things:\n\n\n\n\nUnderstanding the rules of color, Lynda.com\n\n\nTo learn how to style your webpage appropriately, you can \nfollow this tutorial on CSS from codeacademy.com\n.\n\n\nBelow is a graphic that presents how colours are perceived in different cultures:\n\n\n\n\n\n\nExercise 4: Creating an online exhibit with Omeka\n\n\nOmeka\n is an open source Content Management System (CMS) for sharing digital collections and creating media-rich online exhibits. A CMS is a web program that manages content including graphics, text, videos, and more for a web site. A CMS allows users to upload their content via a Graphical User Interface (GUI) and organizes it into appropriate folder structures. Traditional websites are programmed from the ground up \n everything from structure, to styling, to creating folders and specifiying where multi-media content is located. \n\n\nThe main advantage of using a CMS is that it requires little to no actual, background-level programming. That being said, anyone can edit a CMS's source code files, add their own, or modify them to fit their needs. Most CMSs are built with \nPHP\n, a web scripting program used to dynamically create content. However, most CMSs are also accompanied by massive native and third party plugin libraries that allow users to modify their website without touching any source code. Examples of CMSs include Wordpress, Drupal, Grav, Known, Joomla, and many more. \n\n\nUnlike many popular CMSs, Omeka is not a blogging platform. Omeka is used to create online exhibits. Today we are going to create a simple online exhibit. \n\n\nOur overall steps are as follows:\n\n\n\n\nCreate a subdomain of your website where Omeka will live. This will allow you to host Omeka as an entity of its own on your website, keeping your main website separate.\n\n\nInstall Omeka through cPanel on Reclaim Hosting.\n\n\nCreate an online exhibit with several historical items.\n\n\nChange the style of our Omeka site by modifying the CSS of our theme.  \n\n\n\n\nCreating a subdomain\n\n\n\n\n\n\nLogin to your Reclaim Hosting account through the \nReclaim Hosting login page\n.\n\n\n\n\n\n\nIn the client area, select the cPanel tab.\n\n\n\n\n\n\n\n\nUnder the Domains section, select Subdomains. Typically a subdomain divides your overall website into distinct portions. For instance, you could have your landing page at \nmysite.com\n, your blog at \nblog.mysite.com\n, and your class work at \nhist3814.mysite.com\n. That way each subdomain could have different styles and serve different purposes, but still be connected to your overall domain. You can add as many subdomains to your site as you'd like. \n\n\nSubdomains are essentially just an easier way of organizing your files and showing distinct differences from your main domain. You could technically also do \nmysite.com/blog\n or \nmysite.com/hist3814\n. In that case, however, it seems expected they would all be styled similarly to \nmysite.com\n. Furthermore, it might cause difficulty to have, for example, Wordpress running your mysite.com and Omeka running \nmysite.com/Omeka\n. It's best to stick with your free subdomains.\n\n\n\n\n\n\n\n\nEnter the name of your online exhibit in the Subdomain field. You could simply put \nomeka\n, \nomeka-test\n, etc. or get more descriptive and say \ngatineau-fire-maps\n. Choose a title relevant to your Omeka site's purpose.\n\n\n\n\n\n\nLeave the Domain field set to your domain.\n\n\n\n\n\n\nThe Document Root will be automatically created (an advantage of adding a subdomain!). I would not recommend changing this, but if you want to further organize your folder structure, you are able to change the location.\n\n\n\n\n\n\n\n\nClick the Create button. It will take a moment, but your new subdomain will be approved. \n\n\n\n\n\n\n\n\nNB Since the subdomain has nothing in it, if you navigate to it now, you will see 'Index of/' showing the file structure. This is normal.\n\n\nInstalling Omeka\n\n\n\n\n\n\nNavigate back to your cPanel home page. \n\n\n\n\n\n\nUnder the Applications section, select Omeka. \n\n\n\n\n\n\n\n\nOn the right hand side, click the '+ install this application' button. \n\n\n\n\n\n\n\n\nUnder the Location section, select the subdomain you created in the previous instructions. Do not choose your main \nmysite.com\n domain. Choose your \nomeka.mysite.com\n domain or your variant of it. You can choose the \nhttps\n or \nhttp\n version. Note, however, that \nhttps\n is more secure.\n\n\n\n\n\n\nDelete \ncms\n under Directory (Optional). We want Omeka installed to the root of our subdomain, not in a folder called \ncms\n. If you were installing Omeka to your main site, then you'd want a subdirectory called \ncms\n.\n\n\n\n\n\n\n\n\nUnder the Version section, leave the settings as they are. Make sure 'I accept the license agreement' is selected. It's also good practice to allow it to update as needed and to allow backups.\n\n\n\n\n\n\nUnder the Settings section, either create a new Administrator Username and Administrator Password or securely write down the Administrator Username and Administrator Password generated for you. (You can click Show Password to show it.) \nREMEMBER YOUR LOGIN INFORMATION! You NEED the username and password to make changes to your Omeka site.\n\n\n\n\n\n\nAdd an Administrator Email.\n\n\n\n\n\n\nChange the Website Title to a fitting and appropriate title.\n\n\n\n\n\n\n\n\nUnder Advanced, leave the default settings.\n\n\n\n\n\n\nClick the Install button. You will be redirected to 'My Applications'. The installation may take a minute to process.\n\n\n\n\n\n\n\n\nSelect the middle link of your new Omeka application that says \nomeka.mysite.com/admin\n or your variant of the subdomain.\n\n\n\n\n\n\n\n\nLogin using your Administrator Username and your Administrator Password.\n\n\n\n\n\n\n\n\nAt the top of the admin page, click on Settings.\n\n\n\n\n\n\nScroll down to the bottom. In the ImageMagick Directory Path field, type \n/usr/bin\n. Reclaim Hosting comes with ImageMagick preinstalled. ImageMagick resizes and generates thumbnails of your images.\n\n\n\n\n\n\n\n\nClick Save Changes.\n\n\n\n\n\n\nUploading content to Omeka\n\n\nFor the purposes of this exercise, we will use the \nGatineau Valley Historical Society fire insurance maps\n used in \nModule 4, Exercise 8\n. You are also allowed to use any historical sources to create an exhibit. Maybe you want to choose historical newspapers that have been digitized from the Library and Archives or the war diary from \nModule 2, Exercise 2\n. Just make sure to note where you got your sources and, preferably, that you can download them as images.\n\n\n\n\n\n\nOn the left hand side, select Collections.\n\n\n\n\n\n\n\n\nClick Add a Collection. \n\n\n\n\n\n\n\n\nMy new collection will be fire insurance maps. Under title, add Maps of the Gatineau Valley Fire Insurance Plans. \n\n\n\n\n\n\nGo through each metadata field and fill in as much information as you can about your collection.\n\n\n\n\n\n\nUnder Add Collection, click Public to make your collection publicly viewable. Click Add Collection.\n\n\n\n\n\n\n\n\nOn the left hand side, select Items.\n\n\n\n\n\n\nClick Add an item.\n\n\n\n\n\n\n\n\nNavigate to the \nGatineau Valley Historical Society Fire insurance maps\n or your own source of historical files.\n\n\n\n\n\n\nUsing the information provided with first map, fill in as much information as you can about your item. \n\n\n\n\n\n\nClick on the map image.\n\n\n\n\n\n\nRight click and save the image somewhere handy like your desktop. \n\n\n\n\n\n\nSelect the Files tab.\n\n\n\n\n\n\n\n\nClick Browse and choose your image file (note you cannot upload a single image larger than 128MB).\n\n\n\n\n\n\nOn the right hand side under Collection, select your Collection and make sure it is set to public.\n\n\n\n\n\n\n\n\nClick Add Item. It may take a few minutes to upload.\n\n\n\n\n\n\n\n\nOn the right hand side, click the blue View Public Page button.\n\n\n\n\n\n\n\n\nGo ahead and upload several exhibit items. If you can find other items related to your collection elsewhere, that'd be great too. \n\n\n\n\n\n\nGo to your variant of the \nomeka.mysite.com\n subdomain to view how to page looks to the public.\n\n\n\n\n\n\nStyling our exhibit (and making it more accessible!)\n\n\n\n\n\n\nAt the top of the admin page, click Appearance. The default theme is 'Thanks, Roy'. You can choose any theme you want. They are all different and each has different advantages and disadvantages for editing.\n\n\n\n\n\n\nFor this exercise, we will use the 'Thanks, Roy' theme. \n\n\nNB If you choose a different theme to modify, you may see different options. Proceed at your own intent.\n\n\n\n\n\n\nClick the blue Configure Theme button. Each theme configuration allows you to change basic style and content settings.\n\n\n\n\n\n\n\n\nWith the configuration page open in one tab, open your \nomeka.mysite.com\n to view the public page. Whenever you make an update, simply refresh the public page to view it.\n\n\n\n\n\n\nNotice the Browse Items and Browse Collections links on the left of your public page are a bit light and hard to read. In the configuration, copy the text colour hex code \n#444444\n.\n\n\n\n\n\n\nGo to the \nColor Hex website\n and search \n#444444\n. Hex codes are an internet convention where a series of numbers or numbers and letters correspond with a unique colour. \n\n\n\n\n\n\nScroll down to Shades of \n#444444\n. I think \n#1b1b1b\n looks dark enough.\n\n\n\n\n\n\nNavigate back to the Omeka configuration page and replace the Text and Links fields hex codes of \n#444444\n with \n#1b1b1b\n.\n\n\n\n\n\n\n\n\nOn the right hand side, click Save Changes.\n\n\n\n\n\n\nRefresh the public page and notice the text darkens. This will help make your website more readable.\n\n\n\n\n\n\nUsing the browser inspector to make design easier\n\n\n\n\n\n\nNavigate to your public Omeka page.\n\n\n\n\n\n\nRight click on the \"Featured Item\" text.\n\n\n\n\n\n\nSelect Inspect Element. The browser inspection tool allows you to view the code behind a website. \n\n\nNB Chrome and Firefox have great inspector tools. Not all browsers have these tools, however, notably Safari.\n\n\n\n\n\n\n\n\nThe font is not that easy to read either. We will change it, but first let's find the current font. Scroll down on the right hand side under the Rules tab until you see the body CSS. This rule shows the font is called PT serif, written like the following:\n\n\nbody {\n    font-family: \"PT Serif\", Times, serif;\n}\n\n\n\n\n\n\n\n\n\nGo to \nGoogle Fonts\n to choose a new, more readable font.\n\n\n\n\n\n\nSearch Roboto. Roboto is a clean, easy to read font.\n\n\n\n\n\n\nClick the red plus sign to select the font. It will add to a queue below.\n\n\n\n\n\n\nClick the black queue bar.\n\n\n\n\n\n\nScroll to where it says Specify in CSS. \n\n\n\n\n\n\nCopy the text that says \nfont-family: 'Roboto', sans-serif;\n. We will add this rule to our CSS file.\n\n\n\n\n\n\n\n\nNavigate to your cPanel home page and select File Manager at the top of the page.\n\n\n\n\n\n\n\n\nOn the left hand side click on the folder for your Omeka site. The folder should have the same name as your domain. For instance, if your site is called \nomeka.mysite.com\n, the folder will be called \nomeka.mysite.com\n unless you changed the name earlier.\n\n\n\n\n\n\nDouble click each folder until you are in the \ncss\n folder: Themes \n Default \n css\n\n\n\n\n\n\nRight click \nstyle.css\n and select Edit. Reclaim Hosting has some great editing tools within the cPanel so you don't have to make updates within your desktop text editor.\n\n\n\n\n\n\n\n\nSearch \nfont-family\n by hitting ctrl-f (Windows) or cmnd-f (Mac). We \nONLY\n want to change the rules that say \nfont-family: \"PT Serif\", Times, serif;\n.\n\n\n\n\n\n\n\n\nUse the search to replace each instance of \nfont-family: \"PT Serif\", Times, serif;\n with \nfont-family: 'Roboto', sans-serif;\n.\n\n\n\n\n\n\n\n\nClick the blue Save Changes button at the top right of the editor.\n\n\n\n\n\n\nNavigate to your Omeka site and refresh the page. Notice the font changes from PT Serif to Roboto.\n\n\nBefore (default PT Serif font)\n\n\n\n\nAfter (updated Roboto font)\n\n\n\n\n\n\n\n\nUse your browser's inspector to find out the style rules of any specific element on a web page. \n\n\n\n\n\n\nIf you make any major mistakes and want to start from scratch, you can copy the original \nstyle.css\n \nfile from GitHub\n and paste it into your file and save the changes. This will revert it back to the original style.\n\n\n\n\n\n\nNB\n It is important to note that for our purposes, we edited the default \nstyle.css\n file directly. This was to introduce you to CSS and making changes to the source code. Typically, you'd want to add a new file called, for example, \ncustom.css\n and specify the path to that file in our source code. That way you keep your own changes distinct from the default source code files. However, that would involve editing different PHP rules which is beyond the scope of this exercise. This is very important for making upgrades. If your theme ever upgrades, you will lose all your changes if you only edited the default \nstyle.css\n. This applies to any file type. Therefore, you may want to keep a copy of your changes somewhere handy, like on your desktop.\n\n\n\n\nMore\n\n\nMore resources, tutorials, and things to explore.\n\n\nAccessibility and Design\n\n\nWhile most of this applies to websites, think also about how the general principles apply to other ways \n means of telling our data stories.\n\n\n\n\nHow People with Disabilities Use the Web\n\n\nWeb Content Accessibility and Mobile Web\n\n\nAccessibility in HTML5\n\n\nWeb Accessibility Evaluation Tool\n\n\nConsidering the User Perspective\n\n\nConstructing a POUR Website\n Percievable, Operable, Understandable, Robust. Applies much wider than design of websites.\n\n\nChecking Microsoft Office-generated documents for accessibility\n\n\n\n\nInfographics and Storytelling\n\n\nInfographics\n\n\n\n\nThe Difference between Infographics and Visualizations\n\n\nDesign hints for infographics\n\n\nPiktochart\n\n\nInfogr.am\n\n\nInfoactive.co\n (has a free option, but might not allow exports. You'd have to check).\n\n\nEasel.ly\n\n\n\n\nStorytelling\n\n\n\n\nCreatavist\n\n\nMedium\n\n\nCowbird\n\n\nExposure\n\n\nTimeline.js\n\n\nStorymap\n\n\n\n\nManyLines\n\n\nManylines\n is an application that allows you to create narratives from network graphs. In essence, you upload a network file in .gexf format (which you can export from Gephi) and it renders it on the screen. There are some layout options to make the graph more intelligible. Then, you take a series of snapshots zoomed in on the graph in different places, and add text to describe what it is that's important about these networks. The app puts a Prezi-like wrapper around your snapshots, and the whole can then be embedded in a website or be used as a standalone website. \nCheck out my first attempt on medialab.\n \n\n\nYou can also embed nearly anything in the narrative panels \n Youtube videos, \ntimeline.js\n, as long as you know how to correctly format an \niframe\n.  \n\n\nTo give this a try, why not use the Texan Correspondence network we generated in earlier modules? Export it in .gexf format from gephi, import to ManyLines, and go! The interface is fairly straightforward. Just follow the prompts.\n\n\nCaveat Utilitor\n I don't know how long anything made with ManyLines will live on their website. But, knowing what you know about wget and other tools, do you see how you could archive a copy on your own machine? ManyLines is available on \nGitHub\n so you can certainly use it locally.\n\n\nLeaflet\n\n\nMaps can be created through a variety of services (\ntilemill\n, \ncartodb\n and \nmapbox\n for instance). These can then be embedded in your webpages or documents. Often, that's enough. But sometimes, you'd like to take control, and keep all the data, all the map, under your own name, in your own webspace. Visit the gentle introduction to using \nleaflet.js in our supporting materials\n to make, style, and serve your maps. \nVisit a template on my GitHub repository\n for mapping with leaflet, drawing all of your point data and ancillary information from a CSV file. Leaflet also has \na list of background maps you can use\n.\n\n\nLeaflet in the field: Pembroke Soundscapes Project\n\n\nView the \nPembroke Soundscapes Project\n for an example of an academic historical project using Leaflet. This project uses an interactive map with sound clips to explore the industrial history of Pembroke, Ontario. \n\n\nDesigning Maps with Processing and Illustrator\n\n\nNicolas Felton is a renowned designer. Watch \nhis 90 minute workshop on Skillshare\n.\n\n\nNB I do not use Processing or Illustrator, but Processing is free and Inkscape can do many of the things that Illustrator does.", 
            "title": "Exercises"
        }, 
        {
            "location": "/module-5/Exercises/#module-5-exercises", 
            "text": "In this module, you will begin with an introduction to web design concepts and finish by creating your own online exhibit. While we will not explore every possibility here, I do not want to leave you thinking that all digital history work is necessarily text based. To that end, if you are looking for a challenge or for exposure to something rather different, I suggest you at least bookmark my  series of tutorials on augmented reality, games, and 3d models for history and archaeology .   'Accidental Renaissance: the photos that look like Italian paintings' .  The exercises in this module cover:   Design with colour and font  Layout   Manipulating graphics  Creating digital exhibits", 
            "title": "Module 5 Exercises"
        }, 
        {
            "location": "/module-5/Exercises/#exercise-1-layout", 
            "text": "'Layout' can mean different things in different contexts. A general overview on issues in layout is covered by  'What makes a design great, Lynda.com  and  'Exploring principles of layout and composition' . The  Slideshare on effective layouts  gives you a sense of things to watch out for as well.  For academic posters in particular, consider  these suggestions from the APA .  In essence, good layout makes your argument legible, intuitive, and reinforces the rhetorical points you are trying to make. You should take into account 'principles of universal design'   consider design issues with  PowerPoint  and  websites  (although some of the technical solutions proposed in those two documents are a bit out of date, the general principles hold!)  In this exercise, you will design a new poster  or  modify an existing poster. You can use either Inkscape or PowerPoint.", 
            "title": "Exercise 1: Layout"
        }, 
        {
            "location": "/module-5/Exercises/#inkscape", 
            "text": "View the  Inkscape exercise in our supporting materials for a gentle introduction to Inkscape .   Download one of the scientific poster templates from  Ugo Sangiorgi  (These are developed from  Felix Breuer's blog post ; note his discussion on design).   Open it in Inkscape.   Make notes in your open notebook  from the point of view of layout:  what elements of the design work? What aren't working? How would you repurpose this poster to fit the requirements of the assessment exercise ( remember, details here )?  Visit Inkscape's website for help with the basics of Inkscape .   Modify the poster, and upload the SVG, PDF, or PNG version to your repository.", 
            "title": "Inkscape"
        }, 
        {
            "location": "/module-5/Exercises/#powerpoint", 
            "text": "There's a lot of information out there on making posters with PowerPoint.    Read  parts 1, 2, and 3 of the Make Signs tutorial  and then consider  Colin Purrington's advice . Once you've read and digested, pick a poster from  Pimp My Poster  that sticks out to you.   Make notes in your open notebook:  from the point of view of layout  what elements of the design work? What aren't working? How would you repurpose this posert to fit the requirements ot he assessment exercises ( remember to visit GitHub for the details )?   Grab a template from  from Colin Purrington's , and use it to prototype a poster that works.   Upload the poster as a PDF to your repository.   If you want to explore layout in the context of webpage creation, I would point you to the the roster of lessons at  Codeacademy . Same instructions: find an existing site that you will consider from the point of view of what works, what doesn't, and use that reflection to guide the construction of your own.", 
            "title": "PowerPoint"
        }, 
        {
            "location": "/module-5/Exercises/#exercise-2-typography", 
            "text": "Typographic plays an important role in visual communication. It can do everything from reduce eyestrain (do a search for 'easiest fonts to read on screen') to communicate power and authority. Is it any wonder that strong bold capitals come to be known as 'Roman'? But first:  are you a comic sans criminal?  NB  Serif fonts are generally not as accessible as sans-serif fonts. Older screens and monitors can have a difficult time rendering serif fonts. Sans-serif fonts can be more readable across different screens.  However , this does not mean you should throw away serif fonts   many serif fonts can be quite accessible and add to your design goals if used properly. For more information on accessible fonts, check out  WebAIM's article on fonts .  In this exercise,   I want you to read and understand the section on  font choices from the Owl at Purdue .  Then, play some rounds of  Typeconnection . Pay attention to why   or why not   various pairings work.  Then, I want to consider the document you will be preparing for me that accounts for your learning in this course   the document where you choose your best exercises from the modules and explain how your approach to history, data, the digital, etc, has changed over this course. What typographic pair would work best for you?  Finally, you'll make a webpage that uses those fonts and explains why they work.   The first part of this exercise then is to find a pair of fonts and to understand why they work best for you.     Read the materials above, and once you're done with the  Typeconnection site , go to  Google Fonts  and search for a pair that are  suitable for your purpose .     When you find a font you like, click the 'add to collection' button.    At the bottom of the screen, you'll see a link for 'use'. Click on this   Google will ask you if you want to use any of the variants of the font. Tick off accordingly.     Do you see the embed code that Google provides, and the code to integrate the font into your CSS (stylesheet)? Leave this window open   we're going to use it in a moment.    Make a new repository for this exercise.     In your repository, click the button beside 'branch'.     In the search box that opens, type in  gh-pages . This will create a version of your repository that can be served as a website. You're now in the gh-pages branch.    Click on the + sign beside the repository name. This will create a new file in your gh-branch of your repository. Call it  myfontchoice.html   - the .html is important to specify; otherwise your browser will not know how to render your page.    You now need to put some HTML in there. I've written a simple webpage that will use two fonts from Google Fonts, and then applies the font to my text depending on whether or not it is a header, which you specify like this:  h1  this is a header in between header tags  /h1  or a paragraph, which you specify like this:  p blah blah blah a paragraph of text blah blah blah /p . Right-click and open in a new tab  my webpage on GitHub . Copy the HTML into your new HTML document. Commit your changes (ie. save).       Let's see what we've got. To see the website version of your gh-pages branch, you go to  yourusername .github.io/ reponame /myfontchoice.html   - ie. the final part of the URL is the name of the document in your repo. Do that now. You should see a simple webpage, with two very distinctive fonts.    Now let's slide your font choices into the HTML. Go to your HTML page in your gh-pages repo (ie, not the  github.io  version, but the  github.com/ yourusername / repo /myfontchoice.html  version.     Hit the edit button. Look closely at line 6. Do you see my two fonts? Do you see the pipe character (the  |  symbol) between them? That tells Google you want  both  fonts.     Navigate to the  Google Fonts page  again to grab the exact name for your fonts (uppercase letters make a difference!) and paste them into line 5 appropriately.    Lines 8 and 14 specify which font to use for headers, and which font to use for body text. Change appropriately.    Change my silly text for a paragraph that explains what the fonts are, and why they work for this purpose. Commit your changes.    Go to the  github.io  version of your repository (if you forget the address, you can find it under the 'Settings' tab on your normal repository page when you're in the gh-pages branch).     Reload the page several times to clear the older version you've already looked at and to replace it with your version. Ta da! Not only have you thought carefully about typography and fonts, you've also learned how to serve a webpage from a GitHub repository.       Hint  You could use this as the basics of your submission for assessment, for your exercises. Build a webpage, link to your evidence, embed your images. For basic HTML  W3schools has a really good guide to keep around .", 
            "title": "Exercise 2: Typography"
        }, 
        {
            "location": "/module-5/Exercises/#going-further-with-github-pages", 
            "text": "Now that you have learned the basics of creating a simple website using GitHub, you have the ability to go further. Using the DH Box, you can build a simple website through GitHub pages using the MkDocs static site generator. Navigate to the  static site generator exercise using GitHub pages in the supporting materials .", 
            "title": "Going further with GitHub pages"
        }, 
        {
            "location": "/module-5/Exercises/#exercise-3-colour", 
            "text": "There's a lot of bumpf out there on the 'pyschology of colour'. Google it to see what I mean ( Youth Designer has a good example ). A lot of what you see here isn't so much psychology as it is associations (and indeed, western associations, at that). Associations are important of course; you don't want to undermine your message by making some unfortunate connections.  In this exercise, I want you to take the webpage you developed in the previous exercise, and make two versions of it:     One where the colours  support  the broader message of the page (a prototype for your exercises assessment piece, remember?)  And the other where the colours  undermine  that broader message.      Explain, in both cases, how your colour choices enhance and/or detract.  Alternatively, you can make a one page slide in PowerPoint doing the same thing.", 
            "title": "Exercise 3: Colour"
        }, 
        {
            "location": "/module-5/Exercises/#resources", 
            "text": "Below is a graphic and a movie to help with the theoretical side of things:   Understanding the rules of color, Lynda.com  To learn how to style your webpage appropriately, you can  follow this tutorial on CSS from codeacademy.com .  Below is a graphic that presents how colours are perceived in different cultures:", 
            "title": "Resources"
        }, 
        {
            "location": "/module-5/Exercises/#exercise-4-creating-an-online-exhibit-with-omeka", 
            "text": "Omeka  is an open source Content Management System (CMS) for sharing digital collections and creating media-rich online exhibits. A CMS is a web program that manages content including graphics, text, videos, and more for a web site. A CMS allows users to upload their content via a Graphical User Interface (GUI) and organizes it into appropriate folder structures. Traditional websites are programmed from the ground up   everything from structure, to styling, to creating folders and specifiying where multi-media content is located.   The main advantage of using a CMS is that it requires little to no actual, background-level programming. That being said, anyone can edit a CMS's source code files, add their own, or modify them to fit their needs. Most CMSs are built with  PHP , a web scripting program used to dynamically create content. However, most CMSs are also accompanied by massive native and third party plugin libraries that allow users to modify their website without touching any source code. Examples of CMSs include Wordpress, Drupal, Grav, Known, Joomla, and many more.   Unlike many popular CMSs, Omeka is not a blogging platform. Omeka is used to create online exhibits. Today we are going to create a simple online exhibit.   Our overall steps are as follows:   Create a subdomain of your website where Omeka will live. This will allow you to host Omeka as an entity of its own on your website, keeping your main website separate.  Install Omeka through cPanel on Reclaim Hosting.  Create an online exhibit with several historical items.  Change the style of our Omeka site by modifying the CSS of our theme.", 
            "title": "Exercise 4: Creating an online exhibit with Omeka"
        }, 
        {
            "location": "/module-5/Exercises/#creating-a-subdomain", 
            "text": "Login to your Reclaim Hosting account through the  Reclaim Hosting login page .    In the client area, select the cPanel tab.     Under the Domains section, select Subdomains. Typically a subdomain divides your overall website into distinct portions. For instance, you could have your landing page at  mysite.com , your blog at  blog.mysite.com , and your class work at  hist3814.mysite.com . That way each subdomain could have different styles and serve different purposes, but still be connected to your overall domain. You can add as many subdomains to your site as you'd like.   Subdomains are essentially just an easier way of organizing your files and showing distinct differences from your main domain. You could technically also do  mysite.com/blog  or  mysite.com/hist3814 . In that case, however, it seems expected they would all be styled similarly to  mysite.com . Furthermore, it might cause difficulty to have, for example, Wordpress running your mysite.com and Omeka running  mysite.com/Omeka . It's best to stick with your free subdomains.     Enter the name of your online exhibit in the Subdomain field. You could simply put  omeka ,  omeka-test , etc. or get more descriptive and say  gatineau-fire-maps . Choose a title relevant to your Omeka site's purpose.    Leave the Domain field set to your domain.    The Document Root will be automatically created (an advantage of adding a subdomain!). I would not recommend changing this, but if you want to further organize your folder structure, you are able to change the location.     Click the Create button. It will take a moment, but your new subdomain will be approved.      NB Since the subdomain has nothing in it, if you navigate to it now, you will see 'Index of/' showing the file structure. This is normal.", 
            "title": "Creating a subdomain"
        }, 
        {
            "location": "/module-5/Exercises/#installing-omeka", 
            "text": "Navigate back to your cPanel home page.     Under the Applications section, select Omeka.      On the right hand side, click the '+ install this application' button.      Under the Location section, select the subdomain you created in the previous instructions. Do not choose your main  mysite.com  domain. Choose your  omeka.mysite.com  domain or your variant of it. You can choose the  https  or  http  version. Note, however, that  https  is more secure.    Delete  cms  under Directory (Optional). We want Omeka installed to the root of our subdomain, not in a folder called  cms . If you were installing Omeka to your main site, then you'd want a subdirectory called  cms .     Under the Version section, leave the settings as they are. Make sure 'I accept the license agreement' is selected. It's also good practice to allow it to update as needed and to allow backups.    Under the Settings section, either create a new Administrator Username and Administrator Password or securely write down the Administrator Username and Administrator Password generated for you. (You can click Show Password to show it.)  REMEMBER YOUR LOGIN INFORMATION! You NEED the username and password to make changes to your Omeka site.    Add an Administrator Email.    Change the Website Title to a fitting and appropriate title.     Under Advanced, leave the default settings.    Click the Install button. You will be redirected to 'My Applications'. The installation may take a minute to process.     Select the middle link of your new Omeka application that says  omeka.mysite.com/admin  or your variant of the subdomain.     Login using your Administrator Username and your Administrator Password.     At the top of the admin page, click on Settings.    Scroll down to the bottom. In the ImageMagick Directory Path field, type  /usr/bin . Reclaim Hosting comes with ImageMagick preinstalled. ImageMagick resizes and generates thumbnails of your images.     Click Save Changes.", 
            "title": "Installing Omeka"
        }, 
        {
            "location": "/module-5/Exercises/#uploading-content-to-omeka", 
            "text": "For the purposes of this exercise, we will use the  Gatineau Valley Historical Society fire insurance maps  used in  Module 4, Exercise 8 . You are also allowed to use any historical sources to create an exhibit. Maybe you want to choose historical newspapers that have been digitized from the Library and Archives or the war diary from  Module 2, Exercise 2 . Just make sure to note where you got your sources and, preferably, that you can download them as images.    On the left hand side, select Collections.     Click Add a Collection.      My new collection will be fire insurance maps. Under title, add Maps of the Gatineau Valley Fire Insurance Plans.     Go through each metadata field and fill in as much information as you can about your collection.    Under Add Collection, click Public to make your collection publicly viewable. Click Add Collection.     On the left hand side, select Items.    Click Add an item.     Navigate to the  Gatineau Valley Historical Society Fire insurance maps  or your own source of historical files.    Using the information provided with first map, fill in as much information as you can about your item.     Click on the map image.    Right click and save the image somewhere handy like your desktop.     Select the Files tab.     Click Browse and choose your image file (note you cannot upload a single image larger than 128MB).    On the right hand side under Collection, select your Collection and make sure it is set to public.     Click Add Item. It may take a few minutes to upload.     On the right hand side, click the blue View Public Page button.     Go ahead and upload several exhibit items. If you can find other items related to your collection elsewhere, that'd be great too.     Go to your variant of the  omeka.mysite.com  subdomain to view how to page looks to the public.", 
            "title": "Uploading content to Omeka"
        }, 
        {
            "location": "/module-5/Exercises/#styling-our-exhibit-and-making-it-more-accessible", 
            "text": "At the top of the admin page, click Appearance. The default theme is 'Thanks, Roy'. You can choose any theme you want. They are all different and each has different advantages and disadvantages for editing.    For this exercise, we will use the 'Thanks, Roy' theme.   NB If you choose a different theme to modify, you may see different options. Proceed at your own intent.    Click the blue Configure Theme button. Each theme configuration allows you to change basic style and content settings.     With the configuration page open in one tab, open your  omeka.mysite.com  to view the public page. Whenever you make an update, simply refresh the public page to view it.    Notice the Browse Items and Browse Collections links on the left of your public page are a bit light and hard to read. In the configuration, copy the text colour hex code  #444444 .    Go to the  Color Hex website  and search  #444444 . Hex codes are an internet convention where a series of numbers or numbers and letters correspond with a unique colour.     Scroll down to Shades of  #444444 . I think  #1b1b1b  looks dark enough.    Navigate back to the Omeka configuration page and replace the Text and Links fields hex codes of  #444444  with  #1b1b1b .     On the right hand side, click Save Changes.    Refresh the public page and notice the text darkens. This will help make your website more readable.", 
            "title": "Styling our exhibit (and making it more accessible!)"
        }, 
        {
            "location": "/module-5/Exercises/#using-the-browser-inspector-to-make-design-easier", 
            "text": "Navigate to your public Omeka page.    Right click on the \"Featured Item\" text.    Select Inspect Element. The browser inspection tool allows you to view the code behind a website.   NB Chrome and Firefox have great inspector tools. Not all browsers have these tools, however, notably Safari.     The font is not that easy to read either. We will change it, but first let's find the current font. Scroll down on the right hand side under the Rules tab until you see the body CSS. This rule shows the font is called PT serif, written like the following:  body {\n    font-family: \"PT Serif\", Times, serif;\n}     Go to  Google Fonts  to choose a new, more readable font.    Search Roboto. Roboto is a clean, easy to read font.    Click the red plus sign to select the font. It will add to a queue below.    Click the black queue bar.    Scroll to where it says Specify in CSS.     Copy the text that says  font-family: 'Roboto', sans-serif; . We will add this rule to our CSS file.     Navigate to your cPanel home page and select File Manager at the top of the page.     On the left hand side click on the folder for your Omeka site. The folder should have the same name as your domain. For instance, if your site is called  omeka.mysite.com , the folder will be called  omeka.mysite.com  unless you changed the name earlier.    Double click each folder until you are in the  css  folder: Themes   Default   css    Right click  style.css  and select Edit. Reclaim Hosting has some great editing tools within the cPanel so you don't have to make updates within your desktop text editor.     Search  font-family  by hitting ctrl-f (Windows) or cmnd-f (Mac). We  ONLY  want to change the rules that say  font-family: \"PT Serif\", Times, serif; .     Use the search to replace each instance of  font-family: \"PT Serif\", Times, serif;  with  font-family: 'Roboto', sans-serif; .     Click the blue Save Changes button at the top right of the editor.    Navigate to your Omeka site and refresh the page. Notice the font changes from PT Serif to Roboto.  Before (default PT Serif font)   After (updated Roboto font)     Use your browser's inspector to find out the style rules of any specific element on a web page.     If you make any major mistakes and want to start from scratch, you can copy the original  style.css   file from GitHub  and paste it into your file and save the changes. This will revert it back to the original style.    NB  It is important to note that for our purposes, we edited the default  style.css  file directly. This was to introduce you to CSS and making changes to the source code. Typically, you'd want to add a new file called, for example,  custom.css  and specify the path to that file in our source code. That way you keep your own changes distinct from the default source code files. However, that would involve editing different PHP rules which is beyond the scope of this exercise. This is very important for making upgrades. If your theme ever upgrades, you will lose all your changes if you only edited the default  style.css . This applies to any file type. Therefore, you may want to keep a copy of your changes somewhere handy, like on your desktop.", 
            "title": "Using the browser inspector to make design easier"
        }, 
        {
            "location": "/module-5/Exercises/#more", 
            "text": "More resources, tutorials, and things to explore.", 
            "title": "More"
        }, 
        {
            "location": "/module-5/Exercises/#accessibility-and-design", 
            "text": "While most of this applies to websites, think also about how the general principles apply to other ways   means of telling our data stories.   How People with Disabilities Use the Web  Web Content Accessibility and Mobile Web  Accessibility in HTML5  Web Accessibility Evaluation Tool  Considering the User Perspective  Constructing a POUR Website  Percievable, Operable, Understandable, Robust. Applies much wider than design of websites.  Checking Microsoft Office-generated documents for accessibility", 
            "title": "Accessibility and Design"
        }, 
        {
            "location": "/module-5/Exercises/#infographics-and-storytelling", 
            "text": "", 
            "title": "Infographics and Storytelling"
        }, 
        {
            "location": "/module-5/Exercises/#infographics", 
            "text": "The Difference between Infographics and Visualizations  Design hints for infographics  Piktochart  Infogr.am  Infoactive.co  (has a free option, but might not allow exports. You'd have to check).  Easel.ly", 
            "title": "Infographics"
        }, 
        {
            "location": "/module-5/Exercises/#storytelling", 
            "text": "Creatavist  Medium  Cowbird  Exposure  Timeline.js  Storymap", 
            "title": "Storytelling"
        }, 
        {
            "location": "/module-5/Exercises/#manylines", 
            "text": "Manylines  is an application that allows you to create narratives from network graphs. In essence, you upload a network file in .gexf format (which you can export from Gephi) and it renders it on the screen. There are some layout options to make the graph more intelligible. Then, you take a series of snapshots zoomed in on the graph in different places, and add text to describe what it is that's important about these networks. The app puts a Prezi-like wrapper around your snapshots, and the whole can then be embedded in a website or be used as a standalone website.  Check out my first attempt on medialab.    You can also embed nearly anything in the narrative panels   Youtube videos,  timeline.js , as long as you know how to correctly format an  iframe .    To give this a try, why not use the Texan Correspondence network we generated in earlier modules? Export it in .gexf format from gephi, import to ManyLines, and go! The interface is fairly straightforward. Just follow the prompts.  Caveat Utilitor  I don't know how long anything made with ManyLines will live on their website. But, knowing what you know about wget and other tools, do you see how you could archive a copy on your own machine? ManyLines is available on  GitHub  so you can certainly use it locally.", 
            "title": "ManyLines"
        }, 
        {
            "location": "/module-5/Exercises/#leaflet", 
            "text": "Maps can be created through a variety of services ( tilemill ,  cartodb  and  mapbox  for instance). These can then be embedded in your webpages or documents. Often, that's enough. But sometimes, you'd like to take control, and keep all the data, all the map, under your own name, in your own webspace. Visit the gentle introduction to using  leaflet.js in our supporting materials  to make, style, and serve your maps.  Visit a template on my GitHub repository  for mapping with leaflet, drawing all of your point data and ancillary information from a CSV file. Leaflet also has  a list of background maps you can use .", 
            "title": "Leaflet"
        }, 
        {
            "location": "/module-5/Exercises/#leaflet-in-the-field-pembroke-soundscapes-project", 
            "text": "View the  Pembroke Soundscapes Project  for an example of an academic historical project using Leaflet. This project uses an interactive map with sound clips to explore the industrial history of Pembroke, Ontario.", 
            "title": "Leaflet in the field: Pembroke Soundscapes Project"
        }, 
        {
            "location": "/module-5/Exercises/#designing-maps-with-processing-and-illustrator", 
            "text": "Nicolas Felton is a renowned designer. Watch  his 90 minute workshop on Skillshare .  NB I do not use Processing or Illustrator, but Processing is free and Inkscape can do many of the things that Illustrator does.", 
            "title": "Designing Maps with Processing and Illustrator"
        }, 
        {
            "location": "/conclusion/conclusion/", 
            "text": "Conclusion\n\n\nThere will be a profound and witty concluding paragraph here, some day. But if you've noticed anything at all, it's that the process of crafting digital history never comes to an end. \nAdam Crymble\n published a piece recently that throws this into high relief \n pay attention to his second paragraph in his main diagram!\n\n\nIn the meantime, I do want you to know that you too can craft excellent digital history. There were some great moments in the first iteration of this course, and you will have great moments too: like when Matt forked one of my tutorials and rewrote it for the better, or built a virtual machine. Or when Patrick finally slayed Github! Or when Allison got the Canadiana API to work. Or when Phoebe finally persuaded Inkscape to play nice. Or when Matt conquered TWARC. Or when TEI blew Ryan's mind. Or when Christina forked an Anthropology class project at MSU to repurpose for her project. Or... or... or. We covered a lot of ground.\n\n\nYou will too. \n\n\nA word to the wise\n\n\nIf, in your other courses, you decide to use some of the methods here, I will be most gratified. However \n in course work as in life, \nknow\n your audience. Will your prof appreciate this work? Is your prof familiar with the underlying issues \n will she know what to look for, the hidden gotchas, the places where things might get, erm, fudged? It is \nyour\n responsibility to make the argument, in your work, why a particular methodological choice is appropriate. It is \nyour\n responsibility to show how these choices are theoretically informed and meaningful. As in all history, you have to make the argument. Never fall into the trap of thinking the method speaks for itself. This is why I compelled you to create the paradata document for your project. As that master historian Yoda once said,\n\n\n\n\nDo or do not. There is no try.\n\n\n\n\nAlright, not exactly appropriate. But you get the gist: digital history requires explicit paradata. Do. There is no try.", 
            "title": "6. Final Thoughts"
        }, 
        {
            "location": "/conclusion/conclusion/#conclusion", 
            "text": "There will be a profound and witty concluding paragraph here, some day. But if you've noticed anything at all, it's that the process of crafting digital history never comes to an end.  Adam Crymble  published a piece recently that throws this into high relief   pay attention to his second paragraph in his main diagram!  In the meantime, I do want you to know that you too can craft excellent digital history. There were some great moments in the first iteration of this course, and you will have great moments too: like when Matt forked one of my tutorials and rewrote it for the better, or built a virtual machine. Or when Patrick finally slayed Github! Or when Allison got the Canadiana API to work. Or when Phoebe finally persuaded Inkscape to play nice. Or when Matt conquered TWARC. Or when TEI blew Ryan's mind. Or when Christina forked an Anthropology class project at MSU to repurpose for her project. Or... or... or. We covered a lot of ground.  You will too.", 
            "title": "Conclusion"
        }, 
        {
            "location": "/conclusion/conclusion/#a-word-to-the-wise", 
            "text": "If, in your other courses, you decide to use some of the methods here, I will be most gratified. However   in course work as in life,  know  your audience. Will your prof appreciate this work? Is your prof familiar with the underlying issues   will she know what to look for, the hidden gotchas, the places where things might get, erm, fudged? It is  your  responsibility to make the argument, in your work, why a particular methodological choice is appropriate. It is  your  responsibility to show how these choices are theoretically informed and meaningful. As in all history, you have to make the argument. Never fall into the trap of thinking the method speaks for itself. This is why I compelled you to create the paradata document for your project. As that master historian Yoda once said,   Do or do not. There is no try.   Alright, not exactly appropriate. But you get the gist: digital history requires explicit paradata. Do. There is no try.", 
            "title": "A word to the wise"
        }, 
        {
            "location": "/supporting materials/tei/", 
            "text": "Close Reading with TEI (Text Encoding Initiative)\n\n\nThis worksheet, and all related files, are released CC-BY.\n\n\nBy M. H. Beals\n; Adapted for HIST3814o by S Graham\n\n\n\n\nYou will need the files in \nthe Crafting Digital History GitHub module 3 folder\n. \nYou need the folder \ntei-hist3907\n.\n\n\nSelect the Clone or Download button and choose download zip to download that repository as a zip file. \n\n\nUnzip it somewhere handy on your machine. Inside will be the subfolder named \ntei-hist3907\n.\n\n\nOpen the subfolder named \ntei-hist3907\n.\n\n\n\n\nThis exercise will explore a historical text and help you create a digital record of your analysis.\n\n\n\n\n\n\n\n\nVetting a Website\n\n\n\n\nVisit the \nRecovered Histories Website\n.\n\n\nExamine the site's layout and read its introduction.\n\n\n\n\n\n\n\n\nWhat makes you believe this site is a trustworthy provider of historical texts?\n\n\nWhat makes you believe this site is NOT a trustworthy provider of historical texts?\n\n\n\n\n\n\nFinding a Source\n\n\n\n\nVisit the site's collections via the 'Browse' function. \n\n\nLocate the pamphlet \nNegro Slavery\n by Zachary Macaulay and open it.\n\n\n\n\nThis is an abolitionist pamphlet regarding the Atlantic slave trade, presenting and examine evidence of how it is run.  When you approach a primary source like this, it is tempting to read through it from beginning to end, to get an overview of its contents, and then 'mine' or 'cherry-pick' good quotations to include in your assessments.  However, we are going to focus on examining a very small part of the text in a very high level of detail.  \n\n\n\n\n\n\n\n\nSetting Up Your Workspace\n\n\nYou will use your own machine rather than DH Box for this work. \n\n\n\n\nArrange your workspace so that you have the scanned text of the pamphlet easily visible on one side of your screen. \n\n\nOpen the \nblanktemplate.txt\n file in \nSublime Text\n, \nAtom\n, \nTextwrangler\n or \nNotepad++\n (or any text editor that understands encoding) and have that on the other side of the screen.\n\n\n\n\nThe last lines will be \n/body\n/text\n/TEI\n/teiCorpus\n. Everything you write today should be just above \n/body\n.\n\n\nTranscribing Your Page\n\n\n\n\n\n\nThe first thing you will need is go to the tag\n\n\nbiblScope\n1\n/biblScope\n\n\n\n\nand replace the number one (1) with the page number you are transcribing. Which page should you transcribe? \n\n\n\n\n\n\nSelect a page in the document that you find interesting.\n\n\n\n\n\n\nNext, you will need to \nvery carefully\n transcribe your page of text from the image into your document.  Make sure you do not make any changes to the text, even if you think they author has used poor grammar or misspelled a word.  You do not need to worry about line breaks but should start every new paragraph (or heading) with a \n/p\n and end every paragraph (or heading) with a \n/p\n.\n\n\n\n\n\n\nOnce you have completed your transcription, look away from your computer for 30-45 seconds.  Staring into the distance every 10-20 minutes will keep your eyes from straining.  Also, shake out your hands at the wrists, to prevent repetitive stress injuries to your fingers.  \n\n\n\n\n\n\n\n\n\n\n\n\nEncoding Your Transcription\n\n\nYou are now going to \nencode\n or \nmark-up\n your text.  \n\n\n\n\n\n\nRe-read your page and highlight / colour the following:\n\n\n\n\nAny persons mentioned (including any he/she if they refer to a specific person)\n\n\nAny places mentioned\n\n\nAny claims, assertions or arguments made\n\n\n\n\nNow that you have highlighted these, you are going to put proper code around them.\n\n\n\n\n\n\nFor persons, surround your text with the following:\n\n\npersName key=\"Last, First\" from=\"YYYY\" to=\"YYYY\" role=\"Occupation\" ref=\"http://www.website.com/webpage.html\"\n \n/persName\n\n\n\n\n\n\nInside the speech marks for \nkey\n, include the real full name of the person mentioned\n\n\nIn \nfrom\n and \nto\n, include their birth and death years, using ? for unknown years\n\n\nIn \nrole\n, put the occupation, role or 'claim to fame' for this individual.  \n\n\nIn \nref\n, put the URL (link) to the Dictionary of National Biography, Wikipedia or other biography website where you found this information. If there is a \n in your link, you will need to replace this with \namp;\n.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor places, surround your text with the following:\n\n\nplaceName key=\"Sheffield, United Kingdom\" ref=\"http://tools.wmflabs.org/geohack/geohack.php?pagename=Sheffield\nparams=53_23_01_N_1_28_01_W_type:city_region:GB\"\"\n \n/placeName\n\n\n\n\n\n\nIn \nkey\n, put the city and country with best information you can find for the modern names for this location\n\n\nIn \nref\n, put a link to the relevant coordinates on \nWikipedia GeoHack website\n.\n\nTo obtain this, go to the Wikipedia page for this city and click on the latitude/longitude coordinates for the location. For large areas, such as entire countries or continents, just use the Wikipedia page URL.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor claims or arguments, surround your text with the following:\n\n\ninterp key=\"reason\" n=\"citation\" cert=\"high\" ref=\"http://www.website.com/webpage.html\"\n \n/interp\n\n\n\n\n\n\nIn \nkey\n, explain why you believe this claim is true or not\n\n\nIn \nn\n, put a full citation to the relevant source\n\n\nIn \ncert\n (short for certainty), put: high, medium, low or unknown\n\n\nIn \nref\n, put the link to the website where you got the information to assess this claim.\n\n\n\n\nWhen you are happy with your work, hit save your work, give it a useful name, make sure it has \n.xml\n as the extension, and save it \nand the \n.xsl\n file\n to your repository.\n\n\n\n\n\n\n\n\n\n\n\n\nAlex Gill has made \nThe Short and Sweet TEI Handout\n which you might want to explore as well. When you embark on encoding documents for your own research, \nNortheastern Univeristy has some questions to think about\n to help you decide what kinds of tagging you'll need; these \ntemplates from HisTEI\n might be useful (open the whole project with OxygenXML for full functionality, but you can copy those templates in any editor).\n\n\nViewing Your Encoded Text\n\n\nTo see your encoded text, make sure your .xml and .xsl file are in the same folder. \nOpen either Internet Explorer or Firefox\n. The following will not work in Chrome because it has different security settings.\n\n\nMaking sure both your \n(page number).xml\n file and your \n000style.xsl\n file are in the same folder (or both on your desktop), drag the icon for \n(page number).xml\n into your Internet Explorer Browser window.\n\n\nIf you now see a colour-coded version of your text, Congratulations! If you hover over the coloured sections, you should see a pop-up with the additional information you entered.\n\n\nIf your text comes up only in black, with no paragraph divisions or headings, or doesn't come up at all, something has gone wrong. Re-open your .xml file and check that you have:\n\n\n\n\n\n\nPlaced \np\n at the start of every paragraph, including the start of the page\n\n\n\n\n\n\nPlaced \n/p\n at the end of every paragraph, including the end of the page\n\n\n\n\n\n\nMade sure all your \npersName\n, \nplaceName\n and \ninterp\n tags are properly enclosed in \ns\n\n\n\n\n\n\nMade sure you have both an open \n and close \n\\\n tag for each tag you use\n\n\n\n\n\n\nMade sure you attribute values are fully enclosed in \n\"\"\n.\n\n\n\n\n\n\nMade sure you have a space between the \n\"\n of one attribute and the start of the next\n\n\n\n\n\n\nMade sure you do NOT have a space after the \n=\n of an attribute\n\n\n\n\n\n\nIf your text still does not appear formatted, you may need to remove the text one paragraph at a time, refreshing your browser window, until it appears. This will help you identify which paragraph (or sentence) has the error within it).\n\n\n\n\n\n\n\n\nIf you still don't see your text\n\n\nIf you do not see the colour-coded version of your text, this might not necessarily mean that you've done something wrong.\n Some browsers will not perform the transformation, for security reasons.\n\n\nIn which case, we can do the following: \n\n\n\n\nIf you are on a Windows machine using Notepad++, go to 'Plugins' \n Plugin Tools. (If you are on Windows but aren't using Notepad++, Sublime and Atom probably have a similar functionality, but you will have to search to figure it out.) \n\n\nSelect 'XML Tools' from the list, and install it. \n\n\nYou'll probably have to restart the program to complete the plugin installation. \n\n\nOpen up the \n1.xml\n file in Notepad ++.\n\n\nUnder 'plugins'\n'xml tools\" select 'XSL Transformation settings'. \n\n\nIn the popup, click on the elipses: \n...\n to open up the file finder, and select the \n000style.xsl\n stylesheet. \n\n\nClick 'transform'. A new tab will open in Notepad++ \nwith a fully-formed html file displaying your data according to the stylesheet.\n \n\n\nSave this new file and open it in a browser!\n\n\n\n\nYou can also check 'validate' from the XML Tools menu in Notepad++, which will identify errors in your XML. If you're still having errors, a likely culprit might be the way your geographic URLs are encoded. Compare what you've got with what's in the \n1.xml\n reference document.\n\n\nAdvanced:\n If you install a \nWAMP\n or \nMAMP\n server, and put your \nxml\n and \nxsl\n files in the \nWWW\n folder, you \nshould\n be able to see the transformation no problem at \nlocalhost\\myxml.xml\n (for example). (You can also use \nPython's built in webserver if you have Python on your machine\n \n all Mac users for instance do.)\n\n\nYou can access the \nCND.xml\n, transformed into a CSV on my GitHub\n. If you 'view page source', you'll see the original XML again! Save-as the page as \nwhatever-you-want.csv\n and you can do some data mining on it.\n\n\nMore on transformations\n\n\nI made a file I've called \nSG_transformer.xsl\n. Open that file in your text editor. What tags would it be looking for in the xml file? What might it do to your markup? What line would you change in your XML file to get it to point to this stylesheet? Write all this down in your open notebook. It is a good habit to get into to keep track of your thoughts when looking at ancillary files like this.\n\n\nIf the nature of your project will involve a lot of transcription, you would be well advised to use an XML editor like \nOxygenXML\n, which has a free 1 month trial. The editor makes it easy to maintain \nconsistency\n in your markup, and also, to quickly create stylesheets for whatever purpose you need. There are also a number of utility programs freely available that will convert XML to CSV or other formats. One such may be \nfound online on Google code\n. But the best way to transform these XML files is with XSL.", 
            "title": "Text Encoding Initiative"
        }, 
        {
            "location": "/supporting materials/tei/#close-reading-with-tei-text-encoding-initiative", 
            "text": "This worksheet, and all related files, are released CC-BY.  By M. H. Beals ; Adapted for HIST3814o by S Graham   You will need the files in  the Crafting Digital History GitHub module 3 folder .  You need the folder  tei-hist3907 .  Select the Clone or Download button and choose download zip to download that repository as a zip file.   Unzip it somewhere handy on your machine. Inside will be the subfolder named  tei-hist3907 .  Open the subfolder named  tei-hist3907 .   This exercise will explore a historical text and help you create a digital record of your analysis.", 
            "title": "Close Reading with TEI (Text Encoding Initiative)"
        }, 
        {
            "location": "/supporting materials/tei/#vetting-a-website", 
            "text": "Visit the  Recovered Histories Website .  Examine the site's layout and read its introduction.     What makes you believe this site is a trustworthy provider of historical texts?  What makes you believe this site is NOT a trustworthy provider of historical texts?", 
            "title": "Vetting a Website"
        }, 
        {
            "location": "/supporting materials/tei/#finding-a-source", 
            "text": "Visit the site's collections via the 'Browse' function.   Locate the pamphlet  Negro Slavery  by Zachary Macaulay and open it.   This is an abolitionist pamphlet regarding the Atlantic slave trade, presenting and examine evidence of how it is run.  When you approach a primary source like this, it is tempting to read through it from beginning to end, to get an overview of its contents, and then 'mine' or 'cherry-pick' good quotations to include in your assessments.  However, we are going to focus on examining a very small part of the text in a very high level of detail.", 
            "title": "Finding a Source"
        }, 
        {
            "location": "/supporting materials/tei/#setting-up-your-workspace", 
            "text": "You will use your own machine rather than DH Box for this work.    Arrange your workspace so that you have the scanned text of the pamphlet easily visible on one side of your screen.   Open the  blanktemplate.txt  file in  Sublime Text ,  Atom ,  Textwrangler  or  Notepad++  (or any text editor that understands encoding) and have that on the other side of the screen.   The last lines will be  /body /text /TEI /teiCorpus . Everything you write today should be just above  /body .", 
            "title": "Setting Up Your Workspace"
        }, 
        {
            "location": "/supporting materials/tei/#transcribing-your-page", 
            "text": "The first thing you will need is go to the tag  biblScope 1 /biblScope   and replace the number one (1) with the page number you are transcribing. Which page should you transcribe?     Select a page in the document that you find interesting.    Next, you will need to  very carefully  transcribe your page of text from the image into your document.  Make sure you do not make any changes to the text, even if you think they author has used poor grammar or misspelled a word.  You do not need to worry about line breaks but should start every new paragraph (or heading) with a  /p  and end every paragraph (or heading) with a  /p .    Once you have completed your transcription, look away from your computer for 30-45 seconds.  Staring into the distance every 10-20 minutes will keep your eyes from straining.  Also, shake out your hands at the wrists, to prevent repetitive stress injuries to your fingers.", 
            "title": "Transcribing Your Page"
        }, 
        {
            "location": "/supporting materials/tei/#encoding-your-transcription", 
            "text": "You are now going to  encode  or  mark-up  your text.      Re-read your page and highlight / colour the following:   Any persons mentioned (including any he/she if they refer to a specific person)  Any places mentioned  Any claims, assertions or arguments made   Now that you have highlighted these, you are going to put proper code around them.    For persons, surround your text with the following:  persName key=\"Last, First\" from=\"YYYY\" to=\"YYYY\" role=\"Occupation\" ref=\"http://www.website.com/webpage.html\"   /persName    Inside the speech marks for  key , include the real full name of the person mentioned  In  from  and  to , include their birth and death years, using ? for unknown years  In  role , put the occupation, role or 'claim to fame' for this individual.    In  ref , put the URL (link) to the Dictionary of National Biography, Wikipedia or other biography website where you found this information. If there is a   in your link, you will need to replace this with  amp; .        For places, surround your text with the following:  placeName key=\"Sheffield, United Kingdom\" ref=\"http://tools.wmflabs.org/geohack/geohack.php?pagename=Sheffield params=53_23_01_N_1_28_01_W_type:city_region:GB\"\"   /placeName    In  key , put the city and country with best information you can find for the modern names for this location  In  ref , put a link to the relevant coordinates on  Wikipedia GeoHack website . To obtain this, go to the Wikipedia page for this city and click on the latitude/longitude coordinates for the location. For large areas, such as entire countries or continents, just use the Wikipedia page URL.        For claims or arguments, surround your text with the following:  interp key=\"reason\" n=\"citation\" cert=\"high\" ref=\"http://www.website.com/webpage.html\"   /interp    In  key , explain why you believe this claim is true or not  In  n , put a full citation to the relevant source  In  cert  (short for certainty), put: high, medium, low or unknown  In  ref , put the link to the website where you got the information to assess this claim.   When you are happy with your work, hit save your work, give it a useful name, make sure it has  .xml  as the extension, and save it  and the  .xsl  file  to your repository.       Alex Gill has made  The Short and Sweet TEI Handout  which you might want to explore as well. When you embark on encoding documents for your own research,  Northeastern Univeristy has some questions to think about  to help you decide what kinds of tagging you'll need; these  templates from HisTEI  might be useful (open the whole project with OxygenXML for full functionality, but you can copy those templates in any editor).", 
            "title": "Encoding Your Transcription"
        }, 
        {
            "location": "/supporting materials/tei/#viewing-your-encoded-text", 
            "text": "To see your encoded text, make sure your .xml and .xsl file are in the same folder.  Open either Internet Explorer or Firefox . The following will not work in Chrome because it has different security settings.  Making sure both your  (page number).xml  file and your  000style.xsl  file are in the same folder (or both on your desktop), drag the icon for  (page number).xml  into your Internet Explorer Browser window.  If you now see a colour-coded version of your text, Congratulations! If you hover over the coloured sections, you should see a pop-up with the additional information you entered.  If your text comes up only in black, with no paragraph divisions or headings, or doesn't come up at all, something has gone wrong. Re-open your .xml file and check that you have:    Placed  p  at the start of every paragraph, including the start of the page    Placed  /p  at the end of every paragraph, including the end of the page    Made sure all your  persName ,  placeName  and  interp  tags are properly enclosed in  s    Made sure you have both an open   and close  \\  tag for each tag you use    Made sure you attribute values are fully enclosed in  \"\" .    Made sure you have a space between the  \"  of one attribute and the start of the next    Made sure you do NOT have a space after the  =  of an attribute    If your text still does not appear formatted, you may need to remove the text one paragraph at a time, refreshing your browser window, until it appears. This will help you identify which paragraph (or sentence) has the error within it).", 
            "title": "Viewing Your Encoded Text"
        }, 
        {
            "location": "/supporting materials/tei/#if-you-still-dont-see-your-text", 
            "text": "If you do not see the colour-coded version of your text, this might not necessarily mean that you've done something wrong.  Some browsers will not perform the transformation, for security reasons.  In which case, we can do the following:    If you are on a Windows machine using Notepad++, go to 'Plugins'   Plugin Tools. (If you are on Windows but aren't using Notepad++, Sublime and Atom probably have a similar functionality, but you will have to search to figure it out.)   Select 'XML Tools' from the list, and install it.   You'll probably have to restart the program to complete the plugin installation.   Open up the  1.xml  file in Notepad ++.  Under 'plugins' 'xml tools\" select 'XSL Transformation settings'.   In the popup, click on the elipses:  ...  to open up the file finder, and select the  000style.xsl  stylesheet.   Click 'transform'. A new tab will open in Notepad++  with a fully-formed html file displaying your data according to the stylesheet.    Save this new file and open it in a browser!   You can also check 'validate' from the XML Tools menu in Notepad++, which will identify errors in your XML. If you're still having errors, a likely culprit might be the way your geographic URLs are encoded. Compare what you've got with what's in the  1.xml  reference document.  Advanced:  If you install a  WAMP  or  MAMP  server, and put your  xml  and  xsl  files in the  WWW  folder, you  should  be able to see the transformation no problem at  localhost\\myxml.xml  (for example). (You can also use  Python's built in webserver if you have Python on your machine    all Mac users for instance do.)  You can access the  CND.xml , transformed into a CSV on my GitHub . If you 'view page source', you'll see the original XML again! Save-as the page as  whatever-you-want.csv  and you can do some data mining on it.", 
            "title": "If you still don't see your text"
        }, 
        {
            "location": "/supporting materials/tei/#more-on-transformations", 
            "text": "I made a file I've called  SG_transformer.xsl . Open that file in your text editor. What tags would it be looking for in the xml file? What might it do to your markup? What line would you change in your XML file to get it to point to this stylesheet? Write all this down in your open notebook. It is a good habit to get into to keep track of your thoughts when looking at ancillary files like this.  If the nature of your project will involve a lot of transcription, you would be well advised to use an XML editor like  OxygenXML , which has a free 1 month trial. The editor makes it easy to maintain  consistency  in your markup, and also, to quickly create stylesheets for whatever purpose you need. There are also a number of utility programs freely available that will convert XML to CSV or other formats. One such may be  found online on Google code . But the best way to transform these XML files is with XSL.", 
            "title": "More on transformations"
        }, 
        {
            "location": "/supporting materials/git-rstudio/", 
            "text": "Using Git with rstudio-pubs-static\n\n\n\n\n\n\nBefore you can use git to keep track of your changes to your R project, you need to tell the Git program (which keeps snapshots of your changes) who you are. To do this, execute the following commands in the command line:\n    \n$ git config --global user.email \"you@example.com\"\n\n    \n$ git config --global user.name \"Your Name\"\n\n\n\n\n\n\nGo back to RStudio. Under 'Tools' select 'Version Control' then 'project setup'.\n\n\n\n\n\n\nUnder 'Version control system' select the 'Git' tab.\n\n\nYou now have a new option in the pane at top right, beside 'environment' and 'history': 'git'. \n\n\n\n\n\n\nClick on 'git'.\n\n\nThe panel now displays all of the files created in this project folder. \n\n\n\n\n\n\nTick off the files you want to commit. \n\n\n\n\n\n\nClick on the 'Commit' button.\n\n\n\n\nA new window opens called 'RStudio: Review Changes'. This window shows you a preview of the text of each file, in green where material has been added, red where material has been deleted (these are the 'difs'). \n\n\n\n\n\n\nAdd a commit message int the top right 'commit message' box.\n\n\nYou've now made a local commit to your git repository! If things go horribly wrong, you can roll back the changes. Now, let's setup your GitHub repo for this.\n\n\n\n\n\n\nGo to your GitHub account. \n\n\n\n\n\n\nMake a new repository; initialize it with a readme.md\n\n\n\n\n\n\nBack in RStudio, click on the 'more' gearwheel on the Git tab. \n\n\n\n\n\n\nSelect shell (you could do this from the command line too, when you're in your project folder). This will open up a box into which you can type commands; we're going to tell Git the location of our remote repository, add that info into the config, and do two pulls.\n\n\n\n\n\n\nOpen shell and execute the following commands:\n\n\n$ git remote add origin https://github.com/YOUR-ACCOUNT/YOUR-REPO.git\n$ git config remote.origin.url https://github.com/YOUR-ACCOUNT/YOUR-REPO.git\n$ git pull -u origin master\n\n\n\n\n\n\n\nAnd you now can push your changes to your remote repository whenever you make a new commit. There is a variation of markdown called RMarkdown that enables you to embed working R code into a document, and then 'knit' it into HTML or slide shows or PDFs. When you push those to a GitHub repo, you are now making data publications! The official \nR Markdown information can be found on the RStudio website\n.", 
            "title": "Git in RStudio"
        }, 
        {
            "location": "/supporting materials/git-rstudio/#using-git-with-rstudio-pubs-static", 
            "text": "Before you can use git to keep track of your changes to your R project, you need to tell the Git program (which keeps snapshots of your changes) who you are. To do this, execute the following commands in the command line:\n     $ git config --global user.email \"you@example.com\" \n     $ git config --global user.name \"Your Name\"    Go back to RStudio. Under 'Tools' select 'Version Control' then 'project setup'.    Under 'Version control system' select the 'Git' tab.  You now have a new option in the pane at top right, beside 'environment' and 'history': 'git'.     Click on 'git'.  The panel now displays all of the files created in this project folder.     Tick off the files you want to commit.     Click on the 'Commit' button.   A new window opens called 'RStudio: Review Changes'. This window shows you a preview of the text of each file, in green where material has been added, red where material has been deleted (these are the 'difs').     Add a commit message int the top right 'commit message' box.  You've now made a local commit to your git repository! If things go horribly wrong, you can roll back the changes. Now, let's setup your GitHub repo for this.    Go to your GitHub account.     Make a new repository; initialize it with a readme.md    Back in RStudio, click on the 'more' gearwheel on the Git tab.     Select shell (you could do this from the command line too, when you're in your project folder). This will open up a box into which you can type commands; we're going to tell Git the location of our remote repository, add that info into the config, and do two pulls.    Open shell and execute the following commands:  $ git remote add origin https://github.com/YOUR-ACCOUNT/YOUR-REPO.git\n$ git config remote.origin.url https://github.com/YOUR-ACCOUNT/YOUR-REPO.git\n$ git pull -u origin master    And you now can push your changes to your remote repository whenever you make a new commit. There is a variation of markdown called RMarkdown that enables you to embed working R code into a document, and then 'knit' it into HTML or slide shows or PDFs. When you push those to a GitHub repo, you are now making data publications! The official  R Markdown information can be found on the RStudio website .", 
            "title": "Using Git with rstudio-pubs-static"
        }, 
        {
            "location": "/supporting materials/regex/", 
            "text": "A gentle introduction to Regular Expressions\n\n\nThis text is adopted from the first drafts of\n The Macroscope \nwhich is was published by Imperial College Press.\n\n\nIntroduction\n\n\nA regular expression (also called regex) is a powerful tool for finding and manipulating text.  At its simplest, a regular expression is just a way of looking through texts to locate patterns. A regular expression can help you find every line that begins with a number, or every instance of an email address, or whenever a word is used even if there are slight variations in how it's spelled. As long as you can describe the pattern you're looking for, regular expressions can help you find it. Once you've found your patterns, they can then help you manipulate your text so that it fits just what you need.\n\n\nRegular expressions can look pretty complex, but once you know the basic syntax and vocabulary, simple \u2018regexes\u2019 will be easy. Regular expressions can often be used right inside the 'Find and Replace' box in many text and document editors, such as Sublime Text, Atom, or Notepad++. You cannot use Microsoft Word, however!\n\n\nNB\n In text editors, you have to indicate that you wish to do a regex search. For instance, in Notepad++ when you do a search, to use regular expressions you must tick off the checkbox enabling them. Otherwise, Notepad++ will treat your search literally, looking for that exact \ntext\n rather than the \npattern\n. Similarly in Textwrangler, you need to tick off the box marked 'grep' when you bring up the search dialogue panel. In Sublime Text, you need to tick the box that has \n.*\n in the search panel to enable regular expression searches.\n\n\nPlease also note that while this information on regex basics was initially written with the text editors Notepad++ and Textwrangler in mind, all that follows applies equally to other text editors that can work with regular expressions.\n\n\nFor now, just read along. In the actual exercise, we will not be using a text editor, but you may wish to some day in the future.\n\n\nSome basic principles\n\n\nProtip:\n there are libraries of regular expressions, online. For example, if you want to find all postal codes, you can search \u201cregular expression Canadian postal code\u201d and learn what \u2018formula\u2019 to search for to find them\n\n\nLet's say you're looking for all the instances of \"cat\" or \"dog\" in your document. When you type the vertical bar on your keyboard (it looks like \n|\n, shift+backslash on windows keyboards), that means 'or' in regular expressions. So, if your query is dog|cat and you press 'find', it will show you the first time either dog or cat appears in your text.\n\n\nIf you want to replace every instance of either \"cat\" or \"dog\" in your document with the world \"animal\", you would open your find-and-replace box, put \ndog|cat\n in the search query, put animal in the 'replace' box, hit 'replace all', and watch your entire document fill up with references to animals instead of dogs and cats.\n\n\nThe astute reader will have noticed a problem with the instructions above; simply replacing every instance of \"dog\" or \"cat\" with \"animal\" is bound to create problems. Simple searches don't differentiate between letters and spaces, so every time \"cat\" or \"dog\" appear within words, they'll also be replaced with \"animal\". \"catch\" will become \"animalch\"; \"dogma\" will become \"animalma\"; \"certificate\" will become \"certifianimale\". In this case, the solution appears simple; put a space before and after your search query, so now it reads:\n\n\ndog | cat\n  \n\n\nWith the spaces, \"animal\" replace \"dog\" or \"cat\" only in those instances where they're definitely complete words; that is, when they're separated by spaces.\n\n\nThe even more astute reader will notice that this still does not solve our problem of replacing every instance of \"dog\" or \"cat\". What if the word comes at the beginning of a line, so it is not in front of a space? What if the word is at the end of a sentence or a clause, and thus followed by a punctuation? Luckily, in the language of regex, you can represent the beginning or end of a word using special characters.\n\n\n\\\n\n\nmeans the beginning of a word. In some programs, like TextWrangler, this is used instead:\n\n\n\\b\n\n\nso if you search for \n\\\ncat\n , (or, in TextWrangler, \n\\bcat\n )it will find \"cat\", \"catch\", and \"catsup\", but not \"copycat\", because your query searched for words beginning with \"cat\". For patterns at the end of the line, you would use:\n\n\n\\\n\n\nor in TextWrangler,\n\n\n\\b\n\n\nagain.  The remainder of this walk-through imagines that you are using Notepad++, but if you\u2019re using Textwrangler, keep this quirk in mind. If you search for\n\n\ncat\\\n\n\nit will find \"cat\" and \"copycat\", but not \"catch,\" because your query searched for words ending with -\"cat\".\n\n\nRegular expressions can be mixed, so if you wanted to find words only matching \"cat\", no matter where in the sentence, you'd search for\n\n\n\\\ncat\\\n\n\nwhich would find every instance. And, because all regular expressions can be mixed, if you searched for (in Notepad++; what would you change, if you were using TextWrangler?)\n\n\n\\\ncat|dog\\\n\n\nand replaced all with \"animal\", you would have a document that replaced all instances of \"dog\" or \"cat\" with \"animal\", no matter where in the sentence they appear. You can also search for variations within a single word using parentheses. For example if you were looking for instances of \"gray\" or \"grey\", instead of the search query\n\n\ngray|grey\n\n\nyou could type\n\n\ngr(a|e)y\n\n\ninstead. The parentheses signify a group, and like the order of operations in arithmetic, regular expressions read the parentheses before anything else. Similarly, if you wanted to find instances of either \"that dog\" or \"that cat\", you would search for:\n\n\n(that dog)|(that cat)\n\n\nNotice that the vertical bar | can appear either inside or outside the parentheses, depending on what you want to search for.\n\n\nThe period character . in regular expressions directs the search to just find any character at all. For example, if we searched for:\n\n\nd.g\n\n\nthe search would return \"dig\", \"dog\", \"dug\", and so forth.\n\n\nAnother special character from our cheat sheet, the plus + instructs the program to find any number of the previous character. If we search for\n\n\ndo+g\n\n\nit would return any words that looked like \"dog\", \"doog\", \"dooog\", and so forth. Adding parentheses before the plus would make a search for repetitions of whatever is in the parentheses, for example querying\n\n\n(do)+g\n\n\nwould return \"dog\", \"dodog\", \"dododog\", and so forth.\n\n\nCombining the plus '+' and period '.' characters can be particularly powerful in regular expressions, instructing the program to find any amount of any characters within your search. A search for\n\n\nd.+g\n\n\nfor example, might return \"dried fruits are g\", because the string begins with \"d\" and ends with \"g\", and has various characters in the middle. Searching for simply \".+\" will yield query results that are entire lines of text, because you are searching for any character, and any amount of them.\n\n\nParentheses in regular expressions are also very useful when replacing text. The text within a regular expression forms what's called a group, and the software you use to search remembers which groups you queried in order of their appearance. For example, if you search for\n\n\n(dogs)( and )(cats)\n\n\nwhich would find all instances of \"dogs and cats\" in your document, your program would remember \"dogs\" is group 1, \"and\" is group 2, and \"cats\" is group 3. Notepad++ remembers them as \n\"\\1\"\n, \n\"\\2\"\n, and \n\"\\3\"\n for each group respectively.\n\n\nIf you wanted to switch the order of \"dogs\" and \"cats\" every time the phrase \"dogs and cats\" appeared in your document, you would type\n\n\n(dogs)( and )(cats)\n\n\nin the 'find' box, and\n\n\n\\3\\2\\1\n\n\nin the 'replace' box. That would replace the entire string with group 3 (\"cats\") in the first spot, group 2 (\" and \") in the second spot, and group 1 (\"dogs\") in the last spot, thus changing the result to \"cats and dogs\".\n\n\nThe vocabulary of regular expressions is pretty large, but there are many cheat sheets for regex online (one that I sometimes use is the \nregex lib cheat sheet\n. Another good one is the \nregex intro from Active State docs\n)\n\n\nNow, continue on to the \nmain exercise", 
            "title": "Introduction to Regular Expressions (regex)"
        }, 
        {
            "location": "/supporting materials/regex/#a-gentle-introduction-to-regular-expressions", 
            "text": "This text is adopted from the first drafts of  The Macroscope  which is was published by Imperial College Press.", 
            "title": "A gentle introduction to Regular Expressions"
        }, 
        {
            "location": "/supporting materials/regex/#introduction", 
            "text": "A regular expression (also called regex) is a powerful tool for finding and manipulating text.  At its simplest, a regular expression is just a way of looking through texts to locate patterns. A regular expression can help you find every line that begins with a number, or every instance of an email address, or whenever a word is used even if there are slight variations in how it's spelled. As long as you can describe the pattern you're looking for, regular expressions can help you find it. Once you've found your patterns, they can then help you manipulate your text so that it fits just what you need.  Regular expressions can look pretty complex, but once you know the basic syntax and vocabulary, simple \u2018regexes\u2019 will be easy. Regular expressions can often be used right inside the 'Find and Replace' box in many text and document editors, such as Sublime Text, Atom, or Notepad++. You cannot use Microsoft Word, however!  NB  In text editors, you have to indicate that you wish to do a regex search. For instance, in Notepad++ when you do a search, to use regular expressions you must tick off the checkbox enabling them. Otherwise, Notepad++ will treat your search literally, looking for that exact  text  rather than the  pattern . Similarly in Textwrangler, you need to tick off the box marked 'grep' when you bring up the search dialogue panel. In Sublime Text, you need to tick the box that has  .*  in the search panel to enable regular expression searches.  Please also note that while this information on regex basics was initially written with the text editors Notepad++ and Textwrangler in mind, all that follows applies equally to other text editors that can work with regular expressions.  For now, just read along. In the actual exercise, we will not be using a text editor, but you may wish to some day in the future.", 
            "title": "Introduction"
        }, 
        {
            "location": "/supporting materials/regex/#some-basic-principles", 
            "text": "Protip:  there are libraries of regular expressions, online. For example, if you want to find all postal codes, you can search \u201cregular expression Canadian postal code\u201d and learn what \u2018formula\u2019 to search for to find them  Let's say you're looking for all the instances of \"cat\" or \"dog\" in your document. When you type the vertical bar on your keyboard (it looks like  | , shift+backslash on windows keyboards), that means 'or' in regular expressions. So, if your query is dog|cat and you press 'find', it will show you the first time either dog or cat appears in your text.  If you want to replace every instance of either \"cat\" or \"dog\" in your document with the world \"animal\", you would open your find-and-replace box, put  dog|cat  in the search query, put animal in the 'replace' box, hit 'replace all', and watch your entire document fill up with references to animals instead of dogs and cats.  The astute reader will have noticed a problem with the instructions above; simply replacing every instance of \"dog\" or \"cat\" with \"animal\" is bound to create problems. Simple searches don't differentiate between letters and spaces, so every time \"cat\" or \"dog\" appear within words, they'll also be replaced with \"animal\". \"catch\" will become \"animalch\"; \"dogma\" will become \"animalma\"; \"certificate\" will become \"certifianimale\". In this case, the solution appears simple; put a space before and after your search query, so now it reads:  dog | cat     With the spaces, \"animal\" replace \"dog\" or \"cat\" only in those instances where they're definitely complete words; that is, when they're separated by spaces.  The even more astute reader will notice that this still does not solve our problem of replacing every instance of \"dog\" or \"cat\". What if the word comes at the beginning of a line, so it is not in front of a space? What if the word is at the end of a sentence or a clause, and thus followed by a punctuation? Luckily, in the language of regex, you can represent the beginning or end of a word using special characters.  \\  means the beginning of a word. In some programs, like TextWrangler, this is used instead:  \\b  so if you search for  \\ cat  , (or, in TextWrangler,  \\bcat  )it will find \"cat\", \"catch\", and \"catsup\", but not \"copycat\", because your query searched for words beginning with \"cat\". For patterns at the end of the line, you would use:  \\  or in TextWrangler,  \\b  again.  The remainder of this walk-through imagines that you are using Notepad++, but if you\u2019re using Textwrangler, keep this quirk in mind. If you search for  cat\\  it will find \"cat\" and \"copycat\", but not \"catch,\" because your query searched for words ending with -\"cat\".  Regular expressions can be mixed, so if you wanted to find words only matching \"cat\", no matter where in the sentence, you'd search for  \\ cat\\  which would find every instance. And, because all regular expressions can be mixed, if you searched for (in Notepad++; what would you change, if you were using TextWrangler?)  \\ cat|dog\\  and replaced all with \"animal\", you would have a document that replaced all instances of \"dog\" or \"cat\" with \"animal\", no matter where in the sentence they appear. You can also search for variations within a single word using parentheses. For example if you were looking for instances of \"gray\" or \"grey\", instead of the search query  gray|grey  you could type  gr(a|e)y  instead. The parentheses signify a group, and like the order of operations in arithmetic, regular expressions read the parentheses before anything else. Similarly, if you wanted to find instances of either \"that dog\" or \"that cat\", you would search for:  (that dog)|(that cat)  Notice that the vertical bar | can appear either inside or outside the parentheses, depending on what you want to search for.  The period character . in regular expressions directs the search to just find any character at all. For example, if we searched for:  d.g  the search would return \"dig\", \"dog\", \"dug\", and so forth.  Another special character from our cheat sheet, the plus + instructs the program to find any number of the previous character. If we search for  do+g  it would return any words that looked like \"dog\", \"doog\", \"dooog\", and so forth. Adding parentheses before the plus would make a search for repetitions of whatever is in the parentheses, for example querying  (do)+g  would return \"dog\", \"dodog\", \"dododog\", and so forth.  Combining the plus '+' and period '.' characters can be particularly powerful in regular expressions, instructing the program to find any amount of any characters within your search. A search for  d.+g  for example, might return \"dried fruits are g\", because the string begins with \"d\" and ends with \"g\", and has various characters in the middle. Searching for simply \".+\" will yield query results that are entire lines of text, because you are searching for any character, and any amount of them.  Parentheses in regular expressions are also very useful when replacing text. The text within a regular expression forms what's called a group, and the software you use to search remembers which groups you queried in order of their appearance. For example, if you search for  (dogs)( and )(cats)  which would find all instances of \"dogs and cats\" in your document, your program would remember \"dogs\" is group 1, \"and\" is group 2, and \"cats\" is group 3. Notepad++ remembers them as  \"\\1\" ,  \"\\2\" , and  \"\\3\"  for each group respectively.  If you wanted to switch the order of \"dogs\" and \"cats\" every time the phrase \"dogs and cats\" appeared in your document, you would type  (dogs)( and )(cats)  in the 'find' box, and  \\3\\2\\1  in the 'replace' box. That would replace the entire string with group 3 (\"cats\") in the first spot, group 2 (\" and \") in the second spot, and group 1 (\"dogs\") in the last spot, thus changing the result to \"cats and dogs\".  The vocabulary of regular expressions is pretty large, but there are many cheat sheets for regex online (one that I sometimes use is the  regex lib cheat sheet . Another good one is the  regex intro from Active State docs )", 
            "title": "Some basic principles"
        }, 
        {
            "location": "/supporting materials/regex/#now-continue-on-to-the-main-exercise", 
            "text": "", 
            "title": "Now, continue on to the main exercise"
        }, 
        {
            "location": "/supporting materials/regexex/", 
            "text": "REGEX and the Republic of Texas\n\n\nRegex comes in several different flavours. A good text editor on your own computer like \nSublime Text\n or \nAtom\n can do Regex searches and replaces from the find-and-replace box; Word cannot do that. Remember, Regex searches for \npatterns\n in the text. The correspondence of the Republic of Texas was collated into a single volume and published with a helpful index in 1911. It was scanned and OCR'd by Google, and is now available as a text file from the Internet Archive. You can see the OCR'd text at \narchive.org\n. We are going to grab the index from that file, and transform it using regex.\n\n\nThere are several hundred entries in that index. You could clean them up by hand, deleting and cutting and pasting, but with the power of regex, we'll go from this:\n\n\nSam Houston to A. B. Roman, September 12, 1842 101\nSam Houston to A. B. Roman, October 29, 1842 101\nCorrespondence for 1843-1846 \u2014\nIsaac Van Zandt to Anson Jones, January 11, 1843 103\n\n\n\n\n...to nicely CSV-formatted table like this:\n\n\nSam Houston, A. B. Roman, September 12 1842\nSam Houston, A. B. Roman, October 29 1842\nIsaac Van Zandt, Anson Jones, January 11 1843\n\n\n\n\nThe change doesn't look like much, and you might think to yourself, 'hey, I could just do that by hand'. You could but it'd take you ages, and if you made a mistake somewhere \n are you sure you could do this consistently, for a couple of hours at a time? Probably not. Your time is better spent figuring out the search and replace patterns, and then setting your machine loose to implement it.\n\n\nData formatted like this could be fed into a network analysis program, for instance, or otherwise visualized and analyzed (which we will do in the next module). Regex as we are going to use in this tutorial allows us to go from unstructured to structured data.\n\n\nGetting started\n\n\nIn the previous module, we learned how to automatically grab text from sites like Canadiana. In this particular exercise today, we'll quickly download the file using \ncurl\n (it's like wget, though there are some \ndifferences between the two commands\n. It's good to know both).\n\n\n\n\n\n\nAt the command line, type \n$ curl http://archive.org/stream/diplomaticcorre33statgoog/diplomaticcorre33statgoog_djvu.txt \n texas.txt\n\n\nThe \ncurl\n command downloads the txt file and the the \n pushes the result of the curl command to a file called \ntexas.txt\n.\n\n\n\n\n\n\nOpen \ntexas.txt\n with Nano and delete everything except for the index of the list of letters (we just want the index). \n\n\n\n\na. To select a lot of text in Nano, you set a starting point (a mark) with ctrl+shift+6 (the 'carat' symbol: ^). \n\n\nb. Then hit the down arrow on your keyboard, and you will highlight the text. \n\n\nc. When you've selected everything you want, hit ctrl+k to cut the text.\n\n\n\n\nThat is, you\u2019re looking for the table of letters, starting with \u2018Sam Houston to J. Pinckney Henderson, December 31, 1836 51\u2019 and ending with \u2018Wm. Henry Daingerfield to Ebenezer Allen, February 2, 1846 1582\u2019. Your file will now have approximately 2000 lines in it.\n\n\nNotice that there is a lot of text that we are not interested in at the moment: page numbers, headers, footers, or categories. We're going to use regular expressions to get rid of them. What we want to end up with is a spreadsheet that is arranged in three columns:\n\n\nSender, Recipient, Date\n\n\n\n\n\n\n\nScroll down through the text; notice there are many lines which don't include a letter, because they're either header info, or blank, or some other extraneous text. We're going to get rid of all of those lines too.\n\n\nWe want to keep every line that has this information in it: Sender to Recipient, Month, Date, Year, Page\n\n\n\n\n\n\nSave the file in nano: ctrl+x, Y, enter \n\n\n\n\n\n\n\n\n\n\n\n\nWARNING: Regex can be very tricky. When I'm working with Regex, I copy and paste some of the text I'm working on into the box at \nRegExr\n and fiddle with the pattern until it does what I want. In fact, spend some time looking at their examples before you go any further in this exercise.\n\n\nThe workflow\n\n\nWe start by finding every line that looks like a reference to a letter, and put a tilde (a \n~\n symbol) at the beginning of it so we know to save it for later. Next, we get rid of all the lines that don't start with tildes, so that we're left with only the relevant text. After this is done, we format the remaining text by putting commas in appropriate places, so we can import it into a spreadsheet and do further edits there.\n\n\nWe're going to use the \nsed\n and \ngrep\n commands at the command prompt in our DH Box. Sed works by first identifying text that matches a pattern, and then swapping in the text we want to have, like the following:\n\n\n$ sed 's/old text/new text/g' filename\n\n\nand grep works in the following way:\n\n\n$ grep 'PATTERN WE WANT' inputfile\n which will print the results to the screen. If we want to redirect the results to a new file, we add this to the end: \n outputfile\n.\n\n\nStep One\n\n\nIdentifying Lines that have Correspondence Senders and Receivers in them\n\n\nDiscussion:\n Read in full before doing any manipulation of your text!\n\n\nIf you were using a text editor on your own computer like, for instance\n\n\n\n\nNotepad++:\n press ctrl+f or search-\nfind to open the find dialogue box. In that box, go to the 'Replace' tab, and check the radio box for 'Regular expression' at the bottom of the search box. \n\n\nTextWrangler:\n hit command+f to open the find and replace dialogue box. Tick off the \u2018grep\u2019 radio button (which tells TextWrangler that we want to do a regex search) and the \u2018wraparound\u2019 button (which tells TextWrangler to search everywhere). \n\n\nSublime text:\n command+f opens the 'find' box (and shift+command+f opens find \nand\n replace). Tick the '.*' button to tell Sublime we're working with regular expressions.\n\n\n\n\nHowever, we're going to use the two commands sed and grep at the command prompt in DH Box\n.\n\n\nRemember from our basic introduction that there's a way to see if the word \"to\" appears in full. \n\n\n\n\n\n\nType \n$ grep '\\bto\\b' texas.txt\n\n\nThe results print out to the screen. This command finds every instance of the word \"to\" (and not, for instance, also \u2018potato\u2019 or \u2018tomorrow\u2019 \n try \ngrep 'to' texas.txt\n instead to see the difference).\n\n\nWe don't just want to find \"to\", but the entire line that contains it. We assume that every line that contains the word \u201cto\u201d in full is a line that has relevant letter information, and every line that does not is one we do not need. \n\n\nYou learned earlier that the query \n.+\n returns any amount of text, no matter what it says. Thus, the pattern that we will build when we are ready to use the \nsed\n command (where the 's' means 'stream' and 'ed' means 'editor') will include \n.+\\bto\\b.+\n so that we edit every line which includes the word \"to\" in full, no matter what comes before or after it, and none of the lines which don't.\n\n\nAs mentioned earlier, we want to add a tilde \n~\n before each of the lines that look like letters, so we can save them for later. This involves the find-and-replace function, and a query identical to the one before, but with parentheses around it, so it looks like\n\n\n(.+\\\nto\\\n)\n\n\n\nand the entire line is placed within a parenthetical group. Since this the first group in our search expression, we can replace that group with \n\\1\n and put the tilde in front of it like so: \n~\\1\n.\n\n\n\n\n\n\nCopy and paste some of your text into \nRegExr.com\n. \n\n\n\n\n\n\nWrite your regular expression (ie. what you're trying to find), and your substitution (ie. what you're replace with) in the RegExr interface. \n\n\n\n\n\n\nOnce you're satisfied that you've got it right, we put the complete expression into our sed command:\n\n\nsed -r -i.bak 's/(.+\\bto\\b.+)/~\\1/g' texas.txt\n\n\nWhere:\n\n\n\n\n-r\n means extended regex. this saves us from having to 'escape' certain characters.\n\n\n-i.bak\n means make a backup of the original input file, in case things go wrong.\n\n\n-'s/old-pattern/newpattern/g'\n is how we find and switch what we're looking for the final g means 'globally', everywhere in the file.\n\n\ntexas.txt\n the filename that we're looking to change.\n\n\n\n\n\nWhen you hit enter, the computer seems to pause for a moment, and then gives you the command prompt again. \n\n\n\n\n\n\nType \nls\n and you'll see that a new file, \ntexas.txt.bak\n has been created.\n\n\n\n\n\n\nType \nnano texas.txt\n and examine the file. You should now have \n~\n characters at the start of each entry of the index!\n\n\n\n\n\n\n\n\n\n\n\n\nIf for some reason you don't, or you've mangled your original file, you can replace texas.txt with the backup file you made like so: \n$ mv old-file-name new-file-name\n thus, \n$ mv texas.txt.bak texas.txt\n. Use Nano to confirm that you're back to where you needed to be, and try again.\n\n\nStep Two\n\n\nRemoving Lines that Aren\u2019t Relevant\n\n\nDiscussion\n\n\nAfter running the find-and-replace, you should note your document now has most of the lines with tildes in front of it, and a few which do not. The next step is to remove all the lines that do not include a tilde. \nIf you were using a text editor on your own computer\n, the search string to find all lines which don't begin with tildes is \n\\n[^~].+\n\n\nA \n\\n\n at the beginning of a query searches for a new line, which means it's going to start searching at the first character of each new line.  \n\n\nHowever, given the evolution of computing, it may well be that this won\u2019t quite work on your system.\n\n\nLinux based systems use \n\\n\n for a new line, while Windows often uses \n\\r\\n\n, and older Macs just use \n\\r\n. These are the sorts of things that can drive us crazy, and so we digital historians need to keep in mind! Since this will likely cause much frustration, your safest bet will be to save a copy of what you are working on, and then experiment to see what gives you the best result. In most cases, the syntax will be the following:\n\n\n\\r\\n[^~].+\n\n\n\n\nWithin a set of square brackets \n[]\n the carrot \n^\n means search for anything that isn't within these brackets; in this case, the tilde \n~\n. The  \n.+\n as before means search for all the rest of the characters in the line as well. All together, the query returns any full line which does not begin with a tilde; that is, the lines we did not mark as looking like letters.\n\n\nBy finding all \n\\r\\n[^~].+\n and replacing it with nothing, you effectively delete all the lines that don't look like the index entries. What you're left with is a series of entries, and a series of blank lines.\n\n\nBut DH Box makes this so much easier\n\n\nWe are simply going to get grep to find all the lines that have a tilde in them, and write them to a new file: \n$ grep '~' texas.txt \n index.txt\n.\n\n\nUse Nano to confirm that this is true. \n\n\nWasn't that easy?\n\n\n\n\n\n\n\n\nStep Three\n\n\nTransforming into CSV format\n\n\nDiscussion\n\n\nTo turn this text file into a spreadsheet, we'll want to separate it out into one column for sender, one for recipient, and one for date, each separated by a single comma. Notice that most lines have extraneous page numbers attached to them; we can get rid of those with regular expressions. There's also usually a comma separating the month-date and the year, which we'll get rid of as well. In the end, the first line should go from looking like the following:\n\n\n    ~Sam Houston to J. Pinckney Henderson, December 31, 1836 51\n\n\n\nto looking like the following:\n\n\n    Sam Houston, J. Pinckney Henderson, December 31 1836\n\n\n\nsuch that each data point is in its own column.\n\n\nYou will start by removing the page number after the year and the comma between the year and the month-date. To do this, first locate the year on each line by using the regex:\n\n\n    [0-9]{4}\n\n\n\nWe can find any digit between 0 and 9 by searching for \n[0-9]\n, and \n{4}\n will find four of them together. Now extend that search out by appending \n.+\n to the end of the query; as seen before, it will capture the entire rest of the line. The following query:\n\n\n    [0-9]{4}.+\n\n\n\nwill return, for example, \"1836 51\", \"1839 52\", and \"1839 53\" from the first three lines of the text. We also want to capture the comma preceding the year, so add a comma and a space before the query, resulting in the following: \n\n\n     , [0-9]{4}.+\n\n\n\nwhich will return \", 1836 51\", \", 1839 52\", etc.\n\n\nThe next step is making the parenthetical groups which will be used to remove parts of the text with find-and-replace. In this case, we want to remove the comma and everything after year, but not the year or the space before it. Thus our query will look like the following:\n\n\n    (,)( [0-9]{4})(.+)\n\n\n\nwith the comma as the first group \n\"\\1\"\n, the space and the year as the second \n\"\\2\"\n, and the rest of the line as the third \n\"\\3\"\n.  Given that all we care about retaining is the second group (we want to keep the year, but not the comma or the page number), what will the \nreplace\n look like?\n\n\nFind the dates using a regex, and replace so that only the \nsecond\n group in the expression is kept. You might want to consult the introduction to regex again before you execute this one.\n\n\nRemember, the first part of the sed command will be: \nsed -r -i.bak\n then the pattern to find, the pattern to replace with, and the file name. You want to use sed on the new \nindex.txt\n file you made. Can you devise the right pattern?\n\n\n\n\n\n\n\n\nStep Four\n\n\nRemoving the tildes\n*\n\n\n\n\nFind the tildes that we used to mark off our text of interest, and replace them with nothing to delete them.\n\n\n\n\n\n\n\n\n\n\nStep Five\n\n\nSeparating Senders and Receivers\n\n\nDiscussion\n\n\nFinally, to separate the sender and recipient by a comma, we find all instances of the word \"to\" and replace it with a comma. Although we used \n\\b\n and \n\\b\n to denote the beginning and end of a word earlier in the lesson, we don't exactly do that here. We include the space preceding \u201cto\u201d in the regular expression, as well as the \n\\b\n to denote the word ending. Once we find instances of the word and the space preceding it, \nto\\b\n we replace it with a comma \n,\n.\n\n\n\n\nDevise the regex to find the word, and replace with a comma.\n\n\n\n\n\n\n\n\n\n\nStep Six\n\n\nCleaning up\n\n\nDiscussion\n\n\nYou may notice that some lines still do not fit our criteria. Line 22, for example, reads \"Abner S. Lipscomb, James Hamilton and A. T. Bumley, AugUHt 15, \". It has an incomplete date; these we don't need to worry about for our purposes. More worrisome are lines, like 61 \"Copy and summary of instructions United States Department of State, \" which include none of the information we want. We can get rid of these lines later in a spreadsheet.  \n\n\nThe only non-standard lines we need to worry about with regular expressions are the ones with more than 2 commas, like line 178, \"A. J. Donelson, Secretary of State [Allen,. arf interim], December 10 1844\". Notice that our second column, the name of the Recipient, has a comma inside of it. If you were to import this directly into a spreadsheet, you would get four columns, one for Sender, two for Recipient, and one for date, which would break any analysis you would then like to run. Unfortunately these lines need to be fixed by hand, but happily regular expressions make finding them easy. The query:\n\n\n.+,.+,.+,\n\n\nwill show you every line with more than 2 commas, because it finds any line that has any set of characters, then a comma, then any other set, then another comma, and so forth. Use grep to find these.  \n\n\n\n\nAt the top of the file, add a new line that simply reads \"Sender, Recipient, Date\". These will be the column headers. Make a copy as a CSV file by using the \ncp\n command: \ncp index.txt cleaned-correspondence.csv\n.\n\n\n\n\nCongratulations!\n\n\n\n\n\n\n\n\nYou've now used regex to extract, transform, and clean historical text. As a CSV file, you could now load this data into a network analysis program such as \nGephi\n to explore the ramifications of this correspondence network. Upload your file to your repository, and make a note of the original location of the file, the transformations that you've done, and the date/time. You will be using your \ncleaned-correspondence.csv\n file in the next exercise using \nOpen Refine\n, where we'll sort out some of the messy OCR (fixing names, and so on).\n\n\nThe pattern you're after:\n\n\nThe pattern you want in step three is\n\nsed -r -i.bak 's/(,)( [0-9]{4})(.+)/\\2/g' index.txt\n.\n\n\nThe pattern for step four is\n\nsed -r -i.bak 's/~//g' index.txt\n.\n\n\nThe pattern for step five is\n\nsed -r -i.bak 's/(\\b to \\b)/,/g' index.txt\n\n\nThe pattern for step six is\n\n\ngrep -E \".+,.+,.+,\" index.txt\n\n\nIn DH Box, the command will use \n-r\n\n\ngrep -r \".+,.+,.+,\" index.txt\n\n\nThe \n-E\n tells grep to treat the pattern as an extended regex (regex with a few more bells and whistles). On DH Box, the flag would be \n-r\n.", 
            "title": "Regex & the Republic of Texas"
        }, 
        {
            "location": "/supporting materials/regexex/#regex-and-the-republic-of-texas", 
            "text": "Regex comes in several different flavours. A good text editor on your own computer like  Sublime Text  or  Atom  can do Regex searches and replaces from the find-and-replace box; Word cannot do that. Remember, Regex searches for  patterns  in the text. The correspondence of the Republic of Texas was collated into a single volume and published with a helpful index in 1911. It was scanned and OCR'd by Google, and is now available as a text file from the Internet Archive. You can see the OCR'd text at  archive.org . We are going to grab the index from that file, and transform it using regex.  There are several hundred entries in that index. You could clean them up by hand, deleting and cutting and pasting, but with the power of regex, we'll go from this:  Sam Houston to A. B. Roman, September 12, 1842 101\nSam Houston to A. B. Roman, October 29, 1842 101\nCorrespondence for 1843-1846 \u2014\nIsaac Van Zandt to Anson Jones, January 11, 1843 103  ...to nicely CSV-formatted table like this:  Sam Houston, A. B. Roman, September 12 1842\nSam Houston, A. B. Roman, October 29 1842\nIsaac Van Zandt, Anson Jones, January 11 1843  The change doesn't look like much, and you might think to yourself, 'hey, I could just do that by hand'. You could but it'd take you ages, and if you made a mistake somewhere   are you sure you could do this consistently, for a couple of hours at a time? Probably not. Your time is better spent figuring out the search and replace patterns, and then setting your machine loose to implement it.  Data formatted like this could be fed into a network analysis program, for instance, or otherwise visualized and analyzed (which we will do in the next module). Regex as we are going to use in this tutorial allows us to go from unstructured to structured data.", 
            "title": "REGEX and the Republic of Texas"
        }, 
        {
            "location": "/supporting materials/regexex/#getting-started", 
            "text": "In the previous module, we learned how to automatically grab text from sites like Canadiana. In this particular exercise today, we'll quickly download the file using  curl  (it's like wget, though there are some  differences between the two commands . It's good to know both).    At the command line, type  $ curl http://archive.org/stream/diplomaticcorre33statgoog/diplomaticcorre33statgoog_djvu.txt   texas.txt  The  curl  command downloads the txt file and the the   pushes the result of the curl command to a file called  texas.txt .    Open  texas.txt  with Nano and delete everything except for the index of the list of letters (we just want the index).    a. To select a lot of text in Nano, you set a starting point (a mark) with ctrl+shift+6 (the 'carat' symbol: ^).   b. Then hit the down arrow on your keyboard, and you will highlight the text.   c. When you've selected everything you want, hit ctrl+k to cut the text.   That is, you\u2019re looking for the table of letters, starting with \u2018Sam Houston to J. Pinckney Henderson, December 31, 1836 51\u2019 and ending with \u2018Wm. Henry Daingerfield to Ebenezer Allen, February 2, 1846 1582\u2019. Your file will now have approximately 2000 lines in it.  Notice that there is a lot of text that we are not interested in at the moment: page numbers, headers, footers, or categories. We're going to use regular expressions to get rid of them. What we want to end up with is a spreadsheet that is arranged in three columns:  Sender, Recipient, Date    Scroll down through the text; notice there are many lines which don't include a letter, because they're either header info, or blank, or some other extraneous text. We're going to get rid of all of those lines too.  We want to keep every line that has this information in it: Sender to Recipient, Month, Date, Year, Page    Save the file in nano: ctrl+x, Y, enter        WARNING: Regex can be very tricky. When I'm working with Regex, I copy and paste some of the text I'm working on into the box at  RegExr  and fiddle with the pattern until it does what I want. In fact, spend some time looking at their examples before you go any further in this exercise.", 
            "title": "Getting started"
        }, 
        {
            "location": "/supporting materials/regexex/#the-workflow", 
            "text": "We start by finding every line that looks like a reference to a letter, and put a tilde (a  ~  symbol) at the beginning of it so we know to save it for later. Next, we get rid of all the lines that don't start with tildes, so that we're left with only the relevant text. After this is done, we format the remaining text by putting commas in appropriate places, so we can import it into a spreadsheet and do further edits there.  We're going to use the  sed  and  grep  commands at the command prompt in our DH Box. Sed works by first identifying text that matches a pattern, and then swapping in the text we want to have, like the following:  $ sed 's/old text/new text/g' filename  and grep works in the following way:  $ grep 'PATTERN WE WANT' inputfile  which will print the results to the screen. If we want to redirect the results to a new file, we add this to the end:   outputfile .", 
            "title": "The workflow"
        }, 
        {
            "location": "/supporting materials/regexex/#step-one", 
            "text": "Identifying Lines that have Correspondence Senders and Receivers in them  Discussion:  Read in full before doing any manipulation of your text!  If you were using a text editor on your own computer like, for instance   Notepad++:  press ctrl+f or search- find to open the find dialogue box. In that box, go to the 'Replace' tab, and check the radio box for 'Regular expression' at the bottom of the search box.   TextWrangler:  hit command+f to open the find and replace dialogue box. Tick off the \u2018grep\u2019 radio button (which tells TextWrangler that we want to do a regex search) and the \u2018wraparound\u2019 button (which tells TextWrangler to search everywhere).   Sublime text:  command+f opens the 'find' box (and shift+command+f opens find  and  replace). Tick the '.*' button to tell Sublime we're working with regular expressions.   However, we're going to use the two commands sed and grep at the command prompt in DH Box .  Remember from our basic introduction that there's a way to see if the word \"to\" appears in full.     Type  $ grep '\\bto\\b' texas.txt  The results print out to the screen. This command finds every instance of the word \"to\" (and not, for instance, also \u2018potato\u2019 or \u2018tomorrow\u2019   try  grep 'to' texas.txt  instead to see the difference).  We don't just want to find \"to\", but the entire line that contains it. We assume that every line that contains the word \u201cto\u201d in full is a line that has relevant letter information, and every line that does not is one we do not need.   You learned earlier that the query  .+  returns any amount of text, no matter what it says. Thus, the pattern that we will build when we are ready to use the  sed  command (where the 's' means 'stream' and 'ed' means 'editor') will include  .+\\bto\\b.+  so that we edit every line which includes the word \"to\" in full, no matter what comes before or after it, and none of the lines which don't.  As mentioned earlier, we want to add a tilde  ~  before each of the lines that look like letters, so we can save them for later. This involves the find-and-replace function, and a query identical to the one before, but with parentheses around it, so it looks like  (.+\\ to\\ )  and the entire line is placed within a parenthetical group. Since this the first group in our search expression, we can replace that group with  \\1  and put the tilde in front of it like so:  ~\\1 .    Copy and paste some of your text into  RegExr.com .     Write your regular expression (ie. what you're trying to find), and your substitution (ie. what you're replace with) in the RegExr interface.     Once you're satisfied that you've got it right, we put the complete expression into our sed command:  sed -r -i.bak 's/(.+\\bto\\b.+)/~\\1/g' texas.txt  Where:   -r  means extended regex. this saves us from having to 'escape' certain characters.  -i.bak  means make a backup of the original input file, in case things go wrong.  -'s/old-pattern/newpattern/g'  is how we find and switch what we're looking for the final g means 'globally', everywhere in the file.  texas.txt  the filename that we're looking to change.   \nWhen you hit enter, the computer seems to pause for a moment, and then gives you the command prompt again.     Type  ls  and you'll see that a new file,  texas.txt.bak  has been created.    Type  nano texas.txt  and examine the file. You should now have  ~  characters at the start of each entry of the index!       If for some reason you don't, or you've mangled your original file, you can replace texas.txt with the backup file you made like so:  $ mv old-file-name new-file-name  thus,  $ mv texas.txt.bak texas.txt . Use Nano to confirm that you're back to where you needed to be, and try again.", 
            "title": "Step One"
        }, 
        {
            "location": "/supporting materials/regexex/#step-two", 
            "text": "Removing Lines that Aren\u2019t Relevant  Discussion  After running the find-and-replace, you should note your document now has most of the lines with tildes in front of it, and a few which do not. The next step is to remove all the lines that do not include a tilde.  If you were using a text editor on your own computer , the search string to find all lines which don't begin with tildes is  \\n[^~].+  A  \\n  at the beginning of a query searches for a new line, which means it's going to start searching at the first character of each new line.    However, given the evolution of computing, it may well be that this won\u2019t quite work on your system.  Linux based systems use  \\n  for a new line, while Windows often uses  \\r\\n , and older Macs just use  \\r . These are the sorts of things that can drive us crazy, and so we digital historians need to keep in mind! Since this will likely cause much frustration, your safest bet will be to save a copy of what you are working on, and then experiment to see what gives you the best result. In most cases, the syntax will be the following:  \\r\\n[^~].+  Within a set of square brackets  []  the carrot  ^  means search for anything that isn't within these brackets; in this case, the tilde  ~ . The   .+  as before means search for all the rest of the characters in the line as well. All together, the query returns any full line which does not begin with a tilde; that is, the lines we did not mark as looking like letters.  By finding all  \\r\\n[^~].+  and replacing it with nothing, you effectively delete all the lines that don't look like the index entries. What you're left with is a series of entries, and a series of blank lines.  But DH Box makes this so much easier  We are simply going to get grep to find all the lines that have a tilde in them, and write them to a new file:  $ grep '~' texas.txt   index.txt .  Use Nano to confirm that this is true.   Wasn't that easy?", 
            "title": "Step Two"
        }, 
        {
            "location": "/supporting materials/regexex/#step-three", 
            "text": "Transforming into CSV format  Discussion  To turn this text file into a spreadsheet, we'll want to separate it out into one column for sender, one for recipient, and one for date, each separated by a single comma. Notice that most lines have extraneous page numbers attached to them; we can get rid of those with regular expressions. There's also usually a comma separating the month-date and the year, which we'll get rid of as well. In the end, the first line should go from looking like the following:      ~Sam Houston to J. Pinckney Henderson, December 31, 1836 51  to looking like the following:      Sam Houston, J. Pinckney Henderson, December 31 1836  such that each data point is in its own column.  You will start by removing the page number after the year and the comma between the year and the month-date. To do this, first locate the year on each line by using the regex:      [0-9]{4}  We can find any digit between 0 and 9 by searching for  [0-9] , and  {4}  will find four of them together. Now extend that search out by appending  .+  to the end of the query; as seen before, it will capture the entire rest of the line. The following query:      [0-9]{4}.+  will return, for example, \"1836 51\", \"1839 52\", and \"1839 53\" from the first three lines of the text. We also want to capture the comma preceding the year, so add a comma and a space before the query, resulting in the following:        , [0-9]{4}.+  which will return \", 1836 51\", \", 1839 52\", etc.  The next step is making the parenthetical groups which will be used to remove parts of the text with find-and-replace. In this case, we want to remove the comma and everything after year, but not the year or the space before it. Thus our query will look like the following:      (,)( [0-9]{4})(.+)  with the comma as the first group  \"\\1\" , the space and the year as the second  \"\\2\" , and the rest of the line as the third  \"\\3\" .  Given that all we care about retaining is the second group (we want to keep the year, but not the comma or the page number), what will the  replace  look like?  Find the dates using a regex, and replace so that only the  second  group in the expression is kept. You might want to consult the introduction to regex again before you execute this one.  Remember, the first part of the sed command will be:  sed -r -i.bak  then the pattern to find, the pattern to replace with, and the file name. You want to use sed on the new  index.txt  file you made. Can you devise the right pattern?", 
            "title": "Step Three"
        }, 
        {
            "location": "/supporting materials/regexex/#step-four", 
            "text": "Removing the tildes *   Find the tildes that we used to mark off our text of interest, and replace them with nothing to delete them.", 
            "title": "Step Four"
        }, 
        {
            "location": "/supporting materials/regexex/#step-five", 
            "text": "Separating Senders and Receivers  Discussion  Finally, to separate the sender and recipient by a comma, we find all instances of the word \"to\" and replace it with a comma. Although we used  \\b  and  \\b  to denote the beginning and end of a word earlier in the lesson, we don't exactly do that here. We include the space preceding \u201cto\u201d in the regular expression, as well as the  \\b  to denote the word ending. Once we find instances of the word and the space preceding it,  to\\b  we replace it with a comma  , .   Devise the regex to find the word, and replace with a comma.", 
            "title": "Step Five"
        }, 
        {
            "location": "/supporting materials/regexex/#step-six", 
            "text": "Cleaning up  Discussion  You may notice that some lines still do not fit our criteria. Line 22, for example, reads \"Abner S. Lipscomb, James Hamilton and A. T. Bumley, AugUHt 15, \". It has an incomplete date; these we don't need to worry about for our purposes. More worrisome are lines, like 61 \"Copy and summary of instructions United States Department of State, \" which include none of the information we want. We can get rid of these lines later in a spreadsheet.    The only non-standard lines we need to worry about with regular expressions are the ones with more than 2 commas, like line 178, \"A. J. Donelson, Secretary of State [Allen,. arf interim], December 10 1844\". Notice that our second column, the name of the Recipient, has a comma inside of it. If you were to import this directly into a spreadsheet, you would get four columns, one for Sender, two for Recipient, and one for date, which would break any analysis you would then like to run. Unfortunately these lines need to be fixed by hand, but happily regular expressions make finding them easy. The query:  .+,.+,.+,  will show you every line with more than 2 commas, because it finds any line that has any set of characters, then a comma, then any other set, then another comma, and so forth. Use grep to find these.     At the top of the file, add a new line that simply reads \"Sender, Recipient, Date\". These will be the column headers. Make a copy as a CSV file by using the  cp  command:  cp index.txt cleaned-correspondence.csv .   Congratulations!     You've now used regex to extract, transform, and clean historical text. As a CSV file, you could now load this data into a network analysis program such as  Gephi  to explore the ramifications of this correspondence network. Upload your file to your repository, and make a note of the original location of the file, the transformations that you've done, and the date/time. You will be using your  cleaned-correspondence.csv  file in the next exercise using  Open Refine , where we'll sort out some of the messy OCR (fixing names, and so on).", 
            "title": "Step Six"
        }, 
        {
            "location": "/supporting materials/regexex/#the-pattern-youre-after", 
            "text": "The pattern you want in step three is sed -r -i.bak 's/(,)( [0-9]{4})(.+)/\\2/g' index.txt .  The pattern for step four is sed -r -i.bak 's/~//g' index.txt .  The pattern for step five is sed -r -i.bak 's/(\\b to \\b)/,/g' index.txt  The pattern for step six is  grep -E \".+,.+,.+,\" index.txt  In DH Box, the command will use  -r  grep -r \".+,.+,.+,\" index.txt  The  -E  tells grep to treat the pattern as an extended regex (regex with a few more bells and whistles). On DH Box, the flag would be  -r .", 
            "title": "The pattern you're after:"
        }, 
        {
            "location": "/supporting materials/open-refine/", 
            "text": "An Introduction to Open Refine\n\n\nThis text was adopted from the first drafts of The Macroscope\n\n\nAn alternative Open Refine exercise is offered by \nThomas Padilla\n and you may wish to give it a try instead. That would be acceptable.\n\n\nInstall Open Refine\n\n\nOpen Refine (formerly Google Refine) is a powerful tool for working with messy data: cleaning it; transforming it from one format into another; extending it with web services; and linking it to databases like Freebase.\n\n\nThis exercise \ndoes not use\n DH Box.\n\n\nIn this exercise, we are going to use a tool that originated with Google. Since 2012, it has been open source and freely available online. Using it takes a bit of getting used to, however. \n\n\n\n\nVisit the \nOpen Refine home page\n and watch the three videos. \n\n\nDownload \nOpen Refine to your machine\n.\n\n\nFollow the installation instructions. \n\n\nStart Open Refine by double clicking on its icon. This will open a new browser window, pointing to \nhttp://127.0.0.1:3333\n. This location is your own computer, so even though it looks like it\u2019s running on the internet, it isn\u2019t. The \u20183333\u2019 is a \u2018port\u2019, meaning that Open Refine is running much like a server, serving up a webpage via that port to the browser. (If the browser window doesn't open automatically, open one and put \nhttp://127.0.0.1:3333\n in the address bar).\n\n\n\n\n\n\n\n\n\n\nStart Cleaning our Texan Correspondence\n\n\nMake sure you have your data handy that you created in exercise 1, the Texan correspondence. You can get it out of your DH Box by using the DH Box filemanager. \n\n\n\n\nNavigate to where you were working on it, then click on the file name. This will download it to your downloads folder.\n\n\nMove your working file to somewhere safe on your computer.\n\n\nStart a new project by clicking on the \u2018create project\u2019 tab on the left hand side of the screen. \n\n\nClick on \u2018choose files\u2019 and select the Texan correspondence CSV file. Open Refine will load this data, and it will give you a preview of your data. \n\n\nName your project in the box on the top right side (eg., 'm3-exercise2' or similar) and then click \u2018create project\u2019. It may take a few minutes.\n\n\nOnce your project has started, one of the columns that should be visible in your data is \u2018sender\u2019. Click on the arrow to the left of \"Sender\" in OpenRefine and select Facet -\n Text Facet. Do the same with the arrow next to \"Recipient\". A box will appear on the left side of the browser showing all 189 names listed as senders in the spreadsheet. The spreadsheet itself is nearly a thousand rows, so immediately we see that, in this correspondence collection, some names are used multiple times. You may also have noticed that many of the names suffered from errors in the text scan (OCR or Optical Character Recognition errors), rendering some identical names from the book as similar, but not the same, in the spreadsheet. For example the recipient \"Juan de Dios Cafiedo\" is occasionally listed as \"Juan de Dios CaAedo\". Any subsequent analysis will need these errors to be cleared up, and OpenRefine will help fix them.\n\n\nWithin the \"Sender\" facet box on the left-hand side, click on the button labeled \"Cluster\". This feature presents various automatic ways of merging values that appear to be the same. \n\n\nPlay with the values in the drop-down boxes and notice how the number of clusters found change depending on which method is used. Because the methods are slightly different, each will present different matches that may or may not be useful. \n\n\nIf you see two values which should be merged, e.g. \"Ashbel Smith\" and \". Ashbel Smith\", check the box to the right in the 'Merge' column and click the 'Merge Selected \n Re-Cluster' button below.\n\n\nGo through the various cluster methods one-at-a-time, including changing number values, and merge the values which appear to be the same. \"Juan de Dios CaAedo\" clearly should be merged with \"Juan de Dios Cafiedo\", however \"Correspondent in Tampico\" probably should not be merged with \"Correspondent at Vera Cruz.\" Since we are not experts, we will have to use our best judgement in these cases \n or get cracking on some more research to help us make the call. By the end, you should have reduced the number of unique Senders from 189 to around 150. \n\n\nRepeat these steps with Recipients, reducing unique Recipients from 192 to about 160. \n\n\nTo finish the automatic cleaning of the data, click the arrow next to \"Sender\" and select 'Edit Cells -\n Common transforms -\n Trim leading and trailing whitespace'. \n\n\nRepeat step 12 for \"Recipient\". The resulting spreadsheet will not be perfect, but it will be much easier to clean by hand than it would have been before taking this step. \n\n\nClick on \u2018export\u2019 at the top right of the window to get your data back out as a .csv file.\n\n\n\n\n\n\n\n\n\n\nNow what?\n\n\nThe text you've just cleaned could now be loaded into something like \nPalladio\n or \nGephi\n or \nConnect the Dots\n for network analysis! However, every network analysis program has its own idiosyncracies. Gephi and Connect the Dots, for instance, can import lists of relationships if the CSV file has columns labelled 'source' and 'target'. (Connect the Dots will \nonly\n accept those two columns, so you'd have to delete the date column if you wanted to give that a try.)\n\n\nLet's assume that's where we want to visualize and analyze this data. In order to get this correspondence data into a network visualization tool, we will have to rename the \"Sender\" column to \"source\" and the \"Recipient\" column to \"target\". You could do this in a spreadsheet, of course. But since you have Open Refine running: \n\n\n\n\nIn the arrow to the left of Sender in the main OpenRefine window, select Edit column -\n Rename this column, and rename the column \"source\".\n\n\nIn the arrow to the left of Recipient in the main OpenRefine window, select Edit column -\n Rename this column, and rename the column \"target\".\n\n\nIn the top right of the window, select 'Export -\n Custom tabular exporter'.\n\n\nNotice that \"source\", \"target\", and \"Date\" are checked in the content tab; uncheck \"Date\", as it will not be used in Gephi (networks where the nodes have different dates, ie. dynamic networks, are beyond us for the moment). \n\n\nGo to the download tab and change the download option from 'Tab-separated values (TSV)' to 'Comma-separated values (CSV)' and press download. The file will likely download to your automatic download directory. We will revisit this file later. \n\n\nGo ahead and drop this file into the Palladio interface. Do you see any interesting patterns? Make a note!\n\n\nUpload your cleaned file with a new name back \ninto\n your DH Box; we will use this in the next module.\n\n\n\n\n\n\n\n\n\n\nRemember to copy your notes and any other information/observations/thoughts to your GitHub repo\n\n\nOptional: Going further with Open Refine \n Named Entity Extraction\n\n\nSay we wanted, instead of the correspondence network, a visualization of all the places named in this body of letters. It might be interesting to visualize on a map the changing focus of Texas' diplomatic attention over time. There is a plugin for Open Refine that does what is called \nNamed Entity Extraction\n. The plugin, and how to install \n use it, is available on the \nFree your metadata website\n.\n\n\n\n\nUse regex on your original document containing the letters to clean up the data so that you have one letter per line (rather than the index \n did you notice that the full text of all the letters was in the original file?).\n\n\nImport the file into Open Refine\n\n\nExtract Named Entities\n\n\nVisualize the results in a spreadsheet\n\n\nWrite up a 'how to' in your notebook explaining these steps in detail.\n\n\n\n\nAn interesting use case is discussed online\n and on the \nFree your metadata website (PDF opens in new tab)\n.\n\n\nFurther Help\n Visit \nKalani Craig's page on \n\n\nOptional: Exploring other Named Entity Extraction tools\n\n\nVoyant Tools RezoViz\n\n\nVoyant-Tools\n is a text analysis suite that we will explore in more depth in the next module. Feel free to load your material into it and begin exploring; there's nothing you can break. \n\n\nOne interesting tool is called 'RezoViz', which will extract entities and tie them together into a network based on appearing in the same document. \n\n\n\n\nUpload some of your Canadian war diary texts to Voyant-Tools.\n\n\nIn the top right, there's a 'save' icon. \n\n\nSelect 'url for a different tool/skin'. \n\n\nSelect 'RezoViz' from the tools list that pops up. A new URL will appear in the box. \n\n\nCopy, paste into a new browser window (works best on Chrome).\n\n\n\n\nWhat kinds of questions could this answer?\n\n\nStanford NER\n\n\n\n\n\n\nDownload \nStanford NER\n.\n\n\n\n\n\n\nMac instructions for Stanford NER\n. The link is to Michelle Moravec's instructions, for Mac.\n\n\n\n\n\n\nWindows: If you're on windows and want to do this, things are a bit more complicated. Download and unzip the NER package.\n\n\n\n\n\n\nOpen a command prompt in the Stanford NER folder on your Windows machine (you can right-click on the folder in your windows explorer, and select \u2018open command prompt here\u2019).\n\n\n\n\n\n\nChanging the file names as appropriate, type the following as a single line (highlight the text with your mouse \n it scrolls to the right beyond the page, and then copy it):\n\n\njava -mx500m -cp stanford-ner.jar edu.stanford.nlp.ie.crf.CRFClassifier -loadClassifier classifiers/english.all.3class.distsim.crf.ser.gz -textFile texas-letters.txt -outputFormat inlineXML \n \u201cmy-ner-output.txt\u201d\n\n\n\n\n\n\n\nThe first bit, \njava \u2013mx500m\n says how much memory to use. If you have 1gb of memory available, you can type \njava \u2013mx 1g\n (or 2g, or 3g, etc). \n\n\n\n\nThe next part of the command calls the NER programme itself. You can set which classifier to use after the \n\u2013loadClassifier classifiers/\n by typing in the exact file name for the classifier you wish to use (you are telling \u2018loadClassifier\u2019 the exact path to the classifier). \n\n\nAt \n\u2013textFile\n you give it the name of your input file (on our machine, called \ntexas-letters.txt\n, and then specify the outputFormat. \n\n\nThe \n character sends the output to a new text file, here called \nmy-ner-output.txt\n. \n\n\n\n\n\n\n\n\nHit enter, and a few moments later the programme will tell you something along the lines of the following:\n\n\nCRFCLassifier tagged 375281 words in 13745 documents at 10833.43 words per second\n\n\n\n\n\n\n\nOpen the text file in your text editor, and you\u2019ll see output like the following:\n\n\nIn the name of the \nLOCATION\n Republic of Texas \n/LOCATION\n, Free, Sovereign and Independent. \nTo all whom these Presents shall come or may in any wise concern. I \nPERSON\n Sam Houston \n/PERSON\n \nPresident thereof send Greeting\n\n\n\n\n\n\n\nCongratulations! You've tagged a body of letters. What next? You could organize this into XML, you could visualize, you could regex to find and extract all of your locations, or persons, or...", 
            "title": "Open Refine"
        }, 
        {
            "location": "/supporting materials/open-refine/#an-introduction-to-open-refine", 
            "text": "This text was adopted from the first drafts of The Macroscope  An alternative Open Refine exercise is offered by  Thomas Padilla  and you may wish to give it a try instead. That would be acceptable.", 
            "title": "An Introduction to Open Refine"
        }, 
        {
            "location": "/supporting materials/open-refine/#install-open-refine", 
            "text": "Open Refine (formerly Google Refine) is a powerful tool for working with messy data: cleaning it; transforming it from one format into another; extending it with web services; and linking it to databases like Freebase.  This exercise  does not use  DH Box.  In this exercise, we are going to use a tool that originated with Google. Since 2012, it has been open source and freely available online. Using it takes a bit of getting used to, however.    Visit the  Open Refine home page  and watch the three videos.   Download  Open Refine to your machine .  Follow the installation instructions.   Start Open Refine by double clicking on its icon. This will open a new browser window, pointing to  http://127.0.0.1:3333 . This location is your own computer, so even though it looks like it\u2019s running on the internet, it isn\u2019t. The \u20183333\u2019 is a \u2018port\u2019, meaning that Open Refine is running much like a server, serving up a webpage via that port to the browser. (If the browser window doesn't open automatically, open one and put  http://127.0.0.1:3333  in the address bar).", 
            "title": "Install Open Refine"
        }, 
        {
            "location": "/supporting materials/open-refine/#start-cleaning-our-texan-correspondence", 
            "text": "Make sure you have your data handy that you created in exercise 1, the Texan correspondence. You can get it out of your DH Box by using the DH Box filemanager.    Navigate to where you were working on it, then click on the file name. This will download it to your downloads folder.  Move your working file to somewhere safe on your computer.  Start a new project by clicking on the \u2018create project\u2019 tab on the left hand side of the screen.   Click on \u2018choose files\u2019 and select the Texan correspondence CSV file. Open Refine will load this data, and it will give you a preview of your data.   Name your project in the box on the top right side (eg., 'm3-exercise2' or similar) and then click \u2018create project\u2019. It may take a few minutes.  Once your project has started, one of the columns that should be visible in your data is \u2018sender\u2019. Click on the arrow to the left of \"Sender\" in OpenRefine and select Facet -  Text Facet. Do the same with the arrow next to \"Recipient\". A box will appear on the left side of the browser showing all 189 names listed as senders in the spreadsheet. The spreadsheet itself is nearly a thousand rows, so immediately we see that, in this correspondence collection, some names are used multiple times. You may also have noticed that many of the names suffered from errors in the text scan (OCR or Optical Character Recognition errors), rendering some identical names from the book as similar, but not the same, in the spreadsheet. For example the recipient \"Juan de Dios Cafiedo\" is occasionally listed as \"Juan de Dios CaAedo\". Any subsequent analysis will need these errors to be cleared up, and OpenRefine will help fix them.  Within the \"Sender\" facet box on the left-hand side, click on the button labeled \"Cluster\". This feature presents various automatic ways of merging values that appear to be the same.   Play with the values in the drop-down boxes and notice how the number of clusters found change depending on which method is used. Because the methods are slightly different, each will present different matches that may or may not be useful.   If you see two values which should be merged, e.g. \"Ashbel Smith\" and \". Ashbel Smith\", check the box to the right in the 'Merge' column and click the 'Merge Selected   Re-Cluster' button below.  Go through the various cluster methods one-at-a-time, including changing number values, and merge the values which appear to be the same. \"Juan de Dios CaAedo\" clearly should be merged with \"Juan de Dios Cafiedo\", however \"Correspondent in Tampico\" probably should not be merged with \"Correspondent at Vera Cruz.\" Since we are not experts, we will have to use our best judgement in these cases   or get cracking on some more research to help us make the call. By the end, you should have reduced the number of unique Senders from 189 to around 150.   Repeat these steps with Recipients, reducing unique Recipients from 192 to about 160.   To finish the automatic cleaning of the data, click the arrow next to \"Sender\" and select 'Edit Cells -  Common transforms -  Trim leading and trailing whitespace'.   Repeat step 12 for \"Recipient\". The resulting spreadsheet will not be perfect, but it will be much easier to clean by hand than it would have been before taking this step.   Click on \u2018export\u2019 at the top right of the window to get your data back out as a .csv file.", 
            "title": "Start Cleaning our Texan Correspondence"
        }, 
        {
            "location": "/supporting materials/open-refine/#now-what", 
            "text": "The text you've just cleaned could now be loaded into something like  Palladio  or  Gephi  or  Connect the Dots  for network analysis! However, every network analysis program has its own idiosyncracies. Gephi and Connect the Dots, for instance, can import lists of relationships if the CSV file has columns labelled 'source' and 'target'. (Connect the Dots will  only  accept those two columns, so you'd have to delete the date column if you wanted to give that a try.)  Let's assume that's where we want to visualize and analyze this data. In order to get this correspondence data into a network visualization tool, we will have to rename the \"Sender\" column to \"source\" and the \"Recipient\" column to \"target\". You could do this in a spreadsheet, of course. But since you have Open Refine running:    In the arrow to the left of Sender in the main OpenRefine window, select Edit column -  Rename this column, and rename the column \"source\".  In the arrow to the left of Recipient in the main OpenRefine window, select Edit column -  Rename this column, and rename the column \"target\".  In the top right of the window, select 'Export -  Custom tabular exporter'.  Notice that \"source\", \"target\", and \"Date\" are checked in the content tab; uncheck \"Date\", as it will not be used in Gephi (networks where the nodes have different dates, ie. dynamic networks, are beyond us for the moment).   Go to the download tab and change the download option from 'Tab-separated values (TSV)' to 'Comma-separated values (CSV)' and press download. The file will likely download to your automatic download directory. We will revisit this file later.   Go ahead and drop this file into the Palladio interface. Do you see any interesting patterns? Make a note!  Upload your cleaned file with a new name back  into  your DH Box; we will use this in the next module.      Remember to copy your notes and any other information/observations/thoughts to your GitHub repo", 
            "title": "Now what?"
        }, 
        {
            "location": "/supporting materials/open-refine/#optional-going-further-with-open-refine-named-entity-extraction", 
            "text": "Say we wanted, instead of the correspondence network, a visualization of all the places named in this body of letters. It might be interesting to visualize on a map the changing focus of Texas' diplomatic attention over time. There is a plugin for Open Refine that does what is called  Named Entity Extraction . The plugin, and how to install   use it, is available on the  Free your metadata website .   Use regex on your original document containing the letters to clean up the data so that you have one letter per line (rather than the index   did you notice that the full text of all the letters was in the original file?).  Import the file into Open Refine  Extract Named Entities  Visualize the results in a spreadsheet  Write up a 'how to' in your notebook explaining these steps in detail.   An interesting use case is discussed online  and on the  Free your metadata website (PDF opens in new tab) .  Further Help  Visit  Kalani Craig's page on", 
            "title": "Optional: Going further with Open Refine &mdash; Named Entity Extraction"
        }, 
        {
            "location": "/supporting materials/open-refine/#optional-exploring-other-named-entity-extraction-tools", 
            "text": "", 
            "title": "Optional: Exploring other Named Entity Extraction tools"
        }, 
        {
            "location": "/supporting materials/open-refine/#voyant-tools-rezoviz", 
            "text": "Voyant-Tools  is a text analysis suite that we will explore in more depth in the next module. Feel free to load your material into it and begin exploring; there's nothing you can break.   One interesting tool is called 'RezoViz', which will extract entities and tie them together into a network based on appearing in the same document.    Upload some of your Canadian war diary texts to Voyant-Tools.  In the top right, there's a 'save' icon.   Select 'url for a different tool/skin'.   Select 'RezoViz' from the tools list that pops up. A new URL will appear in the box.   Copy, paste into a new browser window (works best on Chrome).   What kinds of questions could this answer?", 
            "title": "Voyant Tools RezoViz"
        }, 
        {
            "location": "/supporting materials/open-refine/#stanford-ner", 
            "text": "Download  Stanford NER .    Mac instructions for Stanford NER . The link is to Michelle Moravec's instructions, for Mac.    Windows: If you're on windows and want to do this, things are a bit more complicated. Download and unzip the NER package.    Open a command prompt in the Stanford NER folder on your Windows machine (you can right-click on the folder in your windows explorer, and select \u2018open command prompt here\u2019).    Changing the file names as appropriate, type the following as a single line (highlight the text with your mouse   it scrolls to the right beyond the page, and then copy it):  java -mx500m -cp stanford-ner.jar edu.stanford.nlp.ie.crf.CRFClassifier -loadClassifier classifiers/english.all.3class.distsim.crf.ser.gz -textFile texas-letters.txt -outputFormat inlineXML   \u201cmy-ner-output.txt\u201d    The first bit,  java \u2013mx500m  says how much memory to use. If you have 1gb of memory available, you can type  java \u2013mx 1g  (or 2g, or 3g, etc).    The next part of the command calls the NER programme itself. You can set which classifier to use after the  \u2013loadClassifier classifiers/  by typing in the exact file name for the classifier you wish to use (you are telling \u2018loadClassifier\u2019 the exact path to the classifier).   At  \u2013textFile  you give it the name of your input file (on our machine, called  texas-letters.txt , and then specify the outputFormat.   The   character sends the output to a new text file, here called  my-ner-output.txt .      Hit enter, and a few moments later the programme will tell you something along the lines of the following:  CRFCLassifier tagged 375281 words in 13745 documents at 10833.43 words per second    Open the text file in your text editor, and you\u2019ll see output like the following:  In the name of the  LOCATION  Republic of Texas  /LOCATION , Free, Sovereign and Independent. \nTo all whom these Presents shall come or may in any wise concern. I  PERSON  Sam Houston  /PERSON  \nPresident thereof send Greeting    Congratulations! You've tagged a body of letters. What next? You could organize this into XML, you could visualize, you could regex to find and extract all of your locations, or persons, or...", 
            "title": "Stanford NER"
        }, 
        {
            "location": "/supporting materials/ner/", 
            "text": "Using the Stanford NER to tag a corpus\n\n\nIn our regular expressions example, we were able to extract some of the metadata from the document because it was more or less already formatted in such a way that we could write a pattern to find it. Sometimes however clear-cut patterns are not quite as easy to apply. For instance, what if we were interested in the place names that appear in the documents? What if we suspected that the focus of diplomatic activity shifted over time? This is where \u2018named entity recognition\u2019 can be useful. \n\n\nNamed entity recognition covers a broad range of techniques, based on machine learning and statistical models of language to laboriously trained classifiers using dictionaries. One of the easiest to use out-of-the-box is the Stanford Named Entity Recognizer.  In essence, we tell it \u2018here is a block of text \u2013 classify!\u2019 It will then process through the text, looking at the structure of your text and matching it against its statistical models of word use to identify person, organization, and locations. One can also expand that classification to extract time, money, percent, and date. \n\n\nGrab the Stanford NER\n\n\nLet us use the NER to extract person, organization, and locations.\n\n\n\n\nFirst, download the \nStanford NER\n and extract it to your machine. \n\n\nOpen the location where you extracted the files. \n\n\nOn a Mac, double-click on the one called \nner-gui.command\n. (Mac Users: there is also an \nexcellent tutorial from Michelle Moravec you may wish to consult\n.\n\n\nOn PC, double-click on ner-gui.bat. This opens up a new window (using Java) with \u2018Stanford Named Entity Recognizer\u2019 and also a terminal window. \n\n\n\n\nDon\u2019t touch the terminal window for now. (\nPC users, hang on a moment\n \u2013 there is a bit more that you need to know before you can use this tool successfully. You will have to use the command line in order to get the output out).\n\n\nRunning the NER via its GUI\n\n\nIn the \u2018Stanford Named Entity Recognizer\u2019 window there is some default text. \n\n\n\n\n\n\nClick inside this window and delete the text.\n\n\n\n\n\n\nClick on \u2018File\u2019 and then \u2018Open,\u2019 and select your text for the diplomatic correspondence of the Republic of Texas (that you should still have from earlier modules in this course). Since this text file contains a lot of extraneous information in it \u2013 information which we are not currently interested in, including the publishing information and the index table of letters \u2013 you should open the file in a text editor first and delete that information. \nWe want just the letters for this exercise.\n \n\n\n\n\n\n\nSave with a new name and then open it using \u2018File \n open\u2019 in the Stanford NER. The file will open within the window. \n\n\n\n\n\n\nIn the Stanford NER window, click on \u2018classifier\u2019 then \u2018load CRF from file\u2019. \n\n\n\n\n\n\nNavigate to where you unzipped the Stanford NER folder. \n\n\n\n\n\n\nClick on the \u2018classifier\u2019 folder. There are a number of files here; the ones that you are interested in end with .gz:\n\n\n\n\nenglish.all.3class.distsim.crf.ser.gz\n\n\nenglish.all.4class.distsim.crf.ser.gz\n\n\neglish.muc.7class.distsim.crf.ser.gz\n\n\n\n\nThese files correspond to these entities to extract:\n\n\n\n\n3class:   Location, Person, Organization\n\n\n4class:   Location, Person, Organization, Misc\n\n\n7class:   Time, Location, Organization, Person, Money, Percent, Date\n\n\n\n\n\n\n\n\nSelect the location, person, and organization classifier (ie. 3class)\n\n\n\n\n\n\nPress \u2018Run NER.\u2019 \n\n\nAt this point, the program will appear to \u2018hang\u2019 \u2013 nothing much will seem to be happening. However, in the background, the program has started to process your text. Depending on the size of your text, this could take anywhere from a few minutes to a few hours. Be patient! Watch the terminal window \u2013 once the program has results for you, these will start to scroll by in the terminal window. \n\n\nIn the main program window, once the entire text has processed, the text will appear with colour-coded highlighting showing which words are location words, which ones are persons, which ones are organizations. You have now classified a text. \n\n\nNote:\n sometimes your computer may run out of memory \u2013 in that case, you\u2019ll see an error referring to \u201cOut of Heap Space\u201d in your terminal window. That\u2019s OK \u2013 just copy and paste a smaller bit of the document, say the first 10,000 lines or so. Then try again.\n\n\n\n\n\n\nManipulating that data\n\n\nMac users can grab the data and paste it elsewhere; PC users will have to run the NER from the command line to get usable output.\n\n\nMac Users\n\n\nOn a Mac, you can copy and paste the output from the terminal window into your text editor of choice. It will look something like: \n\n\nLOCATION: Texas\n\nPERSON: Moore\n\nORGANIZATION: Suprema\n\n\nAnd so forth. Once you've pasted it into Textwrangler (or whatever text editor you use), you can now use regular expressions to manipulate the text further. More in a moment.\n\n\nPC Users\n\n\nOn a PC, things are not so simple because the command window only shows a small fraction of the complete output \u2013 you cannot copy and paste it all! What we have to do instead is type in a command at the command prompt, rather than using the graphical interface, and then redirect the output into a new text file. \n\n\n\n\n\n\nOpen a command prompt in the Stanford NER folder on your Windows machine (you can right-click on the folder in your windows explorer, and select \u2018open command prompt here\u2019).\n\n\n\n\n\n\nType the following as a single line:\n\n\njava -mx500m -cp stanford-ner.jar edu.stanford.nlp.ie.crf.CRFClassifier -loadClassifier classifiers/english.all.3class.distsim.crf.ser.gz -textFile texas-letters.txt -outputFormat inlineXML \n \u201cmy-ner-output.txt\u201d\n\n\n\n\n\nThe first bit, \njava \u2013mx500m\n says how much memory to use. If you have 1gb of memory available, you can type \njava \u2013mx 1g\n (or 2g, or 3g, etc). \n\n\nThe next part of the command calls the NER programme itself. You can set which classifier to use after the \n\u2013loadClassifier classifiers/\n by typing in the exact file name for the classifier you wish to use (you are telling \u2018loadClassifier\u2019 the exact path to the classifier). \n\n\nAt \n\u2013textFile\n you give it the name of your input file (on our machine, called \ntexas-letters.txt\n, and then specify the outputFormat. \n\n\nThe \n character sends the output to a new text file, here called \nmy-ner-output.txt\n. \n\n\n\n\n\n\n\n\nHit enter, and a few moments later the programme will tell you something along the lines of\n\n\nCRFCLassifier tagged 375281 words in 13745 documents at 10833.43 words per second\n\n\n\n\n\n\n\nOpen the text file in Notepad++, and you\u2019ll see output like this:\n\n\nIn the name of the \nLOCATION\nRepublic of Texas\n/LOCATION\n, Free, Sovereign and Independent. To all whom these Presents shall come or may in any wise concern. I \nPERSON\nSam Houston\n/PERSON\n President thereof send Greeting\n\n\n\n\n\n\n\nCongratulations \u2013 you\u2019ve successfully tagged a document using a named entity recognizer!\n\n\nNow you need to do a bit more data-munging before you can do anything useful. \nImagine you wanted to eventually visualize this as a network\n. You will need your regex skills again.", 
            "title": "Stanford Named Entity Recognizer"
        }, 
        {
            "location": "/supporting materials/ner/#using-the-stanford-ner-to-tag-a-corpus", 
            "text": "In our regular expressions example, we were able to extract some of the metadata from the document because it was more or less already formatted in such a way that we could write a pattern to find it. Sometimes however clear-cut patterns are not quite as easy to apply. For instance, what if we were interested in the place names that appear in the documents? What if we suspected that the focus of diplomatic activity shifted over time? This is where \u2018named entity recognition\u2019 can be useful.   Named entity recognition covers a broad range of techniques, based on machine learning and statistical models of language to laboriously trained classifiers using dictionaries. One of the easiest to use out-of-the-box is the Stanford Named Entity Recognizer.  In essence, we tell it \u2018here is a block of text \u2013 classify!\u2019 It will then process through the text, looking at the structure of your text and matching it against its statistical models of word use to identify person, organization, and locations. One can also expand that classification to extract time, money, percent, and date.", 
            "title": "Using the Stanford NER to tag a corpus"
        }, 
        {
            "location": "/supporting materials/ner/#grab-the-stanford-ner", 
            "text": "Let us use the NER to extract person, organization, and locations.   First, download the  Stanford NER  and extract it to your machine.   Open the location where you extracted the files.   On a Mac, double-click on the one called  ner-gui.command . (Mac Users: there is also an  excellent tutorial from Michelle Moravec you may wish to consult .  On PC, double-click on ner-gui.bat. This opens up a new window (using Java) with \u2018Stanford Named Entity Recognizer\u2019 and also a terminal window.    Don\u2019t touch the terminal window for now. ( PC users, hang on a moment  \u2013 there is a bit more that you need to know before you can use this tool successfully. You will have to use the command line in order to get the output out).", 
            "title": "Grab the Stanford NER"
        }, 
        {
            "location": "/supporting materials/ner/#running-the-ner-via-its-gui", 
            "text": "In the \u2018Stanford Named Entity Recognizer\u2019 window there is some default text.     Click inside this window and delete the text.    Click on \u2018File\u2019 and then \u2018Open,\u2019 and select your text for the diplomatic correspondence of the Republic of Texas (that you should still have from earlier modules in this course). Since this text file contains a lot of extraneous information in it \u2013 information which we are not currently interested in, including the publishing information and the index table of letters \u2013 you should open the file in a text editor first and delete that information.  We want just the letters for this exercise.      Save with a new name and then open it using \u2018File   open\u2019 in the Stanford NER. The file will open within the window.     In the Stanford NER window, click on \u2018classifier\u2019 then \u2018load CRF from file\u2019.     Navigate to where you unzipped the Stanford NER folder.     Click on the \u2018classifier\u2019 folder. There are a number of files here; the ones that you are interested in end with .gz:   english.all.3class.distsim.crf.ser.gz  english.all.4class.distsim.crf.ser.gz  eglish.muc.7class.distsim.crf.ser.gz   These files correspond to these entities to extract:   3class:   Location, Person, Organization  4class:   Location, Person, Organization, Misc  7class:   Time, Location, Organization, Person, Money, Percent, Date     Select the location, person, and organization classifier (ie. 3class)    Press \u2018Run NER.\u2019   At this point, the program will appear to \u2018hang\u2019 \u2013 nothing much will seem to be happening. However, in the background, the program has started to process your text. Depending on the size of your text, this could take anywhere from a few minutes to a few hours. Be patient! Watch the terminal window \u2013 once the program has results for you, these will start to scroll by in the terminal window.   In the main program window, once the entire text has processed, the text will appear with colour-coded highlighting showing which words are location words, which ones are persons, which ones are organizations. You have now classified a text.   Note:  sometimes your computer may run out of memory \u2013 in that case, you\u2019ll see an error referring to \u201cOut of Heap Space\u201d in your terminal window. That\u2019s OK \u2013 just copy and paste a smaller bit of the document, say the first 10,000 lines or so. Then try again.", 
            "title": "Running the NER via its GUI"
        }, 
        {
            "location": "/supporting materials/ner/#manipulating-that-data", 
            "text": "Mac users can grab the data and paste it elsewhere; PC users will have to run the NER from the command line to get usable output.", 
            "title": "Manipulating that data"
        }, 
        {
            "location": "/supporting materials/ner/#mac-users", 
            "text": "On a Mac, you can copy and paste the output from the terminal window into your text editor of choice. It will look something like:   LOCATION: Texas \nPERSON: Moore \nORGANIZATION: Suprema  And so forth. Once you've pasted it into Textwrangler (or whatever text editor you use), you can now use regular expressions to manipulate the text further. More in a moment.", 
            "title": "Mac Users"
        }, 
        {
            "location": "/supporting materials/ner/#pc-users", 
            "text": "On a PC, things are not so simple because the command window only shows a small fraction of the complete output \u2013 you cannot copy and paste it all! What we have to do instead is type in a command at the command prompt, rather than using the graphical interface, and then redirect the output into a new text file.     Open a command prompt in the Stanford NER folder on your Windows machine (you can right-click on the folder in your windows explorer, and select \u2018open command prompt here\u2019).    Type the following as a single line:  java -mx500m -cp stanford-ner.jar edu.stanford.nlp.ie.crf.CRFClassifier -loadClassifier classifiers/english.all.3class.distsim.crf.ser.gz -textFile texas-letters.txt -outputFormat inlineXML   \u201cmy-ner-output.txt\u201d   The first bit,  java \u2013mx500m  says how much memory to use. If you have 1gb of memory available, you can type  java \u2013mx 1g  (or 2g, or 3g, etc).   The next part of the command calls the NER programme itself. You can set which classifier to use after the  \u2013loadClassifier classifiers/  by typing in the exact file name for the classifier you wish to use (you are telling \u2018loadClassifier\u2019 the exact path to the classifier).   At  \u2013textFile  you give it the name of your input file (on our machine, called  texas-letters.txt , and then specify the outputFormat.   The   character sends the output to a new text file, here called  my-ner-output.txt .      Hit enter, and a few moments later the programme will tell you something along the lines of  CRFCLassifier tagged 375281 words in 13745 documents at 10833.43 words per second    Open the text file in Notepad++, and you\u2019ll see output like this:  In the name of the  LOCATION Republic of Texas /LOCATION , Free, Sovereign and Independent. To all whom these Presents shall come or may in any wise concern. I  PERSON Sam Houston /PERSON  President thereof send Greeting    Congratulations \u2013 you\u2019ve successfully tagged a document using a named entity recognizer!  Now you need to do a bit more data-munging before you can do anything useful.  Imagine you wanted to eventually visualize this as a network . You will need your regex skills again.", 
            "title": "PC Users"
        }, 
        {
            "location": "/supporting materials/regex-ner/", 
            "text": "Further Munging the output of NER\n\n\nSo, you've learned how to tag a corpus using the Stanford NER (visit \nGitHub for that exercise again, as a reminder\n). There are several ways we might like to visualize that output.\n\n\nUnfortunately, depending on what you want to \ndo\n with that data, you are going to have to go back to the data-munging cycle. Let us imagine that we wanted to visualize the locations mentioned in the letters as a kind of network.\n\n\nWe will use regular expressions to further manipulate the data into a useful source -\n target list.\n\n\nREGEX on Mac to Useful Output (PC, please skip to next section):\n\n\nWe need to organize those locations so that locations mentioned in a letter are connected to each other. To begin, we trim away any line that does not start with LOCATION.\n\n\n\n\nWe do this by using our regular expression skills from module 3, and adding in a few more commands:\n\n\n\n\nFIND: \n^(?!.*LOCATION).*$\n\n\nand replace with nothing. We had a few new commands in there: the \n?!\n tells it to begin looking ahead for the literal phrase LOCATION, and then the dot and dollar sign let us know to end at the end of the line. In any case, this will delete everything that doesn't have the location tag in front.\n\n\n\n\n\n\nNow, let us also mark those blank lines we noticed as the start of a new letter.\n\n\nFIND: \n^\\s*$\n\n\nwhere:\n\n^\n marks the beginning of a line\n\n\n$\n marks the end of a line\n\n\n\\s\n indicates \u2018whitespace\u2019\n\n\n\\*\n indicates a zero or more repetition of the thing in front of it (in this case, whitespace)\n\n\nand we replace with the phrase, \u201cblankspace\u201d. Because there might be a few blank lines, you might see \u2018blankspace\u2019 in places. Where you\u2019ve got \u2018blankspaceblankspace\u2019, that\u2019s showing us the spaces between the original documents (whereas one \u2018blankspace\u2019 was where an ORGANIZATION or PERSON tag was removed).\n\n\n\n\n\n\nAt this point, we might want to remove spaces between place names and use underscores instead, so that when we finally get this into a comma separated format, places like \u2018United States\u2019 will be \u2018United_States\u2019 rather than \u2018united, states\u2019.\n\n\nFind:  \n-a single space\n\nReplace: \n\\_\n\n\n\n\n\n\nNow, we want to reintroduce the space after \u2018LOCATION:\u2019, so\n\n\nFind: \n:_\n       \n-this is a colon, followed by an underscore\n\nReplace: \n:\n \n-this is a colon, followed by a space\n\n\n\n\n\n\nNow we want to get all of the locations for a single letter into a single line. So we want to get rid of new lines and LOCATION:\n\n\nFind: \n\\n(LOCATION:)\n\nReplace:\n\n\n\n\n\n\nIt is beginning to look like a list. Let\u2019s replace \u2018blankspaceblankspace\u2019 with \u2018new-document\u2019.\n\n\nFind: \nblankspaceblankspace\n\nReplace: new-document\n\n\n\n\n\n\nAnd now let\u2019s get those single blankspace lines excised:\n\n\nFind \n\\n(blankspace)\n\nReplace:\n\n\nNow you have something that looks like this:\n\n\np\n\nnew-document Houston Republic_of_Texas United_States Texas\nbr\n\nnew-document Texas\nbr\n\nnew-document United_States Town_of_Columbia\nbr\n\nnew-document United_States Texas\nbr\n\nnew-document United_States United_States_of_ Republic_of_Texas\nbr\n\nnew-document New_Orleans United_States_Govern\nbr\n\nnew-document United_States Houston United_States Texas united_states\nbr\n\nnew-document United_States New_Orleans United_States\nbr\n\nnew-document Houston United_States Texas United_States\nbr\n\nnew-document New_Orleans\nbr\n\n\n/p\n\n\n\n\n\n\n\n\nWhy not leave \u2018new-document\u2019 in the file? It\u2019ll make a handy marker. Let\u2019s replace the spaces with commas:\n\n\nFind: \n     \n-a single space\n\nReplace: \n,\n\n\n\n\n\n\nSave your work as a \n****.csv\n file. You now have a file that you can load into a variety of other platforms or tools to perform your analysis. Of course, NER is not perfect, especially when dealing with names like \u2018Houston\u2019 that were a personal name long before they were a place name.\n\n\n\n\n\n\nREGEX on PC to Useful Output:\n\n\n\n\n\n\nOpen your tagged file in Notepad++. There are a lot of carriage returns, line breaks, and white spaces in this document that will make our regex very complicated and will in fact break it periodically, if we try to work with it as it is. Instead, let us remove all new lines and whitespaces and the outset, and use regex to put structure back. \n\n\n\n\n\n\nTo begin, we want to get the entire document into a single line:\n\n\nFind: \n\\n\n\nReplace with nothing.\n\n\nNotepad++ reports the number of lines in the document at the bottom of the window. It should now report lines: 1.\n\n\n\n\n\n\nLet\u2019s introduce line breaks to correspond with the original letters (ie, a line break signals a new letter), since we want to visualize the network of locations mentioned where we join places together on the basis of being mentioned in the same letter. If we examine the original document, one candidate for something to search for, to signal a new letter, could be  \nPERSON\n to \nPERSON\n but the named entity recognizer sometimes confuses persons for places or organizations; the object character recognition also makes things complicated.  Since the letters are organized chronologically, and we can see \u2018Digitized by Google\u2019 at the bottom of every page (and since no writer in the 1840s is going to use the word digitized) let\u2019s use that as our page break marker. For the most part, one letter corresponds to one page in the book. It\u2019s not ideal, but this is something that the historian will have to discuss in her methods and conclusions. Perhaps, over the seven hundred odd letters in this collection, it doesn\u2019t actually make much difference.\n\n\nFind: \n(digitized)\n\nReplace:\n\\n\\1\n\n\nYou now have on the order of 700 odd lines in the document.\n\n\n\n\n\n\nLet\u2019s find the locations and put them on individual lines, so that we can strip out everything else.\n\n\nFind: \n(LOCATION)(.\\*?)(LOCATION)\n\nReplace: \n\\n\\2\n\n\nIn the search string above, the \n.\\*\n would look for everything between the first \nlocation\n on the line and the last \nlocation\nin the line, so we would get a lot of junk. By using \n.\\*?\n we just get the text between \nlocation\nand the next \nlocation\n tag.\n\n\n\n\n\n\nWe need to replace the \n and \n from the location tags now, as those are characters with special meanings in our regular expressions. Turn off the regular expressions in notepad ++ by unticking the \u2018regular expression\u2019 box. Now search for \n, \n, and \\ in turn, replacing them with tildes:\n\n\nLOCATION\nTexas\n/LOCATION\n becomes \n~Texas~~~\n\n\n\n\n\n\nNow we want to delete in each line everything that isn\u2019t a location. Our locations start the line and are trailed by three tildes, so we just need to find those three tildes and everything that comes after, and delete. Turn regular expressions back on in the search window.\n\n\nFind: \n(~~~)(.*)\n\nReplace with nothing.\n\n\n\n\n\n\nOur marker for a new page in this book was the line that began with \u2018Digitized by\u2019. We need to delete that line in its entirety, leaving a blank line.\n\n\nFind: \n(Digitized)(.\\*)\n\nReplace: \n\\n\n\n\n\n\n\n\nNow we want to get those locations all in the same line again. We need to find the new line character followed by the tilde, and then delete that new line, replacing it with a comma:\n\n\nFind: \n(\\n)(~)\n\nReplace: \n,\n\n\n\n\n\n\nNow we remove extraneous blank lines.\n\n\nFind: \n\\s*$\n\nReplace with nothing.\n\n\nWe\u2019re almost there. \n\n\n\n\n\n\nLet\u2019s remove the comma that starts each line by searching for \n^,\n (remembering that the carat character indicates the start of a line) and replacing with nothing. \n\n\n\n\n\n\nSave this as \ncleaned-locations.csv\n.\n\n\nCongratulations! You\u2019ve taken quite complicated output and cleaned it so that every place mentioned on a single page in the original publication is now on its own line, which means you can import it into a network visualization package, a spreadsheet, or some other tool!\n\n\n\n\n\n\nNow you can upload this CSV to Gephi.", 
            "title": "Going further with regex"
        }, 
        {
            "location": "/supporting materials/regex-ner/#further-munging-the-output-of-ner", 
            "text": "So, you've learned how to tag a corpus using the Stanford NER (visit  GitHub for that exercise again, as a reminder ). There are several ways we might like to visualize that output.  Unfortunately, depending on what you want to  do  with that data, you are going to have to go back to the data-munging cycle. Let us imagine that we wanted to visualize the locations mentioned in the letters as a kind of network.  We will use regular expressions to further manipulate the data into a useful source -  target list.", 
            "title": "Further Munging the output of NER"
        }, 
        {
            "location": "/supporting materials/regex-ner/#regex-on-mac-to-useful-output-pc-please-skip-to-next-section", 
            "text": "We need to organize those locations so that locations mentioned in a letter are connected to each other. To begin, we trim away any line that does not start with LOCATION.   We do this by using our regular expression skills from module 3, and adding in a few more commands:   FIND:  ^(?!.*LOCATION).*$  and replace with nothing. We had a few new commands in there: the  ?!  tells it to begin looking ahead for the literal phrase LOCATION, and then the dot and dollar sign let us know to end at the end of the line. In any case, this will delete everything that doesn't have the location tag in front.    Now, let us also mark those blank lines we noticed as the start of a new letter.  FIND:  ^\\s*$  where: ^  marks the beginning of a line  $  marks the end of a line  \\s  indicates \u2018whitespace\u2019  \\*  indicates a zero or more repetition of the thing in front of it (in this case, whitespace)  and we replace with the phrase, \u201cblankspace\u201d. Because there might be a few blank lines, you might see \u2018blankspace\u2019 in places. Where you\u2019ve got \u2018blankspaceblankspace\u2019, that\u2019s showing us the spaces between the original documents (whereas one \u2018blankspace\u2019 was where an ORGANIZATION or PERSON tag was removed).    At this point, we might want to remove spaces between place names and use underscores instead, so that when we finally get this into a comma separated format, places like \u2018United States\u2019 will be \u2018United_States\u2019 rather than \u2018united, states\u2019.  Find:   -a single space \nReplace:  \\_    Now, we want to reintroduce the space after \u2018LOCATION:\u2019, so  Find:  :_         -this is a colon, followed by an underscore \nReplace:  :   -this is a colon, followed by a space    Now we want to get all of the locations for a single letter into a single line. So we want to get rid of new lines and LOCATION:  Find:  \\n(LOCATION:) \nReplace:    It is beginning to look like a list. Let\u2019s replace \u2018blankspaceblankspace\u2019 with \u2018new-document\u2019.  Find:  blankspaceblankspace \nReplace: new-document    And now let\u2019s get those single blankspace lines excised:  Find  \\n(blankspace) \nReplace:  Now you have something that looks like this:  p \nnew-document Houston Republic_of_Texas United_States Texas br \nnew-document Texas br \nnew-document United_States Town_of_Columbia br \nnew-document United_States Texas br \nnew-document United_States United_States_of_ Republic_of_Texas br \nnew-document New_Orleans United_States_Govern br \nnew-document United_States Houston United_States Texas united_states br \nnew-document United_States New_Orleans United_States br \nnew-document Houston United_States Texas United_States br \nnew-document New_Orleans br  /p     Why not leave \u2018new-document\u2019 in the file? It\u2019ll make a handy marker. Let\u2019s replace the spaces with commas:  Find:        -a single space \nReplace:  ,    Save your work as a  ****.csv  file. You now have a file that you can load into a variety of other platforms or tools to perform your analysis. Of course, NER is not perfect, especially when dealing with names like \u2018Houston\u2019 that were a personal name long before they were a place name.", 
            "title": "REGEX on Mac to Useful Output (PC, please skip to next section):"
        }, 
        {
            "location": "/supporting materials/regex-ner/#regex-on-pc-to-useful-output", 
            "text": "Open your tagged file in Notepad++. There are a lot of carriage returns, line breaks, and white spaces in this document that will make our regex very complicated and will in fact break it periodically, if we try to work with it as it is. Instead, let us remove all new lines and whitespaces and the outset, and use regex to put structure back.     To begin, we want to get the entire document into a single line:  Find:  \\n \nReplace with nothing.  Notepad++ reports the number of lines in the document at the bottom of the window. It should now report lines: 1.    Let\u2019s introduce line breaks to correspond with the original letters (ie, a line break signals a new letter), since we want to visualize the network of locations mentioned where we join places together on the basis of being mentioned in the same letter. If we examine the original document, one candidate for something to search for, to signal a new letter, could be   PERSON  to  PERSON  but the named entity recognizer sometimes confuses persons for places or organizations; the object character recognition also makes things complicated.  Since the letters are organized chronologically, and we can see \u2018Digitized by Google\u2019 at the bottom of every page (and since no writer in the 1840s is going to use the word digitized) let\u2019s use that as our page break marker. For the most part, one letter corresponds to one page in the book. It\u2019s not ideal, but this is something that the historian will have to discuss in her methods and conclusions. Perhaps, over the seven hundred odd letters in this collection, it doesn\u2019t actually make much difference.  Find:  (digitized) \nReplace: \\n\\1  You now have on the order of 700 odd lines in the document.    Let\u2019s find the locations and put them on individual lines, so that we can strip out everything else.  Find:  (LOCATION)(.\\*?)(LOCATION) \nReplace:  \\n\\2  In the search string above, the  .\\*  would look for everything between the first  location  on the line and the last  location in the line, so we would get a lot of junk. By using  .\\*?  we just get the text between  location and the next  location  tag.    We need to replace the   and   from the location tags now, as those are characters with special meanings in our regular expressions. Turn off the regular expressions in notepad ++ by unticking the \u2018regular expression\u2019 box. Now search for  ,  , and \\ in turn, replacing them with tildes:  LOCATION Texas /LOCATION  becomes  ~Texas~~~    Now we want to delete in each line everything that isn\u2019t a location. Our locations start the line and are trailed by three tildes, so we just need to find those three tildes and everything that comes after, and delete. Turn regular expressions back on in the search window.  Find:  (~~~)(.*) \nReplace with nothing.    Our marker for a new page in this book was the line that began with \u2018Digitized by\u2019. We need to delete that line in its entirety, leaving a blank line.  Find:  (Digitized)(.\\*) \nReplace:  \\n    Now we want to get those locations all in the same line again. We need to find the new line character followed by the tilde, and then delete that new line, replacing it with a comma:  Find:  (\\n)(~) \nReplace:  ,    Now we remove extraneous blank lines.  Find:  \\s*$ \nReplace with nothing.  We\u2019re almost there.     Let\u2019s remove the comma that starts each line by searching for  ^,  (remembering that the carat character indicates the start of a line) and replacing with nothing.     Save this as  cleaned-locations.csv .  Congratulations! You\u2019ve taken quite complicated output and cleaned it so that every place mentioned on a single page in the original publication is now on its own line, which means you can import it into a network visualization package, a spreadsheet, or some other tool!    Now you can upload this CSV to Gephi.", 
            "title": "REGEX on PC to Useful Output:"
        }, 
        {
            "location": "/supporting materials/quick-intro-r/", 
            "text": "A quick introduction to R and RStudio\n\n\nR is a powerful language for statistical exploring, visualizing, and manipulating all kinds of data, including textual. It is however not the most intutive of environments to work in. In which case, \nRStudio\n is what we need. Fortunately, you already have this installed for you in your DH Box; however, if you want to have it on your own computer, go get R \nfrom RStudio\n and then the \nRStudio\n. In DH Box, you sign into RStudio with your DH Box credentials.\n\n\nWhy do we bother with this? Why not just use Excel? One word: reproducibility. In Excel, the sequence of clicking that one does to do anything is next to \nimpossible to effectively communicate\n to someone else. Excel also was built for business applications, and those biases are built into its dna (\nwhich can have some nasty effects\n.)\n\n\nR allows us to do scripted analyses. We write out the sequence of transformations or analyses to be done, making a kind of program that we can then feed our data into. Once we have a workflow that does what we need it to do, it becomes trivial to re-use the code on other datasets. What's more, when we do an analysis, we can publish the scripts and the data. We don't have to taken an author's word for it: we can re-run the analysis ourselves or build upon it.\n\n\nThis course only touches in the lightest manner on the potential for R for doing digital history. Please see Lincoln Mullen's \nComputational Historical Thinking with Applications in R\n for more instruction.\n\n\nFor now, we'll do a quick whistle-stop tour of using RStudio to do some analysis in R.\n\n\n\n\nYou should always keep your research materials (scripts and associated data) organized in separate project folders. RStudio makes this easy for you. \n\n\n\n\n\n\nWhen you open RStudio to start a new project, click where it says 'project (none)' at the right hand side of the interface.\n\n\n\n\n\n\nClick on the down arrow, and select new R project. \n\n\n\n\n\n\nFollow the prompts, and create it in a new directory. R keeps track of what you're up to in a project file, so that you can pick up where you left off. \nYou should also keep your code under version control as well\n so that you can recover from disaster and/or share your code/data with collaborators; click on that link to get version control set up.\n\n\n\n\n\n\nSometimes, it might happen that RStudio crashes (this can be, in DH Box, related to memory issues). If that happens \n if it just seems to 'hang' (and you've waited several minutes), you can refresh your browser, and go back to DH Box. You'll probably have to re-open your project file. \n\n\n\n\n\n\nUse the file panel at bottom right to find your *.rproj file (your R project file) and click on it to re-open. \n\n\n\n\n\n\nSave your work often, from the File \n Save menu.\n\n\nRStudio divides the screen up into four neat panes. \n\n\n\n\nThe top left is for writing your analytical script (or code; I will use the two words interchangeably)\n\n\nThe bottom left is the console where the analysis actually happens\n\n\nThe top right is an environment pane which will show you the variables you've created, your history (commands you've run), and (once it's configured) git; indeed, other tools and plugins will appear as tabs here.\n\n\nThe bottom right gives you a preview of any plots or charts you create; once you create one if you click 'zoom' the chart will open in its own pop up so that you can see it better. The file explorer and the help text also appear under tabs in this box.\n\n\n\n\n\n\n\n\nYou write your script in the script box, and then you can get RStudio to run the code one line at a time by clicking on code \n run line (it runs the line of code where you left the cursor). If you select a number of lines of code and hit run line, all of that code will run. R scripts are text files that use .r as their file extension.\n\n\n\n\n\n\nThe console is where the action happens. Click down in the console. \n\n\n\n\n\n\nType \n3 + 5\n and hit enter. RStudio will run the calculation: \n[1] 8\n! The [1] indicates that this is the first result. \n\n\n\n\n\n\nNow type \na = 3\n and hit enter. Over in the top right, the 'environment' pane updates to tell us that yes, a has a value of 3. \n\n\n\n\n\n\nNow type \nb = 5\n. The environment updates accordingly. \n\n\n\n\n\n\nNow type \na + b\n. Hey, we can do algebra! Anything you can do in excel, we can do in code here in RStudio. \n\n\n\n\n\n\nA very good introduction to how R (the language) works is at \nTry R on codeschool\n, an interactive tutorial in your browser. Go give some of that a shot.", 
            "title": "Quick intro to R"
        }, 
        {
            "location": "/supporting materials/quick-intro-r/#a-quick-introduction-to-r-and-rstudio", 
            "text": "R is a powerful language for statistical exploring, visualizing, and manipulating all kinds of data, including textual. It is however not the most intutive of environments to work in. In which case,  RStudio  is what we need. Fortunately, you already have this installed for you in your DH Box; however, if you want to have it on your own computer, go get R  from RStudio  and then the  RStudio . In DH Box, you sign into RStudio with your DH Box credentials.  Why do we bother with this? Why not just use Excel? One word: reproducibility. In Excel, the sequence of clicking that one does to do anything is next to  impossible to effectively communicate  to someone else. Excel also was built for business applications, and those biases are built into its dna ( which can have some nasty effects .)  R allows us to do scripted analyses. We write out the sequence of transformations or analyses to be done, making a kind of program that we can then feed our data into. Once we have a workflow that does what we need it to do, it becomes trivial to re-use the code on other datasets. What's more, when we do an analysis, we can publish the scripts and the data. We don't have to taken an author's word for it: we can re-run the analysis ourselves or build upon it.  This course only touches in the lightest manner on the potential for R for doing digital history. Please see Lincoln Mullen's  Computational Historical Thinking with Applications in R  for more instruction.  For now, we'll do a quick whistle-stop tour of using RStudio to do some analysis in R.   You should always keep your research materials (scripts and associated data) organized in separate project folders. RStudio makes this easy for you.     When you open RStudio to start a new project, click where it says 'project (none)' at the right hand side of the interface.    Click on the down arrow, and select new R project.     Follow the prompts, and create it in a new directory. R keeps track of what you're up to in a project file, so that you can pick up where you left off.  You should also keep your code under version control as well  so that you can recover from disaster and/or share your code/data with collaborators; click on that link to get version control set up.    Sometimes, it might happen that RStudio crashes (this can be, in DH Box, related to memory issues). If that happens   if it just seems to 'hang' (and you've waited several minutes), you can refresh your browser, and go back to DH Box. You'll probably have to re-open your project file.     Use the file panel at bottom right to find your *.rproj file (your R project file) and click on it to re-open.     Save your work often, from the File   Save menu.  RStudio divides the screen up into four neat panes.    The top left is for writing your analytical script (or code; I will use the two words interchangeably)  The bottom left is the console where the analysis actually happens  The top right is an environment pane which will show you the variables you've created, your history (commands you've run), and (once it's configured) git; indeed, other tools and plugins will appear as tabs here.  The bottom right gives you a preview of any plots or charts you create; once you create one if you click 'zoom' the chart will open in its own pop up so that you can see it better. The file explorer and the help text also appear under tabs in this box.     You write your script in the script box, and then you can get RStudio to run the code one line at a time by clicking on code   run line (it runs the line of code where you left the cursor). If you select a number of lines of code and hit run line, all of that code will run. R scripts are text files that use .r as their file extension.    The console is where the action happens. Click down in the console.     Type  3 + 5  and hit enter. RStudio will run the calculation:  [1] 8 ! The [1] indicates that this is the first result.     Now type  a = 3  and hit enter. Over in the top right, the 'environment' pane updates to tell us that yes, a has a value of 3.     Now type  b = 5 . The environment updates accordingly.     Now type  a + b . Hey, we can do algebra! Anything you can do in excel, we can do in code here in RStudio.     A very good introduction to how R (the language) works is at  Try R on codeschool , an interactive tutorial in your browser. Go give some of that a shot.", 
            "title": "A quick introduction to R and RStudio"
        }, 
        {
            "location": "/supporting materials/cyoa.txt/", 
            "text": "Choose your own adventure!\n\n\nIn this exercise, I want \nyou\n to pick a text or network analysis tool from \nDIRT Directory's list\n or \nthe DH Resources page\n and figure out how to use it. You can use our version of the \nColonial Newspaper Database\n as your source text. Write up your own basic tutorial explaining what the tool does, how you got it to work on your data, and what you think it might be telling us or be useful for. Fork and add the link to your tutorial (which you'll have as a .md file in your repository) to this document.", 
            "title": "Choose Your Own Adventure"
        }, 
        {
            "location": "/supporting materials/cyoa.txt/#choose-your-own-adventure", 
            "text": "In this exercise, I want  you  to pick a text or network analysis tool from  DIRT Directory's list  or  the DH Resources page  and figure out how to use it. You can use our version of the  Colonial Newspaper Database  as your source text. Write up your own basic tutorial explaining what the tool does, how you got it to work on your data, and what you think it might be telling us or be useful for. Fork and add the link to your tutorial (which you'll have as a .md file in your repository) to this document.", 
            "title": "Choose your own adventure!"
        }, 
        {
            "location": "/supporting materials/geoparsing-w-python.txt/", 
            "text": "Geoparsing with Python\n\n\nThis exercise draws from the work of \nFred Gibbs\n\n\n\n\nextract, transform, and save as CSV\n\n\nextract geocoded placenames from a text file\n\n\ncreate a kml file with python\n\n\n\n\nIn this exercise, you will need to have Python installed on your machine. \nYou can download it from Python's website\n. I have version 2.7.9 on this machine, and know that what follows works with that version.\n\n\nYou should also read and understand Fred Gibbs' tutorial on \ninstalling python modules\n because you will need to install some helper modules.\n\n\nIn Module 3, you used the NER to extract place names from a text. After some further munging with regex, you might have ended up with a CSV that looks like \nthis one on GitHub\n.\n\n\n\n\nUse Openrefine to open that CSV file. In the same way you tidied up in the \nOpen Refine tutorial in module 3\n, clean up this CSV so that you merge together place names appropriately (ie. so that '4ustin' gets merged with 'Austin'). Do this for all the columns.\n\n\nExport the table as a new CSV \n call it \ncleaned-places.csv\n.\n\n\nOpen that CSV in your spreadsheet program. Copy and paste all of the columns so that they become a single list. (ie. one column of place names).\n\n\nUsing your spreadsheet's filtering options, see if you can remove any more duplicates. (It might be useful to keep track of how many duplicates you delete, in a new file, eg -\n Texas,200 \n- that kind of information might be handy, as in \nthe mapping texts project (PDF opens in new tab)\n).\n\n\nSave the file you were removing the duplicates from (which has just a single column of unique place names) as 'placelist.txt'\n\n\nNow, at this point, we're going to open up our text editor and create a new python program, following \nGibbs' tutorial\n. His complete script is at the bottom of his post, but make sure you understand everything that is going on. Do you see the places where he has to import new python modules to make his script work? Make sure you've installed those modules. Let's call your completed script 'geoparse.py'. Done that? Good. \n\n\nOpen your terminal, navigate to the folder you're working in, and run your script: \npython geoparse.py\n\n\n\n\nDid it work? Did you get an error message? It's entirely possible that you got this message:\n\n\nTraceback (most recent call last):\n  File \ngeolocate.py\n, line 14, in \nmodule\n\n    lat = json['results'][0]['geometry']['location']['lat']\nIndexError: list index out of range\n\n\n\n\n...but check your folder. Do you have a \ngeocoded-places.txt\n file? If so, it worked! Or at least, it got most of your places from the Google maps API. (For the rest, you can try uploading your places list to \nScargill's geoparser\n and then copying and pasting the output to an excel file. This parser will give you several columns of results, where the first column represents its best guess and the other columns other possibilities).\n\n\nYou can now import your geocoded places into many other software packages. Gibbs also shows us how to convert our list into KML, the format that Google Earth and Google Maps can read. \nTry out Gibbs' tutorial\n. You can double-click on the resulting KML file, and if you have Google Earth installed, it will open up there. Within google earth, you can start adding more information, other annotations... pretty soon, you'll have a complete map!\n\n\nRemember to upload your scripts \n data \n obersvations to your open notebook.\n\n\n(Incidentally, if you wanted to load this material into \nPalladio\n you'd need a file that looked like this:\n\n\nPlace   Coordinates\n\nMEXICO  23.634501,-102.552784\n\nCalifornia  36.778261,-119.4179324\n\nBrazos  32.661389,-98.121667\n\n\netc: that is, a tab between 'place' and 'coordinates' in the first line, a tab between 'mexico' and the latitude, and a comma between latitude and logitude. Best way to effect this transformation? Probably using Regex. It's unfortunate that Palladio doesn't accept straightforward place,latitude,longitude comma separated data).", 
            "title": "Geoparsing with Python"
        }, 
        {
            "location": "/supporting materials/geoparsing-w-python.txt/#geoparsing-with-python", 
            "text": "This exercise draws from the work of  Fred Gibbs   extract, transform, and save as CSV  extract geocoded placenames from a text file  create a kml file with python   In this exercise, you will need to have Python installed on your machine.  You can download it from Python's website . I have version 2.7.9 on this machine, and know that what follows works with that version.  You should also read and understand Fred Gibbs' tutorial on  installing python modules  because you will need to install some helper modules.  In Module 3, you used the NER to extract place names from a text. After some further munging with regex, you might have ended up with a CSV that looks like  this one on GitHub .   Use Openrefine to open that CSV file. In the same way you tidied up in the  Open Refine tutorial in module 3 , clean up this CSV so that you merge together place names appropriately (ie. so that '4ustin' gets merged with 'Austin'). Do this for all the columns.  Export the table as a new CSV   call it  cleaned-places.csv .  Open that CSV in your spreadsheet program. Copy and paste all of the columns so that they become a single list. (ie. one column of place names).  Using your spreadsheet's filtering options, see if you can remove any more duplicates. (It might be useful to keep track of how many duplicates you delete, in a new file, eg -  Texas,200  - that kind of information might be handy, as in  the mapping texts project (PDF opens in new tab) ).  Save the file you were removing the duplicates from (which has just a single column of unique place names) as 'placelist.txt'  Now, at this point, we're going to open up our text editor and create a new python program, following  Gibbs' tutorial . His complete script is at the bottom of his post, but make sure you understand everything that is going on. Do you see the places where he has to import new python modules to make his script work? Make sure you've installed those modules. Let's call your completed script 'geoparse.py'. Done that? Good.   Open your terminal, navigate to the folder you're working in, and run your script:  python geoparse.py   Did it work? Did you get an error message? It's entirely possible that you got this message:  Traceback (most recent call last):\n  File  geolocate.py , line 14, in  module \n    lat = json['results'][0]['geometry']['location']['lat']\nIndexError: list index out of range  ...but check your folder. Do you have a  geocoded-places.txt  file? If so, it worked! Or at least, it got most of your places from the Google maps API. (For the rest, you can try uploading your places list to  Scargill's geoparser  and then copying and pasting the output to an excel file. This parser will give you several columns of results, where the first column represents its best guess and the other columns other possibilities).  You can now import your geocoded places into many other software packages. Gibbs also shows us how to convert our list into KML, the format that Google Earth and Google Maps can read.  Try out Gibbs' tutorial . You can double-click on the resulting KML file, and if you have Google Earth installed, it will open up there. Within google earth, you can start adding more information, other annotations... pretty soon, you'll have a complete map!  Remember to upload your scripts   data   obersvations to your open notebook.  (Incidentally, if you wanted to load this material into  Palladio  you'd need a file that looked like this:  Place   Coordinates \nMEXICO  23.634501,-102.552784 \nCalifornia  36.778261,-119.4179324 \nBrazos  32.661389,-98.121667  etc: that is, a tab between 'place' and 'coordinates' in the first line, a tab between 'mexico' and the latitude, and a comma between latitude and logitude. Best way to effect this transformation? Probably using Regex. It's unfortunate that Palladio doesn't accept straightforward place,latitude,longitude comma separated data).", 
            "title": "Geoparsing with Python"
        }, 
        {
            "location": "/supporting materials/glitch/", 
            "text": "Glitching Files\n\n\nGlitching files messses with our expectations of what digital data should be. There is actually quite a large body of literature on the why and how of glitching (\ntry Glitch's GitHub for a place to start\n). For us as digital historians, glitching digital images or other documents reminds of us the ephemerality of digital data. It might also raise other questions about the composition of historical photography, and the ways that no image is an objective record of the past. In the exercise below, you build on what you learned about APIs and Regex to download images from the Open Context repository of archaeological data. Then, you use a script that will perform the same manipulations on every image in your folder. Finally, you will use another script to create a static website with all of your images, a gallery of glitch.\n\n\nSpend some time on the articles and resources curated on \nGlitch's GitHub\n to begin to explore the philosophy and aesthetic of Glitch.\n\n\nWorkflow we're going for:\n\n\n\n\nGet images from Open Context (\nocget.sh\n)\n\n\nGlitch them (\ndo-glitch.sh\n)\n\n\nMake a website out of them (\nexpose\n)\n\n\nPush them to GitHub Pages (\ngit\n)\n\n\n\n\nOpen Context\n\n\nOpen Context publishes archaeological data on the web. Archaeology generates vast amounts of information, both through excavation and through analysis. Open Context exists to publish curated versions of this data with unique digital object identifiers so that the source data of archaeology can be re-examined and re-studied in the future. Archaeology, uniquely amongst the historical disciplines, tends to destroy its subject matter, so reproducibility and open access data are extremely important issues. At \nOpen Context's API page\n, they explain how to programmatically obtain information from their site. \n\n\n\n\n\n\nCheck out \nthis example search on Open Context\n. Open that up, give it a play right now, and see what kinds of information exist. Try to craft a search that retrieves some interesting information.\n\n\nOnce you've crafted a search that retrieves something of interest, it's time to build a script that will retrieve that information for you. Remember the exercise that queried the Canadiana api? Below I have modified that script to grab 10 records from the 'animal bone' category on Open Context. You could use that as a basis for your own search.\n\n\n\n\n\n\nIn DH Box, create a new folder with \nmkdir\n for this exercise, then \ncd\n into it. \n\n\n\n\n\n\nOpen the nano editor and paste the script below in a new file. Make sure to read the comments, because there are a few lines you have to customize. (You'll get file not found errors if you don't pay attention!) \n\n\nThe line beginning with 'sed' searches for the \nthumbnail\n key, and marks it off with a tilde. The \n|\n character is called a 'pipe' and it pipes the output of the command before it into the input of the command after it. The first pipe passes the output of that first sed command to grep, which finds the lines marked with the tilde to the tr (trim) command, which then deletes all of the other information; the next pipe passes to a series of sed commands which then get rid of whitespace and the word thumbnail, the tilde, and any commas, leaving us with a list of direct URLs to the thumbnails.\n\n\nThe wget command grabs each thumbnail in the list in turn, waiting 2 seconds between requests, and using a limited amount of bandwidth (we're good digital citizens, remember).\n\n\n#! /bin/bash\n# we already know how to ask the api for the infromation we're interested in, from studying opencontex.org' api documentation. Below we ask for a 10 records related to animal bones\ncurl 'https://opencontext.org/media-search/.json?prop=rel--oc-gen-cat-object||rel--oc-gen-cat-animal-bone\nresponse=uri-meta\nrows=10' \n results.txt\n# the next two lines take the results.txt file that is quite messy and clean it up. I'll try to explain what they all mean. Basically, tr deletes a given character - so we delete quotation marks \"\\\"\" (the slash tells the computer not to treat the quotation mark as a computer code, but as a quotation mark itself), to erase spaces, and to find the phrase \"key:\" and delete it too.\nsed -r 's/(.+\\bthumbnail\\b.+)/~\\1/g' results.txt | grep '~'| tr -d \"\\\"\" | tr -d \"{\" | tr -d \"}\" | tr -s \" \" | sed '/^\\s*$/d' | tr -d ' ' | sed 's/\\bthumbnail:\\b//g' | sed 's/,//g' | sed 's/~//g'\n thumbnailstograb.txt\n# gotta grab 'em all.\n# in the line below, put the full path to your images file, eg mine is home/shawngraham/oc/images. Yours might be home/yourname/oc/images\nwget -i thumbnailstograb.txt -P path/to/images --limit-rate=20k -w 2\n\n\n\n\n\n\n\nSave it as \nocget.sh\n, then use \n$chmod 755 ocget.sh\n to make it executable. \n\n\n\n\n\n\nThen \n$ mkdir images\n so you have a place to put the images into.\n\n\n\n\n\n\nHey, images keep appearing even though I ctrl+c to stop the download\n I hear some of you say. Well, that doesn't necessarily stop anything; when you ran the command, you started a \nprocess\n and it might still be ticking away in the background. If this seems to be happening to you, type \nps ux\n to get a list of running processes. If wget is still running in the backround, you'll see it listed in the right-most column. The first number in that row is its PIDnumber; to kill it, type \nkill -9 PIDnumber\n where you swap in the actual PIDnumber.\n\n\nAlso\n, before you re-run this script, you need to delete the 'results.txt' file and the 'thumbnailstograb.txt' file, like so: \nrm results.txt\n etc. Otherwise, it'll just add your results to the end of the previous results, making your list longer and longer...\n\n\nAn image glitch script in Python\n\n\nThe next step is to glitch the image. \n\n\n\n\n\n\nGo to \nthe bndr GitHub repository\n. This is a python package for mucking about the bits inside a jpg. \n\n\n\n\n\n\nInstall it at the command line with \n$ pip install bndr\n.\n\n\n\n\n\n\nMake a new folder for the output of this process: \nmkdir out\n.\n\n\nNow, this code is meant to be run on one file at a time, like this: \n$ bndrimg photo-name.jpg\n. Typing in each file name by hand would take a very long time to glitch everything. So instead, we write a little script that we'll call \ndo-glitch.sh\n. \n\n\n\n\n\n\nIn Nano, paste the following:\n\n\n#! /bin/bash\n# do-glitch.sh\n# run the glitch script on each image in the images folder\nfor file in images/*.jpg; do bndrimg \"$file\"; done\n# move the glitched files to the output folder\nfind ./ -name '*out.png' -exec cp -prv '{}' 'out/' ';'\n\n\n\nThis isn't the most elegant script, but it works, and frankly, that's all that matters sometimes.\n\n\n\n\n\n\nChange the permissions so we can run it by typing \n$ chmod 755 do-glitch.sh\n. This script reads each file name in the images folder, and passes them one at a time to the bndr command (you can't run it when you're \ninside\n the images folder, remember). Then, we move just the glitched images to the 'out' folder. \n\n\n\n\n\n\nDownload some of the pics to your own machine using the filemanager to see your glitched art!\n\n\n\n\n\n\nA static website generator for photography\n\n\nFinally, let's turn that folder of pictures into a website. We're going to use the Expos\u00e9 site generator, which you can get from the \nExpos\u00e9 GitHub repository\n. Take a moment to read through the details of that package.\n\n\n\n\n\n\nUse the \ncd\n command to get back up to your main directory (ie. don't do this when you're in your images folder). \n\n\n\n\n\n\nGrab the code:\n    \n$ git clone https://github.com/Jack000/Expose.git\n\n\nThe Expos\u00e9 script relies on some helpers and so on in its folder. But we would like to be able to run that command no matter which folder we're in. We tell the computer that when we type \nexpose\n we \nactually\n mean, the version that lives in that location where we just downloaded it. That is to say, we use the \nalias\n command. On my machine, it looks like this: \nalias expose=/home/shawngraham/oc/Expose/expose.sh\n \n\n\nOn your machine, it might be in a different location. Note also that the Expose repo is uppercase E, while the command is lowercase e. \nalso\n every time you start a session, you'll need to do this alias command or else the computer won't know what you're talking about.\n\n\n\n\n\n\nTo generate the static site, cd into your \nout\n folder that holds your glitched images. \n\n\n\n\n\n\nAt the prompt, type \n$ expose\n. \n\n\nTa da! You now have a fully functional website showing off your glitch art.\n\n\nYou can customize it to use an alternative theme, a few bells and whistles; see the Expose documentation. One thing that you should do is to add captions for your images. You write these as either .txt or .md files, but make sure the file name for the caption is the same as for the image \nand\n put it in the same folder as the source images! You do this \nbefore\n you run the expose script. So \nbeaker-pic.png\n would have as a caption file \nbeaker-pic.txt\n or \nbeaker-pic.md\n. You specify where the caption goes on the image in the YAML; that is, the metadata for your caption. So, if we opened \nbeaker-pic.md\n we might see,\n\n\n---\ntop: 30\nleft: 5\nwidth: 30\nheight: 20\ntextcolor: #ffffff\n---\n# A Picture of a Beaker\nBeakers were used to carry rare aromatic herbs and spices...\n\n\n\n\n\n\n\nMore possibilities for sorting out the position of the text are discussed in the Expose documentation.\n\n\nHost the site on GitHub\n\n\nYou can push the code for your site to GitHub, and then use GitHub's gh-pages feature to serve it up as a live website! Expose creates all of the code for the website inside a new folder called \n_site\n.\n\n\n\n\nType \n$ cd _site\n\n\nTurn this folder into a git repository: \ngit init\n.\n\n\nGo to your account on GitHub.com (in another browser window). Click on the \n+\n at the top-right to make a new repository. Call it whatever you want, but \ndo not\n initialize it with a readme.\n\n\nBack at the command line in your \n_site\n folder, type \n$ git add .\n This stages all of the files and subfolders for a new commit.\n\n\nType \n$ git commit -m \"first commit\"\n to make a commit message\n\n\nTell git where your remote repository is: \n$ git remote add origin https://github.com/YOUR-ACCOUNT/YOUR-NEW-REPO-YOU-JUST-MADE.git\n\n\nPush your materials to GitHub: \n$ git push -u origin master\n (Git might ask for your account username and password)\n\n\nOnce that finishes, go back to GitHub and reload the page at https://github.com/YOUR-ACCOUNT/YOUR-NEW-REPO-YOU-JUST-MADE. You should see your changes.\n\n\nOn the button where it says 'Branch: Master', click on the down arrow. In the box where it says faintly 'Find or create a branch', type \ngh-pages\n. This is a special branch which tells GitHub, 'serve this up as actual code when the user goes to the special github.io version of GitHub'.\n\n\nClick on the gearwheel icon, to go to the settings page (You can also find it at https://github.com/YOUR-ACCOUNT/YOUR-NEW-REPO-YOU-JUST-MADE/settings). \n\n\nScroll down to the box that says 'GitHub Pages'. \n\n\nIn the green box, it says 'you're site is published at https://YOUR-ACCOUNT.github.io/YOUR-NEW-REPO-YOU-JUST-MADE'. \n\n\nClick on that link and you'll see your site! (Visit \nmy site on gh pages\n.)\n\n\n\n\nConclusion\n\n\nI see no reason why digital history cannot bleed into art. Art is meant to provoke, to prompt reflection, discussion, controversy. Glitching images confounds our expectation of what digital data are supposed to be, supposed to do. If you choose your pictures carefully, glitching can tell a story as profound as any essay.", 
            "title": "Glitch"
        }, 
        {
            "location": "/supporting materials/glitch/#glitching-files", 
            "text": "Glitching files messses with our expectations of what digital data should be. There is actually quite a large body of literature on the why and how of glitching ( try Glitch's GitHub for a place to start ). For us as digital historians, glitching digital images or other documents reminds of us the ephemerality of digital data. It might also raise other questions about the composition of historical photography, and the ways that no image is an objective record of the past. In the exercise below, you build on what you learned about APIs and Regex to download images from the Open Context repository of archaeological data. Then, you use a script that will perform the same manipulations on every image in your folder. Finally, you will use another script to create a static website with all of your images, a gallery of glitch.  Spend some time on the articles and resources curated on  Glitch's GitHub  to begin to explore the philosophy and aesthetic of Glitch.", 
            "title": "Glitching Files"
        }, 
        {
            "location": "/supporting materials/glitch/#workflow-were-going-for", 
            "text": "Get images from Open Context ( ocget.sh )  Glitch them ( do-glitch.sh )  Make a website out of them ( expose )  Push them to GitHub Pages ( git )", 
            "title": "Workflow we're going for:"
        }, 
        {
            "location": "/supporting materials/glitch/#open-context", 
            "text": "Open Context publishes archaeological data on the web. Archaeology generates vast amounts of information, both through excavation and through analysis. Open Context exists to publish curated versions of this data with unique digital object identifiers so that the source data of archaeology can be re-examined and re-studied in the future. Archaeology, uniquely amongst the historical disciplines, tends to destroy its subject matter, so reproducibility and open access data are extremely important issues. At  Open Context's API page , they explain how to programmatically obtain information from their site.     Check out  this example search on Open Context . Open that up, give it a play right now, and see what kinds of information exist. Try to craft a search that retrieves some interesting information.  Once you've crafted a search that retrieves something of interest, it's time to build a script that will retrieve that information for you. Remember the exercise that queried the Canadiana api? Below I have modified that script to grab 10 records from the 'animal bone' category on Open Context. You could use that as a basis for your own search.    In DH Box, create a new folder with  mkdir  for this exercise, then  cd  into it.     Open the nano editor and paste the script below in a new file. Make sure to read the comments, because there are a few lines you have to customize. (You'll get file not found errors if you don't pay attention!)   The line beginning with 'sed' searches for the  thumbnail  key, and marks it off with a tilde. The  |  character is called a 'pipe' and it pipes the output of the command before it into the input of the command after it. The first pipe passes the output of that first sed command to grep, which finds the lines marked with the tilde to the tr (trim) command, which then deletes all of the other information; the next pipe passes to a series of sed commands which then get rid of whitespace and the word thumbnail, the tilde, and any commas, leaving us with a list of direct URLs to the thumbnails.  The wget command grabs each thumbnail in the list in turn, waiting 2 seconds between requests, and using a limited amount of bandwidth (we're good digital citizens, remember).  #! /bin/bash\n# we already know how to ask the api for the infromation we're interested in, from studying opencontex.org' api documentation. Below we ask for a 10 records related to animal bones\ncurl 'https://opencontext.org/media-search/.json?prop=rel--oc-gen-cat-object||rel--oc-gen-cat-animal-bone response=uri-meta rows=10'   results.txt\n# the next two lines take the results.txt file that is quite messy and clean it up. I'll try to explain what they all mean. Basically, tr deletes a given character - so we delete quotation marks \"\\\"\" (the slash tells the computer not to treat the quotation mark as a computer code, but as a quotation mark itself), to erase spaces, and to find the phrase \"key:\" and delete it too.\nsed -r 's/(.+\\bthumbnail\\b.+)/~\\1/g' results.txt | grep '~'| tr -d \"\\\"\" | tr -d \"{\" | tr -d \"}\" | tr -s \" \" | sed '/^\\s*$/d' | tr -d ' ' | sed 's/\\bthumbnail:\\b//g' | sed 's/,//g' | sed 's/~//g'  thumbnailstograb.txt\n# gotta grab 'em all.\n# in the line below, put the full path to your images file, eg mine is home/shawngraham/oc/images. Yours might be home/yourname/oc/images\nwget -i thumbnailstograb.txt -P path/to/images --limit-rate=20k -w 2    Save it as  ocget.sh , then use  $chmod 755 ocget.sh  to make it executable.     Then  $ mkdir images  so you have a place to put the images into.    Hey, images keep appearing even though I ctrl+c to stop the download  I hear some of you say. Well, that doesn't necessarily stop anything; when you ran the command, you started a  process  and it might still be ticking away in the background. If this seems to be happening to you, type  ps ux  to get a list of running processes. If wget is still running in the backround, you'll see it listed in the right-most column. The first number in that row is its PIDnumber; to kill it, type  kill -9 PIDnumber  where you swap in the actual PIDnumber.  Also , before you re-run this script, you need to delete the 'results.txt' file and the 'thumbnailstograb.txt' file, like so:  rm results.txt  etc. Otherwise, it'll just add your results to the end of the previous results, making your list longer and longer...", 
            "title": "Open Context"
        }, 
        {
            "location": "/supporting materials/glitch/#an-image-glitch-script-in-python", 
            "text": "The next step is to glitch the image.     Go to  the bndr GitHub repository . This is a python package for mucking about the bits inside a jpg.     Install it at the command line with  $ pip install bndr .    Make a new folder for the output of this process:  mkdir out .  Now, this code is meant to be run on one file at a time, like this:  $ bndrimg photo-name.jpg . Typing in each file name by hand would take a very long time to glitch everything. So instead, we write a little script that we'll call  do-glitch.sh .     In Nano, paste the following:  #! /bin/bash\n# do-glitch.sh\n# run the glitch script on each image in the images folder\nfor file in images/*.jpg; do bndrimg \"$file\"; done\n# move the glitched files to the output folder\nfind ./ -name '*out.png' -exec cp -prv '{}' 'out/' ';'  This isn't the most elegant script, but it works, and frankly, that's all that matters sometimes.    Change the permissions so we can run it by typing  $ chmod 755 do-glitch.sh . This script reads each file name in the images folder, and passes them one at a time to the bndr command (you can't run it when you're  inside  the images folder, remember). Then, we move just the glitched images to the 'out' folder.     Download some of the pics to your own machine using the filemanager to see your glitched art!", 
            "title": "An image glitch script in Python"
        }, 
        {
            "location": "/supporting materials/glitch/#a-static-website-generator-for-photography", 
            "text": "Finally, let's turn that folder of pictures into a website. We're going to use the Expos\u00e9 site generator, which you can get from the  Expos\u00e9 GitHub repository . Take a moment to read through the details of that package.    Use the  cd  command to get back up to your main directory (ie. don't do this when you're in your images folder).     Grab the code:\n     $ git clone https://github.com/Jack000/Expose.git  The Expos\u00e9 script relies on some helpers and so on in its folder. But we would like to be able to run that command no matter which folder we're in. We tell the computer that when we type  expose  we  actually  mean, the version that lives in that location where we just downloaded it. That is to say, we use the  alias  command. On my machine, it looks like this:  alias expose=/home/shawngraham/oc/Expose/expose.sh    On your machine, it might be in a different location. Note also that the Expose repo is uppercase E, while the command is lowercase e.  also  every time you start a session, you'll need to do this alias command or else the computer won't know what you're talking about.    To generate the static site, cd into your  out  folder that holds your glitched images.     At the prompt, type  $ expose .   Ta da! You now have a fully functional website showing off your glitch art.  You can customize it to use an alternative theme, a few bells and whistles; see the Expose documentation. One thing that you should do is to add captions for your images. You write these as either .txt or .md files, but make sure the file name for the caption is the same as for the image  and  put it in the same folder as the source images! You do this  before  you run the expose script. So  beaker-pic.png  would have as a caption file  beaker-pic.txt  or  beaker-pic.md . You specify where the caption goes on the image in the YAML; that is, the metadata for your caption. So, if we opened  beaker-pic.md  we might see,  ---\ntop: 30\nleft: 5\nwidth: 30\nheight: 20\ntextcolor: #ffffff\n---\n# A Picture of a Beaker\nBeakers were used to carry rare aromatic herbs and spices...    More possibilities for sorting out the position of the text are discussed in the Expose documentation.", 
            "title": "A static website generator for photography"
        }, 
        {
            "location": "/supporting materials/glitch/#host-the-site-on-github", 
            "text": "You can push the code for your site to GitHub, and then use GitHub's gh-pages feature to serve it up as a live website! Expose creates all of the code for the website inside a new folder called  _site .   Type  $ cd _site  Turn this folder into a git repository:  git init .  Go to your account on GitHub.com (in another browser window). Click on the  +  at the top-right to make a new repository. Call it whatever you want, but  do not  initialize it with a readme.  Back at the command line in your  _site  folder, type  $ git add .  This stages all of the files and subfolders for a new commit.  Type  $ git commit -m \"first commit\"  to make a commit message  Tell git where your remote repository is:  $ git remote add origin https://github.com/YOUR-ACCOUNT/YOUR-NEW-REPO-YOU-JUST-MADE.git  Push your materials to GitHub:  $ git push -u origin master  (Git might ask for your account username and password)  Once that finishes, go back to GitHub and reload the page at https://github.com/YOUR-ACCOUNT/YOUR-NEW-REPO-YOU-JUST-MADE. You should see your changes.  On the button where it says 'Branch: Master', click on the down arrow. In the box where it says faintly 'Find or create a branch', type  gh-pages . This is a special branch which tells GitHub, 'serve this up as actual code when the user goes to the special github.io version of GitHub'.  Click on the gearwheel icon, to go to the settings page (You can also find it at https://github.com/YOUR-ACCOUNT/YOUR-NEW-REPO-YOU-JUST-MADE/settings).   Scroll down to the box that says 'GitHub Pages'.   In the green box, it says 'you're site is published at https://YOUR-ACCOUNT.github.io/YOUR-NEW-REPO-YOU-JUST-MADE'.   Click on that link and you'll see your site! (Visit  my site on gh pages .)", 
            "title": "Host the site on GitHub"
        }, 
        {
            "location": "/supporting materials/glitch/#conclusion", 
            "text": "I see no reason why digital history cannot bleed into art. Art is meant to provoke, to prompt reflection, discussion, controversy. Glitching images confounds our expectation of what digital data are supposed to be, supposed to do. If you choose your pictures carefully, glitching can tell a story as profound as any essay.", 
            "title": "Conclusion"
        }, 
        {
            "location": "/supporting materials/gephi.txt/", 
            "text": "Working with Gephi\n\n\nBefore we go much further, I would recommend that you look at the work of Clement Levallois, who has a \nsuite of excellent tutorials on working with Gephi\n. The tutorial below is adapted from our open draft of \nThe Macroscope\n.\n\n\nIntroduction\n\n\nGephi is quickly becoming the tool of choice for network analysts who do not need the full suite of algorithms offered by \nPajek\n or \nUCINET\n. It is relatively easy to use (eclipsed in this only by \nNodeXL\n), it is usable on all platforms, it can analyze fairly large networks, and it creates beautiful visualizations. The development community is also extremely active, with improvements being added constantly. We recommend Gephi for the majority of historians undertaking serious network analysis research. \n\n\nDownload and install \nGephi\n onto your machine. \n\n\nInstalling Gephi on OS 10 Mavericks\n\n\nNB Since writing this, Gephi announced the \nrelease of Gephi 0.9\n. This newer Gephi should solve these problems.\n\n\nMac users might have some trouble installing Gephi 0.8. We have found that, on Mac OS X Mavericks, Gephi does not load properly after installation. This is a Java-related issue, so you\u2019ll need to install an earlier version of Java than the one provided. \n\n\nTo fix this:\n\n\n\n\n\n\nControl click (or right-click) on the Gephi package\n\n\n\n\n\n\nSelect \u201cshow package contents.\u201d \n\n\n\n\n\n\nClick on \u201ccontents \n resources \n gephi \n etc.\u201d \n\n\n\n\n\n\nControl-click (or right-click) on \u201cgephi.conf\u201d and open with your text editor. \n\n\n\n\n\n\nFind the line reading:\n\n\n#jdkhome=\"/path/to/jdk\"\n\n\n\nand paste the following code:\n\n\njdkhome=\"/System/Library/Java/JavaVirtualMachines/1.6.0.jdk/Contents/Home\n\n\n\n\n\n\n\nSave that file. Then, go to \nApple support\n and install the older version of Java (Java 6). Once that is installed, Gephi should run normally.\n\n\n\n\n\n\nRun Gephi once it is installed. You will be presented with a welcome prompting you to open a recent file, create a new project, or load a sample file. \n\n\n\n\n\n\nClick \u201cNew Project\u201d and then click the \u201cData Laboratory\u201d tab on the horizontal bar at the top of the Gephi window (Fig. 7.3).\n\n\n\n\n\n\nWhen Not To Use Networks\n\n\nNetworks analysis can be a powerful method for engaging with a historical dataset but it is often not the most appropriate. Any historical database may be represented as a network with enough contriving but few should be. One could take the Atlantic trade network, including cities, ships, relevant governments and corporations, and connect them all together in a giant multipartite network. It would be extremely difficult to derive any meaning from this network, especially using traditional network analysis methodologies. Clustering coefficients (a useful network metric), for example, would be meaningless in this situation, as it would be impossible for the neighbors of any one node to be neighbors with one another. Network scientists have developed algorithms for \nmultimodal networks\n, but they are often created for fairly specific purposes, and one should be very careful before applying them to a different dataset and using that analysis to explore a historiographical question.\n\n\nNetworks, as they are commonly encoded, also suffer from a profound lack of nuance. It is all well to say that, because Henry Oldenburg corresponded with both Gottfried Leibniz and John Milton, he was the short connection between the two men. However, the encoded network holds no information of whether Oldenburg actually transferred information between the two or whether that connection was at all meaningful. In a flight network, Las Vegas and Atlanta might both have very high betweenness centrality because people from many other cities fly to or from those airports, and yet one is much more of a hub than the other. This is because, largely, people tend to fly directly back and forth from Las Vegas, rather than through it, whereas people tend to fly through Atlanta from one city to another. This is the type of information that network analysis, as it is currently used, is not well equipped to handle. Whether this shortcoming affects a historical inquiry depends primarily on the inquiry itself.\n\n\nWhen deciding whether to use networks to explore a historiographical question, the first question should be: to what extent is connectivity specifically important to this research project? The next should be: can any of the network analyses my collaborators or I know how to employ be specifically useful in the research project? If either answer is negative, another approach is probably warranted.\n\n\nTexan Correspondence\n\n\nYou will need the information you \ncreated in Module 3, after cleaning the correspondence data using Open Refine\n.\n\n\nUmm, I never did manage that Open Refine stuff...\n\n\nIn module 3, you used Notepad++ or Textwrangler, regular expressions, and OpenRefine to create a comma-separated value file like \n \n****.csv\n \n of the diplomatic correspondence of the Republic of Texas. The final version of the file you created has a row for each letter listed in the volume, and in each row the name of the sender and the recipient. The file should look like this:\n\n\nsource,target\nSam Houston,J. Pinckney Henderson\nJames Webb,Alc6e La Branche\nDavid G. Burnet,Richard G. Dunlap\n...\n\n\n\n\nThis file is called 'an edge list' \n it's a list of connections, or edges, between the individuals. If you no longer have the file, you can find it on \nThe Macroscope website\n.  \n\n\nQuick instructions for getting the data into Gephi:\n\n\n\n\nOpen Gephi by double-clicking its icon. \n\n\nClick \u201cnew project.\u201d The middle pane of the interface window is the \u201cData Laboratory,\u201d where you can interact with your network data in spreadsheet format. This is where we can import the data cleaned up in OpenRefine. \n\n\nIn the Data Laboratory, select \u201cImport Spreadsheet.\u201d \n\n\nPress the ellipsis \u201c...\u201d and locate the CSV you created. Make sure that the Separator is listed as \u201cComma\u201d and the \u201cAs table\u201d is listed as \u201cEdges table.\u201d \n\n\nPress \u201cNext,\u201d then \u201cFinish.\u201d Your data should load up. \n\n\nClick on the \u201coverview\u201d tab and you will be presented with a tangled network graph.\n\n\n\n\nNavigating Gephi\n\n\nGephi is broken up into three panes: \nOverview\n, \nData Laboratory\n, and \nPreview\n. \n\n\n\n\nThe \nOverview pane\n is used to manipulate the visual properties of the network: change colors of nodes or edges, lay them out in different ways, and so forth. The Overview pane is also where you can apply algorithms to the network, like those you learned about in the previous chapter. \n\n\nThe \nData Laboratory\n is for adding, manipulating, and removing data. \n\n\nUse the \nPreview\n pane to do some final tweaks on the look and feel of the network and to export an image for publication.\n\n\n\n\nThere is one tweak that needs to done in the Data Table before the dataset is fully ready to be explored in Gephi. \n\n\n\n\n\n\nClick on the Data Laboratory tab.\n\n\n\n\n\n\nClick on the \u201cNodes\u201d tab in the Data Table (this should be open already) and notice that, of the three columns, \u201cLabel\u201d (the furthermost field on the right) is blank in every row. This will be a problem when viewing the network visualization, as those labels are essential for the network to be meaningful. \n\n\n\n\n\n\nIn the \u201cNodes\u201d tab, click \u201cCopy data to other column\u201d at the bottom, select \u201cID\u201d, and press \u201cOk\u201d (Fig. 7.5). Upon doing so, the \u201cLabel\u201d column will be filled with the appropriate labels for each correspondent. \n\n\n\n\n\n\nWhile you\u2019re still in the Data Laboratory, look in the \u201cEdges\u201d tab and notice there is a \u201cWeight\u201d column. Gephi automatically counted every time a letter was sent from correspondent A to correspondent B and summed up all the occurrences, resulting in the \u201cWeight.\u201d This means that J. Pinckney Henderson sent three letters to James Webb, because Henderson is in the \u201cSource\u201d column, Webb in the \u201cTarget,\u201d, and the \u201cWeight\u201d is three.\n\n\n\n\n\n\nClicking on the Overview pane will take you to a visual representation of the network you just imported. In the middle of the screen, you will see your network in the \u201cGraph\u201d tab. The \u201cContext\u201d tab, at the top right, will show that you imported 234 nodes and 394 edges. At first, all the nodes will be randomly strewn across the screen and make little visual sense. \n\n\n\n\n\n\n \n\n\n\n\n\n\nFix the nodes by selecting a layout in the \u201cLayout\u201d tab \u2013 the best one for beginners is \u201cForce Atlas 2.\u201d \n\n\n\n\n\n\nPress the \u201cRun\u201d button and watch the nodes and edges reorganize on the screen into something slightly more manageable. After the layout runs for a few minutes, re-press the button (now labeled \u201cStop\u201d) to settle the nodes in their place. \n\n\nYou just ran a force-directed layout. Each dot is a correspondent in the network, and lines between dots represent letters sent between individuals. Thicker lines represent more letters, and arrows represent the direction the letters were sent, such that there may be up to two lines connecting any two correspondents (one for each direction).\n\n\nAbout two-dozen smaller components of the network will appear to shoot off into the distance, unconnected from the large, connected component in the middle. For the purpose of this exercise, we are not interested in those disconnected components, so the next step will be to filter them out of the network. \n\n\n\n\n\n\nThe first step is to calculate which components of the network are connected to which others; do this by clicking \u201cRun\u201d next to the text that says \u201cConnected Components\u201d in the \u201cStatistics\u201d tab on the right-hand side.  \n\n\n\n\n\n\nOnce there, select \u201cUnDirected\u201d and press \u201cOK.\u201d \n\n\n\n\n\n\nPress \u201cClose\u201d when the report pops up indicating that the algorithm has finished running. Now that this is done, Gephi knows which is the giant connected component and has labeled that component \u201c0\u201d. \n\n\n\n\n\n\nTo filter out everything but the giant component, click on the \u201cFilters\u201d tab on the right-hand side and browse to \"Component ID Integer (Node)\" in the folder directory (you\u2019ll find it under \"Attributes,\" then \"Equal\"). \n\n\n\n\n\n\nDouble-click \"Component ID Integer (Node)\" and click the \"Filter\" button at the bottom. Doing this removes the disconnected bundles of nodes.\n\n\nThere are many possible algorithms you could use for the analysis step, but in this case you will use the PageRank of each node in the network. This measurement calculates the prestige of a correspondent according to how often others write to him or her. The process is circular, such that correspondents with high prestige will confer their prestige on those they write to, who in turn pass their prestige along to their own correspondents. For the moment let us take its results to equate with a correspondent\u2019s importance in the Republic of Texas letter network.\n\n\n\n\n\n\nCalculate the PageRank by clicking on the \"Run\" button next to \"PageRank\" in the \"Statistics\" tab. You will be presented with a prompt asking for a few parameters; make sure \"Directed\" network is selected and that the algorithm is taking edge weight into account (by selecting \"Use edge weight\"). Leave all other parameters at their default. \n\n\n\n\n\n\nPress \"OK\".\n\n\n\n\n\n\nOnce PageRank is calculated, if you click back into the \"Data Laboratory\" and select the \"Nodes\" list in the Data Table, you can see that a new \"PageRank\" column has been added, with values for every node. The higher the PageRank, the more central a correspondent is in the network. \n\n\n\n\n\n\n \n\n\n\n\n\n\nGoing back to the Overview pane, you can visualize this centrality by changing the size of each correspondent\u2019s node based on its PageRank. Do this in the \"Ranking\" tab on the left side of the Overview pane.\n\n\n\n\n\n\nMake sure \"Nodes\" is selected, press the icon of a little red diamond, and select PageRank from the drop-down menu. \n\n\n\n\n\n\nIn the parameter options just below, enter the \"Min size\" as 1 and the \"Max size\" as 10. \n\n\n\n\n\n\nPress \"Apply,\" and watch the nodes resize based on their PageRank. \n\n\n\n\n\n\nTo be on the safe side and decrease clutter, re-run the \"Force Atlas 2\" layout as described above, making sure to keep the \"Prevent Overlap\" box checked.\n\n\n\n\n\n\n \n\n\nAt this point, the network is processed enough to visualize in the Preview pane, to finally begin making sense of the data. \n\n\n\n\n\n\nIn Preview, on the left-hand side, select \"Show Labels,\" \"Proportional Size,\" \"Rescale Weight,\" and deselect \"Curved\" edges. \n\n\n\n\n\n\nPress \"Refresh.\" \n\n\n\n\n\n\n \n\n\n\n\n\n\nSo what have we got?\n\n\nThe visualization immediately reveals apparent structure: central figures on the top (Ashbel Smith and Anson Jones) and bottom (Mirabeau B. Lamar, James Webb, and James Treat), and two central figures who connect the two split communities (James Hamilton and James S. Mayfield). A quick search online shows the top network to be associated with the last president of the Republic of Texas, Anson Jones; whereas the bottom network largely revolves around the second president, Mirabeau Lamar. Experts on this period in history could use this analysis to understand the structure of communities building to the annexation of Texas or they could ask meta-questions about the nature of the data themselves. For example, why is Sam Houston, the first and third president of the Republic of Texas, barely visible in this network?\n\n\nWrite up your own observations on this process in your open notebook, and export your gephi file as a \n.graphml\n file (because Gephi's \n.gephi\n format is a bit unstable, always save as or export your work in a variety of formats). Upload that to your repository.", 
            "title": "Social Network Analysis with Gephi"
        }, 
        {
            "location": "/supporting materials/gephi.txt/#working-with-gephi", 
            "text": "Before we go much further, I would recommend that you look at the work of Clement Levallois, who has a  suite of excellent tutorials on working with Gephi . The tutorial below is adapted from our open draft of  The Macroscope .", 
            "title": "Working with Gephi"
        }, 
        {
            "location": "/supporting materials/gephi.txt/#introduction", 
            "text": "Gephi is quickly becoming the tool of choice for network analysts who do not need the full suite of algorithms offered by  Pajek  or  UCINET . It is relatively easy to use (eclipsed in this only by  NodeXL ), it is usable on all platforms, it can analyze fairly large networks, and it creates beautiful visualizations. The development community is also extremely active, with improvements being added constantly. We recommend Gephi for the majority of historians undertaking serious network analysis research.   Download and install  Gephi  onto your machine.", 
            "title": "Introduction"
        }, 
        {
            "location": "/supporting materials/gephi.txt/#installing-gephi-on-os-10-mavericks", 
            "text": "NB Since writing this, Gephi announced the  release of Gephi 0.9 . This newer Gephi should solve these problems.  Mac users might have some trouble installing Gephi 0.8. We have found that, on Mac OS X Mavericks, Gephi does not load properly after installation. This is a Java-related issue, so you\u2019ll need to install an earlier version of Java than the one provided.   To fix this:    Control click (or right-click) on the Gephi package    Select \u201cshow package contents.\u201d     Click on \u201ccontents   resources   gephi   etc.\u201d     Control-click (or right-click) on \u201cgephi.conf\u201d and open with your text editor.     Find the line reading:  #jdkhome=\"/path/to/jdk\"  and paste the following code:  jdkhome=\"/System/Library/Java/JavaVirtualMachines/1.6.0.jdk/Contents/Home    Save that file. Then, go to  Apple support  and install the older version of Java (Java 6). Once that is installed, Gephi should run normally.    Run Gephi once it is installed. You will be presented with a welcome prompting you to open a recent file, create a new project, or load a sample file.     Click \u201cNew Project\u201d and then click the \u201cData Laboratory\u201d tab on the horizontal bar at the top of the Gephi window (Fig. 7.3).", 
            "title": "Installing Gephi on OS 10 Mavericks"
        }, 
        {
            "location": "/supporting materials/gephi.txt/#when-not-to-use-networks", 
            "text": "Networks analysis can be a powerful method for engaging with a historical dataset but it is often not the most appropriate. Any historical database may be represented as a network with enough contriving but few should be. One could take the Atlantic trade network, including cities, ships, relevant governments and corporations, and connect them all together in a giant multipartite network. It would be extremely difficult to derive any meaning from this network, especially using traditional network analysis methodologies. Clustering coefficients (a useful network metric), for example, would be meaningless in this situation, as it would be impossible for the neighbors of any one node to be neighbors with one another. Network scientists have developed algorithms for  multimodal networks , but they are often created for fairly specific purposes, and one should be very careful before applying them to a different dataset and using that analysis to explore a historiographical question.  Networks, as they are commonly encoded, also suffer from a profound lack of nuance. It is all well to say that, because Henry Oldenburg corresponded with both Gottfried Leibniz and John Milton, he was the short connection between the two men. However, the encoded network holds no information of whether Oldenburg actually transferred information between the two or whether that connection was at all meaningful. In a flight network, Las Vegas and Atlanta might both have very high betweenness centrality because people from many other cities fly to or from those airports, and yet one is much more of a hub than the other. This is because, largely, people tend to fly directly back and forth from Las Vegas, rather than through it, whereas people tend to fly through Atlanta from one city to another. This is the type of information that network analysis, as it is currently used, is not well equipped to handle. Whether this shortcoming affects a historical inquiry depends primarily on the inquiry itself.  When deciding whether to use networks to explore a historiographical question, the first question should be: to what extent is connectivity specifically important to this research project? The next should be: can any of the network analyses my collaborators or I know how to employ be specifically useful in the research project? If either answer is negative, another approach is probably warranted.", 
            "title": "When Not To Use Networks"
        }, 
        {
            "location": "/supporting materials/gephi.txt/#texan-correspondence", 
            "text": "You will need the information you  created in Module 3, after cleaning the correspondence data using Open Refine .", 
            "title": "Texan Correspondence"
        }, 
        {
            "location": "/supporting materials/gephi.txt/#umm-i-never-did-manage-that-open-refine-stuff", 
            "text": "In module 3, you used Notepad++ or Textwrangler, regular expressions, and OpenRefine to create a comma-separated value file like    ****.csv    of the diplomatic correspondence of the Republic of Texas. The final version of the file you created has a row for each letter listed in the volume, and in each row the name of the sender and the recipient. The file should look like this:  source,target\nSam Houston,J. Pinckney Henderson\nJames Webb,Alc6e La Branche\nDavid G. Burnet,Richard G. Dunlap\n...  This file is called 'an edge list'   it's a list of connections, or edges, between the individuals. If you no longer have the file, you can find it on  The Macroscope website .", 
            "title": "Umm, I never did manage that Open Refine stuff..."
        }, 
        {
            "location": "/supporting materials/gephi.txt/#quick-instructions-for-getting-the-data-into-gephi", 
            "text": "Open Gephi by double-clicking its icon.   Click \u201cnew project.\u201d The middle pane of the interface window is the \u201cData Laboratory,\u201d where you can interact with your network data in spreadsheet format. This is where we can import the data cleaned up in OpenRefine.   In the Data Laboratory, select \u201cImport Spreadsheet.\u201d   Press the ellipsis \u201c...\u201d and locate the CSV you created. Make sure that the Separator is listed as \u201cComma\u201d and the \u201cAs table\u201d is listed as \u201cEdges table.\u201d   Press \u201cNext,\u201d then \u201cFinish.\u201d Your data should load up.   Click on the \u201coverview\u201d tab and you will be presented with a tangled network graph.", 
            "title": "Quick instructions for getting the data into Gephi:"
        }, 
        {
            "location": "/supporting materials/gephi.txt/#navigating-gephi", 
            "text": "Gephi is broken up into three panes:  Overview ,  Data Laboratory , and  Preview .    The  Overview pane  is used to manipulate the visual properties of the network: change colors of nodes or edges, lay them out in different ways, and so forth. The Overview pane is also where you can apply algorithms to the network, like those you learned about in the previous chapter.   The  Data Laboratory  is for adding, manipulating, and removing data.   Use the  Preview  pane to do some final tweaks on the look and feel of the network and to export an image for publication.   There is one tweak that needs to done in the Data Table before the dataset is fully ready to be explored in Gephi.     Click on the Data Laboratory tab.    Click on the \u201cNodes\u201d tab in the Data Table (this should be open already) and notice that, of the three columns, \u201cLabel\u201d (the furthermost field on the right) is blank in every row. This will be a problem when viewing the network visualization, as those labels are essential for the network to be meaningful.     In the \u201cNodes\u201d tab, click \u201cCopy data to other column\u201d at the bottom, select \u201cID\u201d, and press \u201cOk\u201d (Fig. 7.5). Upon doing so, the \u201cLabel\u201d column will be filled with the appropriate labels for each correspondent.     While you\u2019re still in the Data Laboratory, look in the \u201cEdges\u201d tab and notice there is a \u201cWeight\u201d column. Gephi automatically counted every time a letter was sent from correspondent A to correspondent B and summed up all the occurrences, resulting in the \u201cWeight.\u201d This means that J. Pinckney Henderson sent three letters to James Webb, because Henderson is in the \u201cSource\u201d column, Webb in the \u201cTarget,\u201d, and the \u201cWeight\u201d is three.    Clicking on the Overview pane will take you to a visual representation of the network you just imported. In the middle of the screen, you will see your network in the \u201cGraph\u201d tab. The \u201cContext\u201d tab, at the top right, will show that you imported 234 nodes and 394 edges. At first, all the nodes will be randomly strewn across the screen and make little visual sense.          Fix the nodes by selecting a layout in the \u201cLayout\u201d tab \u2013 the best one for beginners is \u201cForce Atlas 2.\u201d     Press the \u201cRun\u201d button and watch the nodes and edges reorganize on the screen into something slightly more manageable. After the layout runs for a few minutes, re-press the button (now labeled \u201cStop\u201d) to settle the nodes in their place.   You just ran a force-directed layout. Each dot is a correspondent in the network, and lines between dots represent letters sent between individuals. Thicker lines represent more letters, and arrows represent the direction the letters were sent, such that there may be up to two lines connecting any two correspondents (one for each direction).  About two-dozen smaller components of the network will appear to shoot off into the distance, unconnected from the large, connected component in the middle. For the purpose of this exercise, we are not interested in those disconnected components, so the next step will be to filter them out of the network.     The first step is to calculate which components of the network are connected to which others; do this by clicking \u201cRun\u201d next to the text that says \u201cConnected Components\u201d in the \u201cStatistics\u201d tab on the right-hand side.      Once there, select \u201cUnDirected\u201d and press \u201cOK.\u201d     Press \u201cClose\u201d when the report pops up indicating that the algorithm has finished running. Now that this is done, Gephi knows which is the giant connected component and has labeled that component \u201c0\u201d.     To filter out everything but the giant component, click on the \u201cFilters\u201d tab on the right-hand side and browse to \"Component ID Integer (Node)\" in the folder directory (you\u2019ll find it under \"Attributes,\" then \"Equal\").     Double-click \"Component ID Integer (Node)\" and click the \"Filter\" button at the bottom. Doing this removes the disconnected bundles of nodes.  There are many possible algorithms you could use for the analysis step, but in this case you will use the PageRank of each node in the network. This measurement calculates the prestige of a correspondent according to how often others write to him or her. The process is circular, such that correspondents with high prestige will confer their prestige on those they write to, who in turn pass their prestige along to their own correspondents. For the moment let us take its results to equate with a correspondent\u2019s importance in the Republic of Texas letter network.    Calculate the PageRank by clicking on the \"Run\" button next to \"PageRank\" in the \"Statistics\" tab. You will be presented with a prompt asking for a few parameters; make sure \"Directed\" network is selected and that the algorithm is taking edge weight into account (by selecting \"Use edge weight\"). Leave all other parameters at their default.     Press \"OK\".    Once PageRank is calculated, if you click back into the \"Data Laboratory\" and select the \"Nodes\" list in the Data Table, you can see that a new \"PageRank\" column has been added, with values for every node. The higher the PageRank, the more central a correspondent is in the network.          Going back to the Overview pane, you can visualize this centrality by changing the size of each correspondent\u2019s node based on its PageRank. Do this in the \"Ranking\" tab on the left side of the Overview pane.    Make sure \"Nodes\" is selected, press the icon of a little red diamond, and select PageRank from the drop-down menu.     In the parameter options just below, enter the \"Min size\" as 1 and the \"Max size\" as 10.     Press \"Apply,\" and watch the nodes resize based on their PageRank.     To be on the safe side and decrease clutter, re-run the \"Force Atlas 2\" layout as described above, making sure to keep the \"Prevent Overlap\" box checked.       At this point, the network is processed enough to visualize in the Preview pane, to finally begin making sense of the data.     In Preview, on the left-hand side, select \"Show Labels,\" \"Proportional Size,\" \"Rescale Weight,\" and deselect \"Curved\" edges.     Press \"Refresh.\"", 
            "title": "Navigating Gephi"
        }, 
        {
            "location": "/supporting materials/gephi.txt/#so-what-have-we-got", 
            "text": "The visualization immediately reveals apparent structure: central figures on the top (Ashbel Smith and Anson Jones) and bottom (Mirabeau B. Lamar, James Webb, and James Treat), and two central figures who connect the two split communities (James Hamilton and James S. Mayfield). A quick search online shows the top network to be associated with the last president of the Republic of Texas, Anson Jones; whereas the bottom network largely revolves around the second president, Mirabeau Lamar. Experts on this period in history could use this analysis to understand the structure of communities building to the annexation of Texas or they could ask meta-questions about the nature of the data themselves. For example, why is Sam Houston, the first and third president of the Republic of Texas, barely visible in this network?  Write up your own observations on this process in your open notebook, and export your gephi file as a  .graphml  file (because Gephi's  .gephi  format is a bit unstable, always save as or export your work in a variety of formats). Upload that to your repository.", 
            "title": "So what have we got?"
        }, 
        {
            "location": "/supporting materials/multimode-networks.txt/", 
            "text": "Transforming 2-mode network data to 1-mode\n\n\nNetworks can be composed of all sorts of things. Trains, busses, Uber, metro \n all of these can be combined into a 'transportation' network. Books, authors, editors, funders, censors, sponsors \n a publishing network. Students, profs, classes, universities \n an education network. Anytime you have more than one kind of \nthing\n (however defined) in your network, you formally have a bipartite (or tripartite, and so on, depending on the number of things) network. Visualizing such a network as nodes and edges can be a useful heuristic exercise. \n\n\nBut\n\n\nIf you are doing this in Gephi, and you run some of Gephi's metrics (statistics) on a bipartite network, your results may not mean what you think they mean. The metrics, the algorithms assume 1-mode networks. Thus, running them on 2-mode (or more!) networks will result in ....issues.\n\n\n\n\nNow in truth, it's not quite so clear-cut to simply say, 'convert every bimodal network to unimodal before running any stats' (read \nScott's discussion here\n), but as a rule of thumb for when you're getting started, you'll be on much firmer methodological grounds if you transform your mutlimodal networks into a series of 1-mode networks. So convert every bimodal network to unimodal before running any statistics on your network(s).\n\n\nThus, this network: \n\nProfA -\n student1   (where -\n is a directed relationship, 'teaches')\n\nProfA -\n student2 \n\nProfB -\n student3 \n\nProbB -\n student1 \n\n\n...can be transformed into \ntwo\n networks, one where profA is connected to profB by virtue of a shared student (student1). That is, profs connected by students; and the inverse: students connected by profs. You could then run your metrics, and get two different perspectives on the educational dynamics of this particular university.\n\n\nIn this exercise, you'll transform a network of women and social organizations into two 1-mode networks.\n\n\nThe data\n\n\nThe data for this exercise comes from a former Carleton public history MA student, Peter Holdsworth. Peter lodged \na copy of his MA research on Figshare\n. Peter was interested in the social networks surrounding ideas of commemoration of the centenerary of the War of 1812, in 1912. He studied the membership rolls for women\u2019s service organization in Ontario both before and after that centenerary. By making his data public, Peter enables others to build upon his own research in a way not commonly done in history. (You can \nfollow Peter on Twitter\n.)\n\n\n\n\nRight-click and 'save link' to get \nthe data files you'll need for this exercise\n\n\n\n\nConfiguring Gephi\n\n\nThere is a plugin for Gephi that we will use to transform our network. \n\n\n\n\n\n\nOpen Gephi. Across the top of Gephi in the menu ribbon you\u2019ll see \nFile Workspace View Tools Window Plugins Help\n. \n\n\n\n\n\n\nTo get and install the plugin, select \nTools \n Plugins\n (The top level menu item 'Plugins' is empty and not used \n a useful reminder that Gephi is still in \nbeta\n).\n\n\n\n\n\n\nIn the popup, under \u2018available plugins\u2019 look for \u2018MultimodeNetworksTransformation\u2019. Tick this box, then click on Install.\n\n\n\n\n\n\nFollow the instructions, ignore any warnings, click on \u2018finish\u2019. You may or may not need to restart Gephi to get the plugin running. If you suddenly see on the far right of ht Gephi window a new tab besid \u2018statistics\u2019, \u2018filters\u2019, called \u2018Multimode Network\u2019, then you\u2019re ok.\n\n\n\n\n\n\n\n\nImporting the data\n\n\n\n\n\n\nUnder \u2018file\u2019, select -\n New project.\n\n\n\n\n\n\nOn the data  laboratory tab, select Import-spreadsheet, and in the pop-up, make sure to select under \u2018as table: EDGES table. Select \nwomen-orgs.csv\n.\n\n\n\n\n\n\nClick \u2018next\u2019, click finish.\n\n\n\n\n\n\nOn the data table, have \u2018edges\u2019 selected. This is showing you the source and the target for each link (aka \u2018edge\u2019). This implies a directionality to the relationship that we just don\u2019t know \u2013 so down below, when we get to statistics, we will always have to make sure to tell Gephi that we want the network treated as \u2018undirected\u2019. More on that below.\n\n\n\n\nLoading your CSV file, step 1.\n\n\n\n\nLoading your CSV file, step 2\n\n\n\n\n\n\nClick on \u2018copy data to other column\u2019. Select \u2018Id\u2019. In the pop-up, select \u2018Label\u2019. You now have your edges labelled.\n\n\n\n\n\n\nJust as you did above, now import NODES \nwomen-names.csv\n.\n\n\n\n\n\n\nMaking sure you're on the Nodes page in the data laboratory, copy ID to Label to that your nodes are labelled.\n\n\n\n\n\n\nNB You can always add more attribute data to your network this way, as long as you always use a column called Id so that Gephi knows where to slot the new information. Make sure to never tick off the box labeled \u2018force nodes to be created as new ones\u2019\n\n\nPrepping your data\n\n\nWe're now going to manipulate the data a bit in order to get it ready for the transformation. In this data, we have women, and we have organizations. We need to tell Gephi which rows are the organizations. In Peter's original data input phase, he decided to use a unique number for each woman so that he would minimize data entry errors (misspellings and so on), as well as control for the use of maiden and married names. Miss Eliza Smith might become, at a later date, Mrs. George Doe. In Peter's scheme, this is the same person (remember in module 3 in the TEI exercise how we dealt with such issues?).\n\n\n\n\n\n\nOn your NODES page in the data laboratory, add new column, make it boolean. Call it \u2018organization\u2019\n\n\n\n\n\n\n\n\nIn the Filter box, type [a-z], and select Id \u2013 this filters out all the women. (What would you have to do to filter out the organizations? Remember, this is a regex search!)\n\n\n\n\n\n\nTick off the check boxes in the \u2018organization\u2019 columns.\n\n\n\n\nI note a TYPO in the image above, 'a-b'. that should be, 'a-z'\n\n\n\n\n\n\nSave this as \nwomen-organizations-2-mode.gephi\n.\n\n\nPro tip\n: always export your data from gephi (file \n export) in .net or .graphml or .gefx format as the .gephi format (which is your only option under file \n save as) is unstable. That is, sometimes gephi won't read .gephi files! I did say this was \nbeta\n software).\n\n\n\n\n\n\nTransforming the network\n\n\nAt this point, you have a two mode network in gephi. You could click on the 'overview' panel and play with some of the layouts and begin to form impressions about the nature of your data. Remember though any metrics calculated at this point would be largely spurious. Let's transform this two-mode network so that we can explore how women are connected to other women via shared membership.\n\n\n\n\nOn the multimode networks projection tab:\n\n\n\n\n\n\nClick load attributes.\n\n\n\n\n\n\nIn \u2018attribute type\u2019, select organization\n\n\n\n\n\n\nIn left matrix, select \u2018false \u2013 true\u2019 (or \u2018null \u2013 true\u2019)\n\n\n\n\n\n\nIn right matrix, select \u2018true \u2013 false\u2019. (or \u2018true \u2013 null\u2019) (do you see why this is the case? what would selecting the inverse accomplish?)\n\n\n\n\n\n\nSelect \u2018remove edges\u2019 and \u2018remove nodes\u2019.\n\n\n\n\n\n\nOnce you hit \u2018run\u2019, organizations will be removed from your bipartite network, leaving you with a single-mode network. hit \u2018run\u2019.\n\n\n\n\n\n\nSave as \nwomen-to-women-network.gephi\n \nand\n export as \nwomen-to-women.net\n\n\nNB If your nodes data table is blank, your filter might still be active. make sure the filter box is clear. You should be left with a list of women (ie, a list of nodes where the identiers are numbers, per Peter's schema).\n\n\n\n\n\n\nAt this point, you could re-start Gephi and reload your \u2018women-organizations-2-mode.gephi\u2019 file and re-run the multimode networks projection so that you are left with an organization to organization network. Do this, and save and export with appropriate file names.\n\n\n\n\n\n\nExploring this network\n\n\nPeter's data has a number of \nattributes\n describing it, including the membership year. So let's see what this network of women looks like in 1902.\n\n\n\n\nUnder the filters tab at the right hand side of the Gephi interface, select \u2018attributes \u2013 equal\u2019 and then drag \u20181902\u2019 to the queries box.\n\n\nIn \u2018pattern\u2019 enter [0-9] and tick the \u2018use regex\u2019 box.\n\n\nClick ok and then click \u2018filter\u2019.\n\n\n\n\nYou should now have a network with 188 nodes and 8728 edges, showing the women who were active in 1902.\n\n\nLet\u2019s learn something about this network. Under the statistics tab at the right hand side of the Gephi interface,\n\n\n\n\nRun \u2018avg. path length\u2019 by clicking on \u2018run\u2019\n\n\nIn the pop up that opens, select \u2018undirected\u2019 (as we know nothing about directionality in this network; we simply know that two women were members of the same organization at the same time. Note also that if the same pair were members of the more than one organziation, the weight of their connection will be corresponding stronger).\n\n\nClick ok.\n\n\nRun \u2018modularity\u2019 to look for subgroups. make sure \u2018randomize\u2019 and \u2018use weights\u2019 are selected. Leave \u2018resolution\u2019 at 1.0\n\n\n\n\nWe selected 'average path length' because one of the byproducts of this routine is 'betweeness centrality'. We're making an assumption here that a woman who has a high betweeness centrality score was in a position to affect information flow in 1902 society. Modularity looks at similar patterns of connections to cluster women who have more-or-less similar connections into groups.\n\n\nLet\u2019s visualize what we\u2019ve just learned.\n\n\n\n\nOn the \u2018partition\u2019 tab, over on the left hand side of the \u2018overview\u2019 screen, click on nodes, then click the green arrows beside \u2018choose a partition parameter\u2019.\n\n\nClick on \u2018choose a partition parameter\u2019. \n\n\nScroll down to modularity class. The different groups will be listed, with their colours and their % composition of the network.\n\n\nHit \u2018apply\u2019 to recolour your network graph.\n\n\n\n\nLet\u2019s resize the nodes to show off betweeness-centrality (to figure out which woman was in the greatest position to influence flows of information in this network.) \n\n\n\n\nClick \u2018ranking\u2019. (It's on the left hand side of the interface, beside 'partition' and just below 'overview'.\n\n\nClick \u2018nodes\u2019.\n\n\nClick the down arrow on \u2018choose a rank parameter\u2019. \n\n\nSelect \u2018betweeness centrality\u2019.\n\n\nClick the red diamond. This will resize the nodes according to their \u2018betweeness centrality\u2019.\n\n\nClick \u2018apply\u2019.\n\n\nDown at the bottom of the middle panel, click the large black \u2018T\u2019 to display labels.\n\n\nClick the black letter \u2018A\u2019 and select \u2018node size\u2019.\n\n\n\n\nMrs. Mary Elliot-Murray-Kynynmound and Mrs. John Henry Wilson should now dominate your network (if you go back to the original data zip, you'll be able to find Peter's key to figure out who's who). Who were these ladies? What organizations were they members of? Who were they connected to? To the archives!\n\n\nCongratulations! You\u2019ve imported historical network data into Gephi, transformed it, manipulated it, and run some analyses. Play with the settings on \u2018preview\u2019 in order to share your visualization as SVG, PDF, or PNG.\n\n\nNow go back to your original gephi file, and recast it as organizations to organizations via shared members, to figure out which organizations were key in early 20th century Ontario\u2026 make appropriate notes in your open notebook.", 
            "title": "Multimode Networks"
        }, 
        {
            "location": "/supporting materials/multimode-networks.txt/#transforming-2-mode-network-data-to-1-mode", 
            "text": "Networks can be composed of all sorts of things. Trains, busses, Uber, metro   all of these can be combined into a 'transportation' network. Books, authors, editors, funders, censors, sponsors   a publishing network. Students, profs, classes, universities   an education network. Anytime you have more than one kind of  thing  (however defined) in your network, you formally have a bipartite (or tripartite, and so on, depending on the number of things) network. Visualizing such a network as nodes and edges can be a useful heuristic exercise.   But  If you are doing this in Gephi, and you run some of Gephi's metrics (statistics) on a bipartite network, your results may not mean what you think they mean. The metrics, the algorithms assume 1-mode networks. Thus, running them on 2-mode (or more!) networks will result in ....issues.   Now in truth, it's not quite so clear-cut to simply say, 'convert every bimodal network to unimodal before running any stats' (read  Scott's discussion here ), but as a rule of thumb for when you're getting started, you'll be on much firmer methodological grounds if you transform your mutlimodal networks into a series of 1-mode networks. So convert every bimodal network to unimodal before running any statistics on your network(s).  Thus, this network:  \nProfA -  student1   (where -  is a directed relationship, 'teaches') \nProfA -  student2  \nProfB -  student3  \nProbB -  student1   ...can be transformed into  two  networks, one where profA is connected to profB by virtue of a shared student (student1). That is, profs connected by students; and the inverse: students connected by profs. You could then run your metrics, and get two different perspectives on the educational dynamics of this particular university.  In this exercise, you'll transform a network of women and social organizations into two 1-mode networks.", 
            "title": "Transforming 2-mode network data to 1-mode"
        }, 
        {
            "location": "/supporting materials/multimode-networks.txt/#the-data", 
            "text": "The data for this exercise comes from a former Carleton public history MA student, Peter Holdsworth. Peter lodged  a copy of his MA research on Figshare . Peter was interested in the social networks surrounding ideas of commemoration of the centenerary of the War of 1812, in 1912. He studied the membership rolls for women\u2019s service organization in Ontario both before and after that centenerary. By making his data public, Peter enables others to build upon his own research in a way not commonly done in history. (You can  follow Peter on Twitter .)   Right-click and 'save link' to get  the data files you'll need for this exercise", 
            "title": "The data"
        }, 
        {
            "location": "/supporting materials/multimode-networks.txt/#configuring-gephi", 
            "text": "There is a plugin for Gephi that we will use to transform our network.     Open Gephi. Across the top of Gephi in the menu ribbon you\u2019ll see  File Workspace View Tools Window Plugins Help .     To get and install the plugin, select  Tools   Plugins  (The top level menu item 'Plugins' is empty and not used   a useful reminder that Gephi is still in  beta ).    In the popup, under \u2018available plugins\u2019 look for \u2018MultimodeNetworksTransformation\u2019. Tick this box, then click on Install.    Follow the instructions, ignore any warnings, click on \u2018finish\u2019. You may or may not need to restart Gephi to get the plugin running. If you suddenly see on the far right of ht Gephi window a new tab besid \u2018statistics\u2019, \u2018filters\u2019, called \u2018Multimode Network\u2019, then you\u2019re ok.", 
            "title": "Configuring Gephi"
        }, 
        {
            "location": "/supporting materials/multimode-networks.txt/#importing-the-data", 
            "text": "Under \u2018file\u2019, select -  New project.    On the data  laboratory tab, select Import-spreadsheet, and in the pop-up, make sure to select under \u2018as table: EDGES table. Select  women-orgs.csv .    Click \u2018next\u2019, click finish.    On the data table, have \u2018edges\u2019 selected. This is showing you the source and the target for each link (aka \u2018edge\u2019). This implies a directionality to the relationship that we just don\u2019t know \u2013 so down below, when we get to statistics, we will always have to make sure to tell Gephi that we want the network treated as \u2018undirected\u2019. More on that below.   Loading your CSV file, step 1.   Loading your CSV file, step 2    Click on \u2018copy data to other column\u2019. Select \u2018Id\u2019. In the pop-up, select \u2018Label\u2019. You now have your edges labelled.    Just as you did above, now import NODES  women-names.csv .    Making sure you're on the Nodes page in the data laboratory, copy ID to Label to that your nodes are labelled.    NB You can always add more attribute data to your network this way, as long as you always use a column called Id so that Gephi knows where to slot the new information. Make sure to never tick off the box labeled \u2018force nodes to be created as new ones\u2019", 
            "title": "Importing the data"
        }, 
        {
            "location": "/supporting materials/multimode-networks.txt/#prepping-your-data", 
            "text": "We're now going to manipulate the data a bit in order to get it ready for the transformation. In this data, we have women, and we have organizations. We need to tell Gephi which rows are the organizations. In Peter's original data input phase, he decided to use a unique number for each woman so that he would minimize data entry errors (misspellings and so on), as well as control for the use of maiden and married names. Miss Eliza Smith might become, at a later date, Mrs. George Doe. In Peter's scheme, this is the same person (remember in module 3 in the TEI exercise how we dealt with such issues?).    On your NODES page in the data laboratory, add new column, make it boolean. Call it \u2018organization\u2019     In the Filter box, type [a-z], and select Id \u2013 this filters out all the women. (What would you have to do to filter out the organizations? Remember, this is a regex search!)    Tick off the check boxes in the \u2018organization\u2019 columns.   I note a TYPO in the image above, 'a-b'. that should be, 'a-z'    Save this as  women-organizations-2-mode.gephi .  Pro tip : always export your data from gephi (file   export) in .net or .graphml or .gefx format as the .gephi format (which is your only option under file   save as) is unstable. That is, sometimes gephi won't read .gephi files! I did say this was  beta  software).", 
            "title": "Prepping your data"
        }, 
        {
            "location": "/supporting materials/multimode-networks.txt/#transforming-the-network", 
            "text": "At this point, you have a two mode network in gephi. You could click on the 'overview' panel and play with some of the layouts and begin to form impressions about the nature of your data. Remember though any metrics calculated at this point would be largely spurious. Let's transform this two-mode network so that we can explore how women are connected to other women via shared membership.   On the multimode networks projection tab:    Click load attributes.    In \u2018attribute type\u2019, select organization    In left matrix, select \u2018false \u2013 true\u2019 (or \u2018null \u2013 true\u2019)    In right matrix, select \u2018true \u2013 false\u2019. (or \u2018true \u2013 null\u2019) (do you see why this is the case? what would selecting the inverse accomplish?)    Select \u2018remove edges\u2019 and \u2018remove nodes\u2019.    Once you hit \u2018run\u2019, organizations will be removed from your bipartite network, leaving you with a single-mode network. hit \u2018run\u2019.    Save as  women-to-women-network.gephi   and  export as  women-to-women.net  NB If your nodes data table is blank, your filter might still be active. make sure the filter box is clear. You should be left with a list of women (ie, a list of nodes where the identiers are numbers, per Peter's schema).    At this point, you could re-start Gephi and reload your \u2018women-organizations-2-mode.gephi\u2019 file and re-run the multimode networks projection so that you are left with an organization to organization network. Do this, and save and export with appropriate file names.", 
            "title": "Transforming the network"
        }, 
        {
            "location": "/supporting materials/multimode-networks.txt/#exploring-this-network", 
            "text": "Peter's data has a number of  attributes  describing it, including the membership year. So let's see what this network of women looks like in 1902.   Under the filters tab at the right hand side of the Gephi interface, select \u2018attributes \u2013 equal\u2019 and then drag \u20181902\u2019 to the queries box.  In \u2018pattern\u2019 enter [0-9] and tick the \u2018use regex\u2019 box.  Click ok and then click \u2018filter\u2019.   You should now have a network with 188 nodes and 8728 edges, showing the women who were active in 1902.  Let\u2019s learn something about this network. Under the statistics tab at the right hand side of the Gephi interface,   Run \u2018avg. path length\u2019 by clicking on \u2018run\u2019  In the pop up that opens, select \u2018undirected\u2019 (as we know nothing about directionality in this network; we simply know that two women were members of the same organization at the same time. Note also that if the same pair were members of the more than one organziation, the weight of their connection will be corresponding stronger).  Click ok.  Run \u2018modularity\u2019 to look for subgroups. make sure \u2018randomize\u2019 and \u2018use weights\u2019 are selected. Leave \u2018resolution\u2019 at 1.0   We selected 'average path length' because one of the byproducts of this routine is 'betweeness centrality'. We're making an assumption here that a woman who has a high betweeness centrality score was in a position to affect information flow in 1902 society. Modularity looks at similar patterns of connections to cluster women who have more-or-less similar connections into groups.  Let\u2019s visualize what we\u2019ve just learned.   On the \u2018partition\u2019 tab, over on the left hand side of the \u2018overview\u2019 screen, click on nodes, then click the green arrows beside \u2018choose a partition parameter\u2019.  Click on \u2018choose a partition parameter\u2019.   Scroll down to modularity class. The different groups will be listed, with their colours and their % composition of the network.  Hit \u2018apply\u2019 to recolour your network graph.   Let\u2019s resize the nodes to show off betweeness-centrality (to figure out which woman was in the greatest position to influence flows of information in this network.)    Click \u2018ranking\u2019. (It's on the left hand side of the interface, beside 'partition' and just below 'overview'.  Click \u2018nodes\u2019.  Click the down arrow on \u2018choose a rank parameter\u2019.   Select \u2018betweeness centrality\u2019.  Click the red diamond. This will resize the nodes according to their \u2018betweeness centrality\u2019.  Click \u2018apply\u2019.  Down at the bottom of the middle panel, click the large black \u2018T\u2019 to display labels.  Click the black letter \u2018A\u2019 and select \u2018node size\u2019.   Mrs. Mary Elliot-Murray-Kynynmound and Mrs. John Henry Wilson should now dominate your network (if you go back to the original data zip, you'll be able to find Peter's key to figure out who's who). Who were these ladies? What organizations were they members of? Who were they connected to? To the archives!  Congratulations! You\u2019ve imported historical network data into Gephi, transformed it, manipulated it, and run some analyses. Play with the settings on \u2018preview\u2019 in order to share your visualization as SVG, PDF, or PNG.  Now go back to your original gephi file, and recast it as organizations to organizations via shared members, to figure out which organizations were key in early 20th century Ontario\u2026 make appropriate notes in your open notebook.", 
            "title": "Exploring this network"
        }, 
        {
            "location": "/supporting materials/graphing-the-net.txt/", 
            "text": "Graphing the Net\n\n\nIt may be that you are interested in the structure of links on the internet. Perhaps you'd like to see how a particular topic plays out across Wikiepedia. What you'd have to do is a \nweb-crawl\n. Below are quick instructions for setting up a webcrawl, which will follow every link on a page to a particular depth, and echo the results through your browser into Gephi for visualization and analysis.\n\n\nYou will need:\n\n\n\n\nThe Chrome browser with \nSite Spider Mk II\n installed\n\n\nGephi\n\n\nHTTP Graph Generator Plugin for Gephi installed\n\n\n\n\nNB\n You can install the graph generator plugin from \nwithin\n Gephi:\n\n\n\n\nSelect 'Tools' on the main menu ribbon at the top of the screen (and \nnot\n the 'plugins' item).\n\n\nWithin 'Tools', select 'plugins' and then 'available plugins'\n\n\nSearch for 'HTTPGraph'\n\n\nTick off the box and install it. \n\n\nWhen finished, close and restart Gephi with a new project.\n\n\n\n\nGetting set up to scrape\n\n\n\n\n\n\nIn Chrome, go to the settings page (the 'hamburg' icon at the extreme right of the address bar, or by typing \nchrome://settings/\n in the address bar. \n\n\n\n\n\n\nClick on 'show advanced settings' at the bottom of the page.\n\n\n\n\n\n\nScroll down to 'Network' and click on 'change proxy settings'. \n\n\n\n\n\n\nIn the popup that opens, click the 'connections' tab, and then the 'LAN Settings' button. \n\n\nAnother\n popup will open. \n\n\n\n\n\n\nSelect the 'Use a proxy server for your LAN'. \n\n\n\n\n\n\nEnter \n127.0.0.1\n for the address, and \n8088\n for the port. Now, whenever you go to a website, the info will be echoed through that port. We need to set Gephi up to hear what's being passed.\n\n\n\n\n\n\nOpen Gephi and start a new project. \n\n\n\n\n\n\nGo to 'file', then 'generate', and then 'http graph'. A pop up will open, asking you to specify a port. \n\n\n\n\n\n\nInput \n8088\n. Accept the defaults, and press OK. On the overview panel, nodes will begin to appear when you go to a URL in Chrome. \n\n\n\n\n\n\nBegin the scrape\n\n\n\n\nGo back to Chrome. \n\n\nPut in the URL that you want to start your scrape on, eg \nhttp://en.wikipedia.org/wiki/Archaeology\n.\n\n\nClick the SiteSpider II button in your toolbar. Site spider will open a popup asking you how far and how deep you want to scrape. \n\n\nSet the SiteSpider II parameters accordingly. \n\n\nHit 'go' and then flip over to your Gephi window. You'll see the network begin to populate! Let it run as long as you want. \n\n\nWhen you're finished, save your network by 'exporting' it (on the file menu) as any other format than .gephi. I say this because I find sometimes the .gephi format is unstable. You can then filter your network for resources or html pages or what have you. I suggest deleting the node labelled 127.0.0.1 because that's your computer and it will throw off any metrics you choose to calculate. \n\n\n\n\nWhat does it all mean?\n\n\nWell, that depends.\n\n\nFor an example of why you might want to do all this, and what you might find, see \n'Shouting Into the Void?'\n a piece where I tried to understand the shape of the archaeological web.", 
            "title": "Graphing the Net"
        }, 
        {
            "location": "/supporting materials/graphing-the-net.txt/#graphing-the-net", 
            "text": "It may be that you are interested in the structure of links on the internet. Perhaps you'd like to see how a particular topic plays out across Wikiepedia. What you'd have to do is a  web-crawl . Below are quick instructions for setting up a webcrawl, which will follow every link on a page to a particular depth, and echo the results through your browser into Gephi for visualization and analysis.", 
            "title": "Graphing the Net"
        }, 
        {
            "location": "/supporting materials/graphing-the-net.txt/#you-will-need", 
            "text": "The Chrome browser with  Site Spider Mk II  installed  Gephi  HTTP Graph Generator Plugin for Gephi installed   NB  You can install the graph generator plugin from  within  Gephi:   Select 'Tools' on the main menu ribbon at the top of the screen (and  not  the 'plugins' item).  Within 'Tools', select 'plugins' and then 'available plugins'  Search for 'HTTPGraph'  Tick off the box and install it.   When finished, close and restart Gephi with a new project.", 
            "title": "You will need:"
        }, 
        {
            "location": "/supporting materials/graphing-the-net.txt/#getting-set-up-to-scrape", 
            "text": "In Chrome, go to the settings page (the 'hamburg' icon at the extreme right of the address bar, or by typing  chrome://settings/  in the address bar.     Click on 'show advanced settings' at the bottom of the page.    Scroll down to 'Network' and click on 'change proxy settings'.     In the popup that opens, click the 'connections' tab, and then the 'LAN Settings' button.   Another  popup will open.     Select the 'Use a proxy server for your LAN'.     Enter  127.0.0.1  for the address, and  8088  for the port. Now, whenever you go to a website, the info will be echoed through that port. We need to set Gephi up to hear what's being passed.    Open Gephi and start a new project.     Go to 'file', then 'generate', and then 'http graph'. A pop up will open, asking you to specify a port.     Input  8088 . Accept the defaults, and press OK. On the overview panel, nodes will begin to appear when you go to a URL in Chrome.", 
            "title": "Getting set up to scrape"
        }, 
        {
            "location": "/supporting materials/graphing-the-net.txt/#begin-the-scrape", 
            "text": "Go back to Chrome.   Put in the URL that you want to start your scrape on, eg  http://en.wikipedia.org/wiki/Archaeology .  Click the SiteSpider II button in your toolbar. Site spider will open a popup asking you how far and how deep you want to scrape.   Set the SiteSpider II parameters accordingly.   Hit 'go' and then flip over to your Gephi window. You'll see the network begin to populate! Let it run as long as you want.   When you're finished, save your network by 'exporting' it (on the file menu) as any other format than .gephi. I say this because I find sometimes the .gephi format is unstable. You can then filter your network for resources or html pages or what have you. I suggest deleting the node labelled 127.0.0.1 because that's your computer and it will throw off any metrics you choose to calculate.", 
            "title": "Begin the scrape"
        }, 
        {
            "location": "/supporting materials/graphing-the-net.txt/#what-does-it-all-mean", 
            "text": "Well, that depends.  For an example of why you might want to do all this, and what you might find, see  'Shouting Into the Void?'  a piece where I tried to understand the shape of the archaeological web.", 
            "title": "What does it all mean?"
        }, 
        {
            "location": "/supporting materials/topicmodel-r-dhbox/", 
            "text": "Topic Modeling in R, DH Box version\n\n\nIn this exercise, we're going to grab the Colonial Newspaper Database from my GitHub page, do some exploratory visualizations, and then create a topic model whose output can then be visualized further in other platforms (including as a network in Gephi or other such packaged). At the appropriate point, I show you how to import a directory of texts rather than a single file of data, and to feed that into the script.\n\n\nGo to your DH Box, and click on RStudio. At the right hand side where it says 'project (none)', click and create a new project in a new empty directory. (If you want to put this directory under version control with git, so that you can push your work to your GitHub account, please read \nthe RStudio instructions\n.)\n\n\nIn the script panel (top left; click on the green plus side and select new R script if this pane isn't open) paste the following code and then run each line by putting the cursor in the line and hitting code \n run lines.\n\n\n    install.packages(\"mallet\")\n    library(\"mallet\")\n    install.packages(\"RCurl\")\n    library(\"RCurl\")\n\n\n\nIn the future, now that you've installed these packages you won't have to again, so you can comment them out by placing a \n\\#\n in front.\n\n\nHow to fix RCurl Package installation error\n\n\nWhen attempting to run \ninstall.packages(\"RCurl\")\n you may get an error along the lines of \n\n\nchecking for curl-config... no\nCannot find curl-config\nERROR: configuration failed for package \u2018RCurl\u2019\n\n\n\n\nTo fix this, navigate to the DH Box command line and type \n$ sudo apt-get install libcurl4-gnutls-dev\n. When the installation is successful, you can now install RCurl by running the last two lines of the script:\n\n\ninstall.packages(\nRCurl\n)\nlibrary(\nRCurl\n)\n\n\n\n\n\n\n\n\n \n\n\nImporting data directly from the web\n\n\nMelodee Beals has been using TEI to markup newspaper articles, creating the Colonial Newspapers Database (which she shared on GitHub). We then used GitHub Pages and an XLST stylesheet to convert that database into a table of comma-separated values, a copy of which is at https://raw.githubusercontent.com/shawngraham/exercise/gh-pages/CND.csv. We are now going to topic model the text of those newspaper articles, to see what patterns of discourse may lie within.\n\n\nNow we want to tell RStudio to grab our data from our GitHub page. The thing is, RStudio can easily grab materials from websites where the url is http; but when it is https (as it is with GitHub), things get a bit more fussy. So what we do is use a special package to grab the data, and then shove it into a variable that we can then tease apart for our analysis.\n\n\nx \n- getURL(\nhttps://raw.githubusercontent.com/shawngraham/exercise/gh-pages/CND.csv\n, .opts = list(ssl.verifypeer = FALSE))\n\n\n\n\nThat line reaches out to the webpage and grabs the information and puts it into a variable called \nx\n.\n\n\ndocuments \n- read.csv(text = x, col.names=c(\nArticle_ID\n, \nNewspaper Title\n, \nNewspaper City\n, \nNewspaper Province\n, \nNewspaper Country\n, \nYear\n, \nMonth\n, \nDay\n, \nArticle Type\n, \nText\n, \nKeywords\n), colClasses=rep(\ncharacter\n, 3), sep=\n,\n, quote=\n)\n\n\n\n\nNow we've created a variable called \ndocuments\n and the \nread.csv\n command read all of the data pulled into x, and tells R that \ndocuments\n has columns called \"Newspaper Title\" etc. When we only want information from a particular column, we modify the variable slightly, eg \ndocuments$Keywords\n would only look at the information in the keywords column. Let's go on a brief digression and actually do that, and see what we learn about this corpus:\n\n\ncounts \n- table(documents$Newspaper.City)\n\n\n\n\nWe tell R to make a new variable called 'counts', and fill it with the information from the column 'newspaper city' in 'documents'. It counts them up! Let's make a simple barplot:\n\n\nbarplot(counts, main=\nCities\n, xlab=\nNumber of Articles\n)\n\n\n\n\nThe plot will appear in the bottom right pane of RStudio. You can click on 'zoom' to see the plot in a popup window. You can also export it as a PNG or PDF file. Clearly, we\u2019re getting an Edinburgh/Glasgow perspective on things. And somewhere in our data, there\u2019s a mispelled \u2018Edinbugh\u2019. Do you see any other error(s) in the plot? How would you correct it(them)?\n\n\nLet's do the same thing for year, and count the number of articles per year in this corpus:\n\n\nyears \n- table(documents$Year)\nbarplot(years, main=\nPublication Year\n, xlab=\nYear\n, ylab=\nNumber of Articles\n)\n\n\n\n\nThere\u2019s a lot of material in 1789, another peak around 1819, againg in the late 1830s. We can ask ourselves now: is this an artefact of the data, or of our collection methods? This would be a question a reviewer would want answers to. Let\u2019s assume for now that these two plots are \u2018true\u2019 \n that, for whatever reasons, only Edinburgh and Glasgow were concerned with these colonial reports, and that they were particulary interested during those three periods. This is already an interesting question that we as historians would want to explore. Try making some more visualizations like this of other aspects of the data. What other patterns do you see that are worth investigating?\n\n\nNow, let's return to getting our data ready to create a topic model. In the line below, note that there is a file called \nen.txt\n that it wants to load up. You need to create this file; it's a list of stopwords, or common words that we think do not add value to our model (words like 'the', 'and', 'of' and so on.) The decision of which words to exclude from our analysis is of course a theoretical position...\n\n\nTo create that file, click on the 'new file' icon in the tool ribbon and select new text file. This will open a blank file in the edit window here. Copy and paste the list of words at \nen.txt\n into that blank file and save it as \nen.txt\n. This file was put together by Matt Jockers.\n\n\nmallet.instances \n- mallet.import(documents$Article_ID, documents$Text, \nen.txt\n, token.regexp = \n\\\\p{L}[\\\\p{L}\\\\p{P}]+\\\\p{L}\n)\n\n\n\n\nThat line above passes the article ID and the text of our newspaper articles to the Mallet routine.  The stopwords list is generic; it might need to be curated to take into account the pecularities of your data. You might want to create your own, one for each project given the particulars of your project. Note that Jockers compiled hist stoplist for his research in literary history of the 19th century. Your mileage may vary! Finally, the last bit after \u2018token.regexp\u2019 applies a regular expression against our newspaper articles, cleaning them up.\n\n\n\n\n\n\n\n\nReading data from a directory\n\n\nThis is an alternative way of ingesting documents for topic modeling. Earlier, you learned how to use wget and some other scripts to download full text documents from Canadiana.org as well as from the Library and Archives Canada (the Canadian war diary). The code below loads those documents into Mallet, after which you can proceed to build a topic model. In the command line, cd into your folder that has your downloaded materials. At the command line, cd into your folder, and type $ pwd to get the full path. Copy it, go back to RStudio, and paste it into the line below between the \" marks.\n\n\ndocuments \n- mallet.read.dir(\n/home/shawngraham/war-diary-text\n)\n\n# your path will probably look like /home/your-account-in-dhbox/your-folder-of-materials\n\nmallet.instances \n- mallet.import(documents$id, documents$text, \nen.txt\n, token.regexp = \n\\\\p{L}[\\\\p{L}\\\\p{P}]+\\\\p{L}\n)\n\n\n\n\nNB Do either one or the other, but not both: read from a file, where each row contains the complete text of the document, or read from a folder where each file contains the complete text of the document.\n\n\n\n\n\n\n\n\nBuilding the topic model\n\n\nThe 'correct' number of topics is going to require trial-and-error. You don't want too few, because that would hide the complexity of your data; too many, and you're starting to split hair.\n\n\n#set the number of desired topics\nnum.topics \n- 20\ntopic.model \n- MalletLDA(num.topics)\n\n\n\n\nNow we\u2019ve told Mallet how many topics to search for; this is a number you\u2019d want to fiddle with, to find the \u2018best\u2019 number of topics. The next line creates a variable \u2018topic.model\u2019 which will eventually be filled by Mallet using the LDA approach, for 20 topics. Let\u2019s get some info on our topic model, on our distribution of words in these materials.\n\n\ntopic.model$loadDocuments(mallet.instances)\n## Get the vocabulary, and some statistics about word frequencies.\n## These may be useful in further curating the stopword list.\nvocabulary \n- topic.model$getVocabulary()\nword.freqs \n- mallet.word.freqs(topic.model)\nhead(word.freqs)\n\n\n\n\nIt is handy to write our output to our project folder periodically; that way you can bring it into other programs if you want to visualize it or explore it further, or if you simply want to have a record of what was going on at a particular point. The line below uses the \nwrite.csv\n command, where the first element is the variable and the second the filename.\n\n\nwrite.csv(word.freqs, \nword-freqs.csv\n )\n\n\n\n\nWord frequencies are handy to look at because it will tell you if you've got words that are 'drowning out' the others. Some of these words you might want to consider adding to your stop words list (and thus, going back to where we first created the mallet.instances and restarting the analysis). By the way, do you see how you might create a bar plot of word frequencies?\n\n\nNow we hit the heavy lifting: generating a topic model. Some of the comments below are very technical; just make sure to run each line of code! (Original code here came from Ben Marwick's analysis of the \nDay of Archaeology\n.)\n\n\n## Optimize hyperparameters every 20 iterations,\n## after 50 burn-in iterations.\ntopic.model$setAlphaOptimization(20, 50)\n## Now train a model. Note that hyperparameter optimization is on, by default.\n## We can specify the number of iterations. Here we'll use a large-ish round number.\n## When you run the next line, a *lot* of information will scroll through your console.\n## Just be patient and wait til it hits that 1000 iteration.\ntopic.model$train(1000)\n## Run through a few iterations where we pick the best topic for each token,\n## rather than sampling from the posterior distribution.\ntopic.model$maximize(10)\n## Get the probability of topics in documents and the probability of words in topics.\n## By default, these functions return raw word counts. Here we want probabilities,\n## so we normalize, and add \nsmoothing\n so that nothing has exactly 0 probability.\ndoc.topics \n- mallet.doc.topics(topic.model, smoothed=T, normalized=T)\ntopic.words \n- mallet.topic.words(topic.model, smoothed=T, normalized=T)\n\n\n\n\n\n\n\n\n\n\nCongratulations! You now have a topic model. Let\u2019s look at some of our topics. What are the top words in topic 7? Notice that R indexes from 1, so this will be the topic that mallet called topic 6:\n\n\nmallet.top.words(topic.model, topic.words[7,])\n\n\n\n\nNow we\u2019ll write the distribution of the topics by document (ie. newspaper article) to a CSV file that we could explore/visualize with other tools. Then, we\u2019ll take a look at the key words describing each topic.\n\n\ntopic.docs \n- t(doc.topics)\ntopic.docs \n- topic.docs / rowSums(topic.docs)\nwrite.csv(topic.docs, \ntopics-docs.csv\n )\n# that file enables you to see what topics are most present in what issues/documents\n\n## Get a vector containing short names for the topics\ntopics.labels \n- rep(\n, num.topics)\nfor (topic in 1:num.topics) topics.labels[topic] \n- paste(mallet.top.words(topic.model, topic.words[topic,], num.top.words=5)$words, collapse=\n \n)\n# have a look at keywords for each topic\ntopics.labels\n\nwrite.csv(topics.labels, \ntopics-labels.csv\n)\n\n\n\n\nSome interesting patterns suggest themselves already! But a list of words doesn\u2019t capture the relative importance of particular words in particular topics. A word might appear in more than one topic, for instance, but really dominate one rather than the other. When you examine the CSV files, you'll notice that each document is given a series of percentages; these add up to 1, and are indicating the percentage which the different topics contribute to the overall composition of that document. Look for the largest numbers to get a sense of what's going on. We could ask R to cluster similarly composed documents together though...\n\n\n\n\n\n\n\n\nA simple histogram\n\n\nplot(hclust(dist(topic.words)), labels=topics.labels)\n\n\n\n\nDo you see any interesting clusters? Topics that end up in the same clusters we interpret as being related in some fashion. The plot is a bit crowded; in RStudio you can open it in a new window by clickining 'zoom' to see the dendrogram more clearly. You can also google 'hclust cran-r' to find tutorials to make a better plot. One thing we can do is to plot it again without labels, to see the structure a bit better:\n\n\nplot(hclust(dist(topic.words)))\n\n\n\n\n\n\n\n\n\n\nNow, if we want to get really fancy, we can make a network visualization of how topics interlink due to their distribution in documents. The next bit of code does that, and saves in .graphml format, which packages like Gephi http://gephi.org can read.\n\n\ntopic_docs \n- data.frame(topic.docs)\nnames(topic_docs) \n- documents$article_id\n\ninstall.packages(\ncluster\n)\nlibrary(cluster)\ntopic_df_dist \n- as.matrix(daisy(t(topic_docs), metric = \neuclidean\n, stand = TRUE))\n# Change row values to zero if less than row minimum plus row standard deviation\n# keep only closely related documents and avoid a dense spagetti diagram\n# that's difficult to interpret (hat-tip: http://stackoverflow.com/a/16047196/1036500)\ntopic_df_dist[ sweep(topic_df_dist, 1, (apply(topic_df_dist,1,min) + apply(topic_df_dist,1,sd) )) \n 0 ] \n- 0\n\n\n\n\n\n\n\n\n\n\ninstall.packages(\nigraph\n)\n# the line above would normally install igraph. However, the latest version is not compatible\n# with this version of R. Thus, go to command line in DH Box, cd to the R folder, cd x86_64-pc-linux-gnu-library, cd 3.0 folder.\n# wget the older version of igraph that'll work: https://cran.r-project.org/src/contrib/Archive/igraph/igraph_0.6-3.tar.gz\n# then, at command line, run the following R command: $ R CMD INSTALL igraph_0.6-3.tar.gz\n# this'll install igraph. Indeed, for any package you can look for older versions of it by slotting in\n# the name of the package in the url above and browsing the archive.\n# Remember, we're working with R version 3.03, from 2013, so we need stuff earlier than that.\n\n# once installed, call it:\nlibrary(igraph)\n\n# we transform the information from the previous code block into a network\ng \n- as.undirected(graph.adjacency(topic_df_dist))\n\n# then we specify the layout and the number of iterations to make it pretty\nlayout1 \n- layout.fruchterman.reingold(g, niter=100)\n\n#then we plot it out\nplot(g, layout=layout1, edge.curved = TRUE, vertex.size = 1, vertex.color= \ngrey\n, edge.arrow.size = 0, vertex.label.dist=0.5, vertex.label = NA)\n\n\n\n\nWhen you look at this network, you can see clusters of documents by virtue of largely shared topics. We export this data in a text format called 'graphml', which can be opened by any text editor, and visualized in nearly any network analysis program for further refinement and analysis. It might be interesting to explore why some issues are so topically focussed, for instance.\n\n\nwrite.graph(g, file=\ncnd.graphml\n, format=\ngraphml\n)\n\n\n\n\n\n\n\n\n\n\nThere are many ways of visualizing and transforming our data. This document only captures a small fraction of the kinds of things you could do. Another good exploration is at \nMatthew Jockers' website\n or the \nglobal stopwords lists\n. \nBen Marwick does really fun things with the Day of Archaeology blog posts\n and indeed, some of the code above comes from Marwick\u2019s explorations. Keep your R scripts in your open notebook, and somebody might come along and use them, cite them, improve them, share them! Keep also all your data. Visit \nmy own work for an example\n.", 
            "title": "Topic Modeling, DH Box"
        }, 
        {
            "location": "/supporting materials/topicmodel-r-dhbox/#topic-modeling-in-r-dh-box-version", 
            "text": "In this exercise, we're going to grab the Colonial Newspaper Database from my GitHub page, do some exploratory visualizations, and then create a topic model whose output can then be visualized further in other platforms (including as a network in Gephi or other such packaged). At the appropriate point, I show you how to import a directory of texts rather than a single file of data, and to feed that into the script.  Go to your DH Box, and click on RStudio. At the right hand side where it says 'project (none)', click and create a new project in a new empty directory. (If you want to put this directory under version control with git, so that you can push your work to your GitHub account, please read  the RStudio instructions .)  In the script panel (top left; click on the green plus side and select new R script if this pane isn't open) paste the following code and then run each line by putting the cursor in the line and hitting code   run lines.      install.packages(\"mallet\")\n    library(\"mallet\")\n    install.packages(\"RCurl\")\n    library(\"RCurl\")  In the future, now that you've installed these packages you won't have to again, so you can comment them out by placing a  \\#  in front.", 
            "title": "Topic Modeling in R, DH Box version"
        }, 
        {
            "location": "/supporting materials/topicmodel-r-dhbox/#how-to-fix-rcurl-package-installation-error", 
            "text": "When attempting to run  install.packages(\"RCurl\")  you may get an error along the lines of   checking for curl-config... no\nCannot find curl-config\nERROR: configuration failed for package \u2018RCurl\u2019  To fix this, navigate to the DH Box command line and type  $ sudo apt-get install libcurl4-gnutls-dev . When the installation is successful, you can now install RCurl by running the last two lines of the script:  install.packages( RCurl )\nlibrary( RCurl )", 
            "title": "How to fix RCurl Package installation error"
        }, 
        {
            "location": "/supporting materials/topicmodel-r-dhbox/#importing-data-directly-from-the-web", 
            "text": "Melodee Beals has been using TEI to markup newspaper articles, creating the Colonial Newspapers Database (which she shared on GitHub). We then used GitHub Pages and an XLST stylesheet to convert that database into a table of comma-separated values, a copy of which is at https://raw.githubusercontent.com/shawngraham/exercise/gh-pages/CND.csv. We are now going to topic model the text of those newspaper articles, to see what patterns of discourse may lie within.  Now we want to tell RStudio to grab our data from our GitHub page. The thing is, RStudio can easily grab materials from websites where the url is http; but when it is https (as it is with GitHub), things get a bit more fussy. So what we do is use a special package to grab the data, and then shove it into a variable that we can then tease apart for our analysis.  x  - getURL( https://raw.githubusercontent.com/shawngraham/exercise/gh-pages/CND.csv , .opts = list(ssl.verifypeer = FALSE))  That line reaches out to the webpage and grabs the information and puts it into a variable called  x .  documents  - read.csv(text = x, col.names=c( Article_ID ,  Newspaper Title ,  Newspaper City ,  Newspaper Province ,  Newspaper Country ,  Year ,  Month ,  Day ,  Article Type ,  Text ,  Keywords ), colClasses=rep( character , 3), sep= , , quote= )  Now we've created a variable called  documents  and the  read.csv  command read all of the data pulled into x, and tells R that  documents  has columns called \"Newspaper Title\" etc. When we only want information from a particular column, we modify the variable slightly, eg  documents$Keywords  would only look at the information in the keywords column. Let's go on a brief digression and actually do that, and see what we learn about this corpus:  counts  - table(documents$Newspaper.City)  We tell R to make a new variable called 'counts', and fill it with the information from the column 'newspaper city' in 'documents'. It counts them up! Let's make a simple barplot:  barplot(counts, main= Cities , xlab= Number of Articles )  The plot will appear in the bottom right pane of RStudio. You can click on 'zoom' to see the plot in a popup window. You can also export it as a PNG or PDF file. Clearly, we\u2019re getting an Edinburgh/Glasgow perspective on things. And somewhere in our data, there\u2019s a mispelled \u2018Edinbugh\u2019. Do you see any other error(s) in the plot? How would you correct it(them)?  Let's do the same thing for year, and count the number of articles per year in this corpus:  years  - table(documents$Year)\nbarplot(years, main= Publication Year , xlab= Year , ylab= Number of Articles )  There\u2019s a lot of material in 1789, another peak around 1819, againg in the late 1830s. We can ask ourselves now: is this an artefact of the data, or of our collection methods? This would be a question a reviewer would want answers to. Let\u2019s assume for now that these two plots are \u2018true\u2019   that, for whatever reasons, only Edinburgh and Glasgow were concerned with these colonial reports, and that they were particulary interested during those three periods. This is already an interesting question that we as historians would want to explore. Try making some more visualizations like this of other aspects of the data. What other patterns do you see that are worth investigating?  Now, let's return to getting our data ready to create a topic model. In the line below, note that there is a file called  en.txt  that it wants to load up. You need to create this file; it's a list of stopwords, or common words that we think do not add value to our model (words like 'the', 'and', 'of' and so on.) The decision of which words to exclude from our analysis is of course a theoretical position...  To create that file, click on the 'new file' icon in the tool ribbon and select new text file. This will open a blank file in the edit window here. Copy and paste the list of words at  en.txt  into that blank file and save it as  en.txt . This file was put together by Matt Jockers.  mallet.instances  - mallet.import(documents$Article_ID, documents$Text,  en.txt , token.regexp =  \\\\p{L}[\\\\p{L}\\\\p{P}]+\\\\p{L} )  That line above passes the article ID and the text of our newspaper articles to the Mallet routine.  The stopwords list is generic; it might need to be curated to take into account the pecularities of your data. You might want to create your own, one for each project given the particulars of your project. Note that Jockers compiled hist stoplist for his research in literary history of the 19th century. Your mileage may vary! Finally, the last bit after \u2018token.regexp\u2019 applies a regular expression against our newspaper articles, cleaning them up.", 
            "title": "Importing data directly from the web"
        }, 
        {
            "location": "/supporting materials/topicmodel-r-dhbox/#reading-data-from-a-directory", 
            "text": "This is an alternative way of ingesting documents for topic modeling. Earlier, you learned how to use wget and some other scripts to download full text documents from Canadiana.org as well as from the Library and Archives Canada (the Canadian war diary). The code below loads those documents into Mallet, after which you can proceed to build a topic model. In the command line, cd into your folder that has your downloaded materials. At the command line, cd into your folder, and type $ pwd to get the full path. Copy it, go back to RStudio, and paste it into the line below between the \" marks.  documents  - mallet.read.dir( /home/shawngraham/war-diary-text )\n\n# your path will probably look like /home/your-account-in-dhbox/your-folder-of-materials\n\nmallet.instances  - mallet.import(documents$id, documents$text,  en.txt , token.regexp =  \\\\p{L}[\\\\p{L}\\\\p{P}]+\\\\p{L} )  NB Do either one or the other, but not both: read from a file, where each row contains the complete text of the document, or read from a folder where each file contains the complete text of the document.", 
            "title": "Reading data from a directory"
        }, 
        {
            "location": "/supporting materials/topicmodel-r-dhbox/#building-the-topic-model", 
            "text": "The 'correct' number of topics is going to require trial-and-error. You don't want too few, because that would hide the complexity of your data; too many, and you're starting to split hair.  #set the number of desired topics\nnum.topics  - 20\ntopic.model  - MalletLDA(num.topics)  Now we\u2019ve told Mallet how many topics to search for; this is a number you\u2019d want to fiddle with, to find the \u2018best\u2019 number of topics. The next line creates a variable \u2018topic.model\u2019 which will eventually be filled by Mallet using the LDA approach, for 20 topics. Let\u2019s get some info on our topic model, on our distribution of words in these materials.  topic.model$loadDocuments(mallet.instances)\n## Get the vocabulary, and some statistics about word frequencies.\n## These may be useful in further curating the stopword list.\nvocabulary  - topic.model$getVocabulary()\nword.freqs  - mallet.word.freqs(topic.model)\nhead(word.freqs)  It is handy to write our output to our project folder periodically; that way you can bring it into other programs if you want to visualize it or explore it further, or if you simply want to have a record of what was going on at a particular point. The line below uses the  write.csv  command, where the first element is the variable and the second the filename.  write.csv(word.freqs,  word-freqs.csv  )  Word frequencies are handy to look at because it will tell you if you've got words that are 'drowning out' the others. Some of these words you might want to consider adding to your stop words list (and thus, going back to where we first created the mallet.instances and restarting the analysis). By the way, do you see how you might create a bar plot of word frequencies?  Now we hit the heavy lifting: generating a topic model. Some of the comments below are very technical; just make sure to run each line of code! (Original code here came from Ben Marwick's analysis of the  Day of Archaeology .)  ## Optimize hyperparameters every 20 iterations,\n## after 50 burn-in iterations.\ntopic.model$setAlphaOptimization(20, 50)\n## Now train a model. Note that hyperparameter optimization is on, by default.\n## We can specify the number of iterations. Here we'll use a large-ish round number.\n## When you run the next line, a *lot* of information will scroll through your console.\n## Just be patient and wait til it hits that 1000 iteration.\ntopic.model$train(1000)\n## Run through a few iterations where we pick the best topic for each token,\n## rather than sampling from the posterior distribution.\ntopic.model$maximize(10)\n## Get the probability of topics in documents and the probability of words in topics.\n## By default, these functions return raw word counts. Here we want probabilities,\n## so we normalize, and add  smoothing  so that nothing has exactly 0 probability.\ndoc.topics  - mallet.doc.topics(topic.model, smoothed=T, normalized=T)\ntopic.words  - mallet.topic.words(topic.model, smoothed=T, normalized=T)     Congratulations! You now have a topic model. Let\u2019s look at some of our topics. What are the top words in topic 7? Notice that R indexes from 1, so this will be the topic that mallet called topic 6:  mallet.top.words(topic.model, topic.words[7,])  Now we\u2019ll write the distribution of the topics by document (ie. newspaper article) to a CSV file that we could explore/visualize with other tools. Then, we\u2019ll take a look at the key words describing each topic.  topic.docs  - t(doc.topics)\ntopic.docs  - topic.docs / rowSums(topic.docs)\nwrite.csv(topic.docs,  topics-docs.csv  )\n# that file enables you to see what topics are most present in what issues/documents\n\n## Get a vector containing short names for the topics\ntopics.labels  - rep( , num.topics)\nfor (topic in 1:num.topics) topics.labels[topic]  - paste(mallet.top.words(topic.model, topic.words[topic,], num.top.words=5)$words, collapse=   )\n# have a look at keywords for each topic\ntopics.labels\n\nwrite.csv(topics.labels,  topics-labels.csv )  Some interesting patterns suggest themselves already! But a list of words doesn\u2019t capture the relative importance of particular words in particular topics. A word might appear in more than one topic, for instance, but really dominate one rather than the other. When you examine the CSV files, you'll notice that each document is given a series of percentages; these add up to 1, and are indicating the percentage which the different topics contribute to the overall composition of that document. Look for the largest numbers to get a sense of what's going on. We could ask R to cluster similarly composed documents together though...", 
            "title": "Building the topic model"
        }, 
        {
            "location": "/supporting materials/topicmodel-r-dhbox/#a-simple-histogram", 
            "text": "plot(hclust(dist(topic.words)), labels=topics.labels)  Do you see any interesting clusters? Topics that end up in the same clusters we interpret as being related in some fashion. The plot is a bit crowded; in RStudio you can open it in a new window by clickining 'zoom' to see the dendrogram more clearly. You can also google 'hclust cran-r' to find tutorials to make a better plot. One thing we can do is to plot it again without labels, to see the structure a bit better:  plot(hclust(dist(topic.words)))     Now, if we want to get really fancy, we can make a network visualization of how topics interlink due to their distribution in documents. The next bit of code does that, and saves in .graphml format, which packages like Gephi http://gephi.org can read.  topic_docs  - data.frame(topic.docs)\nnames(topic_docs)  - documents$article_id\n\ninstall.packages( cluster )\nlibrary(cluster)\ntopic_df_dist  - as.matrix(daisy(t(topic_docs), metric =  euclidean , stand = TRUE))\n# Change row values to zero if less than row minimum plus row standard deviation\n# keep only closely related documents and avoid a dense spagetti diagram\n# that's difficult to interpret (hat-tip: http://stackoverflow.com/a/16047196/1036500)\ntopic_df_dist[ sweep(topic_df_dist, 1, (apply(topic_df_dist,1,min) + apply(topic_df_dist,1,sd) ))   0 ]  - 0     install.packages( igraph )\n# the line above would normally install igraph. However, the latest version is not compatible\n# with this version of R. Thus, go to command line in DH Box, cd to the R folder, cd x86_64-pc-linux-gnu-library, cd 3.0 folder.\n# wget the older version of igraph that'll work: https://cran.r-project.org/src/contrib/Archive/igraph/igraph_0.6-3.tar.gz\n# then, at command line, run the following R command: $ R CMD INSTALL igraph_0.6-3.tar.gz\n# this'll install igraph. Indeed, for any package you can look for older versions of it by slotting in\n# the name of the package in the url above and browsing the archive.\n# Remember, we're working with R version 3.03, from 2013, so we need stuff earlier than that.\n\n# once installed, call it:\nlibrary(igraph)\n\n# we transform the information from the previous code block into a network\ng  - as.undirected(graph.adjacency(topic_df_dist))\n\n# then we specify the layout and the number of iterations to make it pretty\nlayout1  - layout.fruchterman.reingold(g, niter=100)\n\n#then we plot it out\nplot(g, layout=layout1, edge.curved = TRUE, vertex.size = 1, vertex.color=  grey , edge.arrow.size = 0, vertex.label.dist=0.5, vertex.label = NA)  When you look at this network, you can see clusters of documents by virtue of largely shared topics. We export this data in a text format called 'graphml', which can be opened by any text editor, and visualized in nearly any network analysis program for further refinement and analysis. It might be interesting to explore why some issues are so topically focussed, for instance.  write.graph(g, file= cnd.graphml , format= graphml )     There are many ways of visualizing and transforming our data. This document only captures a small fraction of the kinds of things you could do. Another good exploration is at  Matthew Jockers' website  or the  global stopwords lists .  Ben Marwick does really fun things with the Day of Archaeology blog posts  and indeed, some of the code above comes from Marwick\u2019s explorations. Keep your R scripts in your open notebook, and somebody might come along and use them, cite them, improve them, share them! Keep also all your data. Visit  my own work for an example .", 
            "title": "A simple histogram"
        }, 
        {
            "location": "/supporting materials/topicmodel-r-yourmachine/", 
            "text": "Topic Modeling in R On Your Own machine\n\n\nIn this exercise, we're going to grab an archived copy of Melodee Beals' Colonial Newspaper Database from my GitHub page, do some exploratory visualizations, and then create a topic model whose output can then be visualized further in other platforms (including as a network in Gephi). The walkthrough \ncan be found on the HIST3907b website\n. Each gray block is something to copy-and-paste into your script window in RStudio. Then, put the cursor at the start of the first line, and hit ctrl+enter to get RStudio to execute each line. In the walkthrough, when you get to another gray block, just copy and paste it into your script window after the earlier block. Work your way through the walkthrough. The walkthrough gives you an indication of what the output should look like as you move through it. (The walkthrough was written inside R, and then turned into HTML using an R package called 'Knitr'. You can see that this has implications for open research! For reference, \nvisit the original Rmd (R markdown) file that generated the walkthrough\n.)\n\n\nWhen you start RStudio the first time for this exercise\n make sure to create a new project in a new directory.\n\n\nBy the way: when you run this line: \ntopic.model$train(1000)\n your console will fill up with data as it iterates 1000 times over the entire corpus, fitting a topic model to it. This is as it should be!\n\n\nIn this way, you'll build up an entire script for topic modeling materials you find on the web. You can then save your script and upload it to your open notebook. In the future, you'd be able to make just a few changes here and there in order to grab and explore different data.\n\n\nMake a note in your open notebook about your process and your observations.\n\n\nGoing further\n If you wanted to use that script on the materials you collected in module 2, you would have to tell R to load up those materials from a directory, rather than by reading a CSV file. Take a look at \nmy script for topic modeling the Ferguson Grand Jury documents\n, especially this line:\n\n\ndocuments \n- mallet.read.dir(\"originaldocs/1000chunks/\")\n\n\nYou feed it the path to your documents. If you are on a windows machine, the path would look a bit different, for instance:\n\n\n\"C:\\\\research\\\\originaldocs\\\\1000chunks\\\\\"", 
            "title": "Topic Modeling, Local"
        }, 
        {
            "location": "/supporting materials/topicmodel-r-yourmachine/#topic-modeling-in-r-on-your-own-machine", 
            "text": "In this exercise, we're going to grab an archived copy of Melodee Beals' Colonial Newspaper Database from my GitHub page, do some exploratory visualizations, and then create a topic model whose output can then be visualized further in other platforms (including as a network in Gephi). The walkthrough  can be found on the HIST3907b website . Each gray block is something to copy-and-paste into your script window in RStudio. Then, put the cursor at the start of the first line, and hit ctrl+enter to get RStudio to execute each line. In the walkthrough, when you get to another gray block, just copy and paste it into your script window after the earlier block. Work your way through the walkthrough. The walkthrough gives you an indication of what the output should look like as you move through it. (The walkthrough was written inside R, and then turned into HTML using an R package called 'Knitr'. You can see that this has implications for open research! For reference,  visit the original Rmd (R markdown) file that generated the walkthrough .)  When you start RStudio the first time for this exercise  make sure to create a new project in a new directory.  By the way: when you run this line:  topic.model$train(1000)  your console will fill up with data as it iterates 1000 times over the entire corpus, fitting a topic model to it. This is as it should be!  In this way, you'll build up an entire script for topic modeling materials you find on the web. You can then save your script and upload it to your open notebook. In the future, you'd be able to make just a few changes here and there in order to grab and explore different data.  Make a note in your open notebook about your process and your observations.  Going further  If you wanted to use that script on the materials you collected in module 2, you would have to tell R to load up those materials from a directory, rather than by reading a CSV file. Take a look at  my script for topic modeling the Ferguson Grand Jury documents , especially this line:  documents  - mallet.read.dir(\"originaldocs/1000chunks/\")  You feed it the path to your documents. If you are on a windows machine, the path would look a bit different, for instance:  \"C:\\\\research\\\\originaldocs\\\\1000chunks\\\\\"", 
            "title": "Topic Modeling in R On Your Own machine"
        }, 
        {
            "location": "/supporting materials/netviz/", 
            "text": "Using igraph to visualize network data\n\n\nIn this exercise, we are doing a quick first pass on the network data generated from the Republic of Texas correspondence. I am providing you with the \nedge list\n, the links of to \n from that we generated earlier. I am also providing you the \nnode list\n, the list of individuals extracted from that edge list. If we keep our nodes and edges separated in CSV tables, we can add other \nattributes\n later (things like date, or number of times a pair of individuals corresponded ie weight, or gender, or location, or what have you) to create different views or analyses. I used Open Refine to clean this data up for you (recall our \nlesson on open refine\n). Remember, 80 percent of all digital history work involves cleaning data!\n\n\nBelow is the data that you need; download both files and use the filemanager to load them into your DH Box into your new R project for this tutorial.\n\n\n\n\nList of links (this downloads a CSV file)\n\n\nList of nodes(this downloads a CSV file)\n\n\n\n\nThis tutorial will illustrate to you some of the ways the \nigraph\n package can be used to create quick visualizations or to generate network statistics. These of course on their own mean very little: this is where your historical sensibility comes into play, triangulating this generated data with other things you know about the historical context of the time, place and players!\n\n\nRemember to make a new project in RStudio by clicking the down arrow on the R button at the right hand side of the RStudio interface; start the project in a new folder. Then using the DH Box filemanager, upload the nodes and links data from your computer to that folder you just created.\n\n\nInstalling igraph\n\n\nOnce that's accomplished, go back into RStudio and start a new script:\n\n\n# install igraph; this might take a long time\n# you only run this line the first time you install igraph:\ninstall.packages('igraph')\n\n# a lot of stuff gets downloaded and installed.\n\n# now tell RStudio you want to use the igraph pacakge and its functions:\nlibrary('igraph')\n\n\n\n\n\n\n\n\n\n\nGetting the data into your project\n\n\nFor future reference, we're adapting our script from a more in-depth \ntutorial of R igraph\n.\n\n\n\n\n\n\nBringing data into R is straightforward. We create a variable 'nodes' and a variable for 'links' and load the data from our CSV files into them:\n\n\n# now let's load up the data by putting the csv files into nodes and links.\nnodes \n- read.csv(\"texasnodes.csv\", header=T, as.is=T)\nlinks \n- read.csv(\"texaslinks.csv\", header=T, as.is=T)\n\n\n\n\n\n\n\nWe can examine the start or end of a variable with the head or tail command; these look at the first few lines at the start ('head') or end ('tail') of the variable, eg:\n\n\n#examine data\nhead(nodes)\nhead(links)\n\nnrow(nodes); length(unique(nodes$id))\n# which gives the number of nodes in our data\n\nnrow(links); nrow(unique(links[,c(\"source\", \"target\")]))\n# which gives the number of sources, and number of targets\n# which means some people sent more than one letter, and some people received more than one letter\n\n\n\n\n\n\n\nLet's rearrange things so that instead of this:\n\n\nAlice -\n Bob, 1 letter\nAlice -\n Bob, 1 letter\nAlice -\n Bob, 1 letter\n\n\n\nwe have this:\n\n\nAlice -\n Bob, 3 letters\n\n\n\nThat is, we're going to count up the number of times a particular relationship exists, and assign that count to the weight column. Much tidier all around! \n\n\n\n\n\n\nWe do that like this:\n\n\nlinks \n- aggregate(links[,3], links[,-3], sum)\n\nlinks \n- links[order(links$target, links$source),]\n\ncolnames(links)[3] \n- \"weight\"\n\nrownames(links) \n- NULL\n\nhead(links)\n\n\n\nYou should see this:\n\n\n head(links)\n           source            target weight\n1      James Webb       Abb6 Anduz6      1\n2   A. de Saligny Abner S. Lipscomb      1\n3     E. W. Moore Abner S. Lipscomb      1\n4  James Hamilton Abner S. Lipscomb     14\n5     James Treat Abner S. Lipscomb     11\n6 Nathaniel Amory Abner S. Lipscomb      1\n\n\n\n\n\n\n\nNow, let's tell R to stitch our links together into a network object (a data frame) that it can visualize and analyze:\n\n\n# let's make a net\n# notice that we are telling igraph that the network is directed, that the relationship Alice to Bob is different than Bob's to Alice (Alice is the _sender_, and Bob is the _receiver_)\n\n# In older DH Box version of igraph in RStudio: \nnet \n- graph.data.frame(d=links, vertices=nodes, directed=T)\n\n# OR Newer version of igraph in desktop RStudio:\nnet \n- graph_from_data_frame(d=links, vertices=nodes, directed=T)\n\n# type 'net' again and run the line to see how the network is represented.\nnet\n\n# let's visualizae it\nplot(net, edge.arrow.size=.4,vertex.label=NA)\n\n# two quite distinct groupings, it would appear.\n\n\n\n\n\n\n\n\n\nBefore we jump down the rabbit hole of visualization, let's recognize right now that \nvisualizing\n a network is only rarely of analytical value. The value of network analysis comes from the various questions we can now start posing of our data when it is represented as a network. In this correspondence network, who is in the centre of the web? To whom would information flow? To whom would information leak? Are there cliques or ingroups? When we identify such individuals, how does that confirm or confound our expectations of the period and place?\n\n\nMany different kinds of metrics can be calculated (and \nthe Kateto R igraph tutorial will show you how\n) but it's always worth remembering that a metric is only meaningful for a given network when we're dealing with like things \n a network of people who write letters to one another; a network of banks that swap mortgages with one another. These are called 'one mode networks'. A network of people connected to the banks they use \n a two mode network, because it connects two different kinds of things \n might be useful to \nvisualize\n but the metrics calculated might not be \nvalid\n if the metric was designed to work on a one-mode network (for more on this and allied issues, visit \nScott Weingart's website\n).\n\n\n\n\n\n\nGiven our correspondence network, let's imagine that 'closeness' (a measure of how central a person is) and 'betweenness' (a measure of how many different strands of the network pass through this person) are the most historically interesting. Further, we're going to try to determine if there are subgroups in our network, cliques.\n\n\n## the 'degree' of a node is the count of its connections. In this code chunk, we calculate degree, then make both a histogram of the counts and a plot of the network where we size the nodes proportionately to their degree. What do we learn from these two visualizations?\ndeg \n- degree(net, mode=\"all\")\nhist(deg, breaks=1:vcount(net)-1, main=\"Histogram of node degree\")\nplot(net, vertex.size=deg*2, vertex.label = NA)\n## write this info to file for safekeeping\nwrite.csv(deg, 'degree.csv')\n\n\n\n\n\n\n\nNow we'll look at closeness. If you know the width or \ndiameter\n of your network (the maximum number of steps to get across it), then the node that is on average the shortest number of steps from all the others is the one that is most close. We calculate it like this:\n\n\nclosepeople \n- closeness(net, mode=\"all\", weights=NA)\nsort(closepeople, decreasing = T) # so that we see who is most close first\nwrite.csv(closepeople, 'closeness.csv') # so we have it on file.\n\n\n\n\n\n\n\nWe can ask which individuals are hubs, and which are authorities. In the lingo, 'hubs' are individuals with many outgoing links (they \nsent\n lots of letters) while 'authorities' are individuals who \nreceived\n lots of letters. In the code below,\n\n\n# In older DH Box version of igraph in RStudio: \nhs \n- hub.score(net, weights=NA)$vector\nas \n- authority.score(net, weights=NA)$vector\n\n# OR Newer version of igraph in desktop RStudio:\nhs \n- hub_score(net, weights=NA)$vector\nas \n- authority_score(net, weights=NA)$vector\n\npar(mfrow=c(1,2))\n\n# vertex.label.cex sets the size of the label; play with the sizes until you see something appealing.\nplot(net, vertex.size=hs*40, vertex.label.cex =.2, edge.arrow.size=.1, main=\"Hubs\")\nplot(net, vertex.size=as*20, vertex.label = NA, edge.arrow.size=.1, main=\"Authorities\")\n\n\n\nCan you work out what command to give to write the hub score or the authority scores to a file?\n\n\n\n\n\n\nLet's look for 'modules' within this network. Broadly speaking, these are clumps of nodes that have more or less the same pattern of ties between them, \nwithin\n the group, than without.\n\n\n# In older DH Box version of igraph in RStudio: \ncfg \n- fastgreedy.community(as.undirected(net))\n\n# OR Newer version of igraph in desktop RStudio:\ncfg \n- cluster_fast_greedy(as.undirected(net))\n\nlapply(cfg, function(x) write.table( data.frame(x), 'cfg.csv'  , append= T, sep=',' ))\n\n\n\nWe create a new variable called 'cfg' and get the 'cluster_fast_greedy' algorithm to perform its calculations. The next line writes the groups to a file, separating each group with an x. (If you tried 'write.csv' as before, you'll get an error message because the output of the algorithm gives a different kind of data type. R is fussy like this.) Examine that file \n what groups do you spot? What might these mean, if you went back to the content of the original letters?\n\n\n\n\n\n\n\n\n\n\n\n\nVisualization\n\n\n\n\n\n\nThe line below will plot out our network, colouring it by the communities discerned above:\n\n\nplot(cfg, net, vertex.size = 1, vertex.label.cex =.2, edge.arrow.size=.1, main=\"Communities\")\n\n\n\n\n\n\n\nYou can export the plot by clicking on the 'export' button in the plot panel, to PDF or to PNG. But this is a pretty ugly network. We need to apply a \nlayout\n to try to make it more visually understandable. There are many different layout options in igraph. We'll assign the layout we want to a variable, and then we'll give that variable to the plot command:\n\n\n# In older DH Box version of igraph in RStudio:\nl1 \n- layout.fruchterman.reingold(net)\n\n# OR Newer version of igraph in desktop RStudio:\nl1 \n- layout_with_fr(net)\n\nplot(cfg, net, layout=l1, vertex.size = 1, vertex.label.cex =.2, edge.arrow.size=.1, main=\"Communities\")\n\n\n\nThe 'layout_with_fr' is calling the 'Fruchterman-Reingold' algorithm, a kind of layout that imagines each node as a repulsive power, pushing against all the other nodes (it's part of a family of layouts called 'Forced Atlast'). That also means that if you ran the plot command a couple of times, the nodes might end up in different places each time \n they are jostling for relative position, and this positioning itself carries no meaning in and of itself (so if a node is at the top of the screen, or the centre of the screen, don't read that to mean anything about importance).\n\n\n\n\n\n\nGood luck! Remember to save your script, and upload the entire project folder to your GitHub repository.", 
            "title": "Network visualization with igraph"
        }, 
        {
            "location": "/supporting materials/netviz/#using-igraph-to-visualize-network-data", 
            "text": "In this exercise, we are doing a quick first pass on the network data generated from the Republic of Texas correspondence. I am providing you with the  edge list , the links of to   from that we generated earlier. I am also providing you the  node list , the list of individuals extracted from that edge list. If we keep our nodes and edges separated in CSV tables, we can add other  attributes  later (things like date, or number of times a pair of individuals corresponded ie weight, or gender, or location, or what have you) to create different views or analyses. I used Open Refine to clean this data up for you (recall our  lesson on open refine ). Remember, 80 percent of all digital history work involves cleaning data!  Below is the data that you need; download both files and use the filemanager to load them into your DH Box into your new R project for this tutorial.   List of links (this downloads a CSV file)  List of nodes(this downloads a CSV file)   This tutorial will illustrate to you some of the ways the  igraph  package can be used to create quick visualizations or to generate network statistics. These of course on their own mean very little: this is where your historical sensibility comes into play, triangulating this generated data with other things you know about the historical context of the time, place and players!  Remember to make a new project in RStudio by clicking the down arrow on the R button at the right hand side of the RStudio interface; start the project in a new folder. Then using the DH Box filemanager, upload the nodes and links data from your computer to that folder you just created.", 
            "title": "Using igraph to visualize network data"
        }, 
        {
            "location": "/supporting materials/netviz/#installing-igraph", 
            "text": "Once that's accomplished, go back into RStudio and start a new script:  # install igraph; this might take a long time\n# you only run this line the first time you install igraph:\ninstall.packages('igraph')\n\n# a lot of stuff gets downloaded and installed.\n\n# now tell RStudio you want to use the igraph pacakge and its functions:\nlibrary('igraph')", 
            "title": "Installing igraph"
        }, 
        {
            "location": "/supporting materials/netviz/#getting-the-data-into-your-project", 
            "text": "For future reference, we're adapting our script from a more in-depth  tutorial of R igraph .    Bringing data into R is straightforward. We create a variable 'nodes' and a variable for 'links' and load the data from our CSV files into them:  # now let's load up the data by putting the csv files into nodes and links.\nnodes  - read.csv(\"texasnodes.csv\", header=T, as.is=T)\nlinks  - read.csv(\"texaslinks.csv\", header=T, as.is=T)    We can examine the start or end of a variable with the head or tail command; these look at the first few lines at the start ('head') or end ('tail') of the variable, eg:  #examine data\nhead(nodes)\nhead(links)\n\nnrow(nodes); length(unique(nodes$id))\n# which gives the number of nodes in our data\n\nnrow(links); nrow(unique(links[,c(\"source\", \"target\")]))\n# which gives the number of sources, and number of targets\n# which means some people sent more than one letter, and some people received more than one letter    Let's rearrange things so that instead of this:  Alice -  Bob, 1 letter\nAlice -  Bob, 1 letter\nAlice -  Bob, 1 letter  we have this:  Alice -  Bob, 3 letters  That is, we're going to count up the number of times a particular relationship exists, and assign that count to the weight column. Much tidier all around!     We do that like this:  links  - aggregate(links[,3], links[,-3], sum)\n\nlinks  - links[order(links$target, links$source),]\n\ncolnames(links)[3]  - \"weight\"\n\nrownames(links)  - NULL\n\nhead(links)  You should see this:   head(links)\n           source            target weight\n1      James Webb       Abb6 Anduz6      1\n2   A. de Saligny Abner S. Lipscomb      1\n3     E. W. Moore Abner S. Lipscomb      1\n4  James Hamilton Abner S. Lipscomb     14\n5     James Treat Abner S. Lipscomb     11\n6 Nathaniel Amory Abner S. Lipscomb      1    Now, let's tell R to stitch our links together into a network object (a data frame) that it can visualize and analyze:  # let's make a net\n# notice that we are telling igraph that the network is directed, that the relationship Alice to Bob is different than Bob's to Alice (Alice is the _sender_, and Bob is the _receiver_)\n\n# In older DH Box version of igraph in RStudio: \nnet  - graph.data.frame(d=links, vertices=nodes, directed=T)\n\n# OR Newer version of igraph in desktop RStudio:\nnet  - graph_from_data_frame(d=links, vertices=nodes, directed=T)\n\n# type 'net' again and run the line to see how the network is represented.\nnet\n\n# let's visualizae it\nplot(net, edge.arrow.size=.4,vertex.label=NA)\n\n# two quite distinct groupings, it would appear.     Before we jump down the rabbit hole of visualization, let's recognize right now that  visualizing  a network is only rarely of analytical value. The value of network analysis comes from the various questions we can now start posing of our data when it is represented as a network. In this correspondence network, who is in the centre of the web? To whom would information flow? To whom would information leak? Are there cliques or ingroups? When we identify such individuals, how does that confirm or confound our expectations of the period and place?  Many different kinds of metrics can be calculated (and  the Kateto R igraph tutorial will show you how ) but it's always worth remembering that a metric is only meaningful for a given network when we're dealing with like things   a network of people who write letters to one another; a network of banks that swap mortgages with one another. These are called 'one mode networks'. A network of people connected to the banks they use   a two mode network, because it connects two different kinds of things   might be useful to  visualize  but the metrics calculated might not be  valid  if the metric was designed to work on a one-mode network (for more on this and allied issues, visit  Scott Weingart's website ).    Given our correspondence network, let's imagine that 'closeness' (a measure of how central a person is) and 'betweenness' (a measure of how many different strands of the network pass through this person) are the most historically interesting. Further, we're going to try to determine if there are subgroups in our network, cliques.  ## the 'degree' of a node is the count of its connections. In this code chunk, we calculate degree, then make both a histogram of the counts and a plot of the network where we size the nodes proportionately to their degree. What do we learn from these two visualizations?\ndeg  - degree(net, mode=\"all\")\nhist(deg, breaks=1:vcount(net)-1, main=\"Histogram of node degree\")\nplot(net, vertex.size=deg*2, vertex.label = NA)\n## write this info to file for safekeeping\nwrite.csv(deg, 'degree.csv')    Now we'll look at closeness. If you know the width or  diameter  of your network (the maximum number of steps to get across it), then the node that is on average the shortest number of steps from all the others is the one that is most close. We calculate it like this:  closepeople  - closeness(net, mode=\"all\", weights=NA)\nsort(closepeople, decreasing = T) # so that we see who is most close first\nwrite.csv(closepeople, 'closeness.csv') # so we have it on file.    We can ask which individuals are hubs, and which are authorities. In the lingo, 'hubs' are individuals with many outgoing links (they  sent  lots of letters) while 'authorities' are individuals who  received  lots of letters. In the code below,  # In older DH Box version of igraph in RStudio: \nhs  - hub.score(net, weights=NA)$vector\nas  - authority.score(net, weights=NA)$vector\n\n# OR Newer version of igraph in desktop RStudio:\nhs  - hub_score(net, weights=NA)$vector\nas  - authority_score(net, weights=NA)$vector\n\npar(mfrow=c(1,2))\n\n# vertex.label.cex sets the size of the label; play with the sizes until you see something appealing.\nplot(net, vertex.size=hs*40, vertex.label.cex =.2, edge.arrow.size=.1, main=\"Hubs\")\nplot(net, vertex.size=as*20, vertex.label = NA, edge.arrow.size=.1, main=\"Authorities\")  Can you work out what command to give to write the hub score or the authority scores to a file?    Let's look for 'modules' within this network. Broadly speaking, these are clumps of nodes that have more or less the same pattern of ties between them,  within  the group, than without.  # In older DH Box version of igraph in RStudio: \ncfg  - fastgreedy.community(as.undirected(net))\n\n# OR Newer version of igraph in desktop RStudio:\ncfg  - cluster_fast_greedy(as.undirected(net))\n\nlapply(cfg, function(x) write.table( data.frame(x), 'cfg.csv'  , append= T, sep=',' ))  We create a new variable called 'cfg' and get the 'cluster_fast_greedy' algorithm to perform its calculations. The next line writes the groups to a file, separating each group with an x. (If you tried 'write.csv' as before, you'll get an error message because the output of the algorithm gives a different kind of data type. R is fussy like this.) Examine that file   what groups do you spot? What might these mean, if you went back to the content of the original letters?", 
            "title": "Getting the data into your project"
        }, 
        {
            "location": "/supporting materials/netviz/#visualization", 
            "text": "The line below will plot out our network, colouring it by the communities discerned above:  plot(cfg, net, vertex.size = 1, vertex.label.cex =.2, edge.arrow.size=.1, main=\"Communities\")    You can export the plot by clicking on the 'export' button in the plot panel, to PDF or to PNG. But this is a pretty ugly network. We need to apply a  layout  to try to make it more visually understandable. There are many different layout options in igraph. We'll assign the layout we want to a variable, and then we'll give that variable to the plot command:  # In older DH Box version of igraph in RStudio:\nl1  - layout.fruchterman.reingold(net)\n\n# OR Newer version of igraph in desktop RStudio:\nl1  - layout_with_fr(net)\n\nplot(cfg, net, layout=l1, vertex.size = 1, vertex.label.cex =.2, edge.arrow.size=.1, main=\"Communities\")  The 'layout_with_fr' is calling the 'Fruchterman-Reingold' algorithm, a kind of layout that imagines each node as a repulsive power, pushing against all the other nodes (it's part of a family of layouts called 'Forced Atlast'). That also means that if you ran the plot command a couple of times, the nodes might end up in different places each time   they are jostling for relative position, and this positioning itself carries no meaning in and of itself (so if a node is at the top of the screen, or the centre of the screen, don't read that to mean anything about importance).    Good luck! Remember to save your script, and upload the entire project folder to your GitHub repository.", 
            "title": "Visualization"
        }, 
        {
            "location": "/supporting materials/inkscape/", 
            "text": "Sprucing up a PDF in Inkscape\n\n\nSome of the tools that we used in Module 4 give visual output as raster images, others as vectors. Sometimes, we would like to tweak these outputs to make them more visually appealling, or clearer, or more useful. A program like MS Paint is only useful for dealing with raster images (and then, only in certain kinds of cases). We need a program that can deal with both, and also, lets us edit the image by treating each edit we do as a mostly-transparent layer on top of the image. That way, we can add, rearrange, hide or reveal, our edits to create a composite image. A free program that is immensely useful in this regard is \nInkscape\n. Inkscape is also quite useful in that we can open a pdf file in it, break the visual elements of the PDF into individual layers, and then rearrange/touch up/fix them up to make them more esthetically appealing.\n\n\nRaster versus Vector\n\n\nThe first thing to know is that graphic images come in two distinct flavours \n raster and vector.\n\n\nRaster images are composed of pixels, such that if you zoom into them, they become a pointilist blur (if you remember \nFerris Beuller's Day Off\n, there's a scene where Cameron stares and stares at a painting, falling into its individual points of colour). Vector images on the other hand are described by mathematical functions, of lines and arcs and areas. No matter how deep you delve into these kinds of images, the image is always sharp \n because the zoom is just another function of the mathematics describing the image.\n\n\nRasters:\n blocks of colours\n\n\nVectors:\n descriptions of points and arcs\n\n\nIn this first exercise, we will take the plot we generated in Module 4's exercise on topic modeling in R where we made a bar chart showing the number of articles by year. In R we exported that plot as a PDF. In Inkscape, we can import that pdf and 'explode' it so that we can manipulate its parts individually. We are going to take the simple bar chart and make it more legible, more visually appealing, for incorporation on a webpage.\n\n\n\n\n\n\nDownload and install \nInkscape\n \n\n\nNB Mac the installation instructions are a bit more complicated for Mac. Pay attention and follow closely!\n\n\n\n\n\n\nDownload the \nPDF we generated in R called publication-year.pdf (this downloads the PDF)\n\n\n\n\n\n\nOpen that PDF. It's a pretty plain graphic. Right away there are at least two things we could do to make it more visually appealling. We could change the orientation of the characters in the y-axis to make them more legible. We can highlight bars of interest. And we could apply a colour scheme more generally that would make our graphic legible to folks with colour-blindness (see the 'going further' section at bottom).\n\n\n\n\n\n\nStart Inkscape. \n\n\n\n\n\n\nClick File \n Import \n and then navigate to where you save the \npublication-year.pdf\n. \n\n\n\n\n\n\nClick Ok when you've got it selected. \n\n\n\n\n\n\nIn the next pop-up, just accept the default settings and click 'ok'. Your Inkscape window should now look like this:\n\n\n\n\n\n\n\n\nThe PDF is now a layer in the new image you are creating in Inkscape. You can save this drawing, \nwith its information about the layers and what is in each one\n by clicking File \n Save As. (\nVisit GitHub for my version\n). 'SVG' stands for 'scalable vector graphic'. (SVG is a kind of text file that describes the complete geometry of your illustration).\n\n\n\n\n\n\nDo you see the bounding box around the plot? If you grab any of those handles (the double-arrow things), you can make it bigger or smaller on your sheet. \nTo retain the image proportions, hold ctrl + shift as you drag.\n \n\n\n\n\n\n\nWe can't edit any of the other elements yet \n we can't change the colour of the bars, or the fonts of the text. We have to tell Inkscape to 'explode' these elements into their own 'objects'. In the menu bar at top, got to Object \n Ungroup. There are now a series of overlapping bounding boxes around each object.\n\n\n\n\n\n\nZoom in (by pressing the + sign on your keyboard) so that you're looking at the numbers of the y-axis. We're going to rotate these by 90 degrees to make them more legible. Select the arrow icon from the toolbar on the left side of the screen.\n\n\n\n\n\n\nClick on the '50'. You'll get a bounding box around it. \n\n\n\n\n\n\nClick Object \n Rotate 90 CW. The 50 is now rotated! Do the same for the other numbers. \n\n\n\n\n\n\nSave. (If you double-click on the number, you might trigger the 'text edit' function. If you do that, no problem \n you can change the font, change the number... although if you did that, it'd be a bit dishonest, right? Click on the arrow pointer icon in the toolbar again to get out of the text-editing function).\n\n\n\n\n\n\nLet's imagine, for whatever reason, that you wanted to change one of the bars to a different colour, to highlight its importance to your argument. With the arrow icon, click on one of the bars so that you get the bounding box around it. Then, click on one of the colours from the palette at the bottom. Boom! You've got a newly colourized bar. \n\n\n\n\n\n\nSave\n\n\n\n\n\n\nAdd a legend. Write it so that the important message you want your viewer to get is immediately clear. Choose a font from Inkscape's included fonts that \nsupports\n your message.\n\n\n\n\n\n\nTo export your image so that you can use it in a website or paper, click Edit \n Select All in All Layers. Every element of your image will now have a bounding box around it.\n\n\n\n\n\n\nGo to File \n Export Bitmap. Never mind the options in the popup; just hit 'Export'. Inkscape will automatically assign your drawing a name with .png; here's \nmy version on GitHub\n. \n\n\n\n\n\n\nRemember\n if you want to edit this image again later, hit the 'save' button to save it as an svg. The svg will preserve all your layer information, while the png file is the visual representation (the png is in fact a raster graphic). Most browsers can handle svg files, so you could use that in your website; programs like Word seem to be able to handle raster graphics better than they do svg. You might want to experiment. In any event, every journal has different requirements for image formats. You would export your image to whatever those specifications are.\n\n\nGoing further\n\n\nIn \ninfoheap's tutorial on inkscape\n, you will learn how to load a custom colour palette. Why might you want to do that? You should be designing your work so that it is as universally accessible as possible. Many folks are colour-blind. \n\n\n\n\nUse \nColor Brewer\n to generate a colour-blind safe palette. \n\n\nThen look for the 'GIMP and Inkscape \n GIMP color palette for this scheme.' \n\n\nClick on that link, and you'll get a text file with the scheme you generated. \n\n\nUse that scheme to alter the colours on your plot.", 
            "title": "Inkscape"
        }, 
        {
            "location": "/supporting materials/inkscape/#sprucing-up-a-pdf-in-inkscape", 
            "text": "Some of the tools that we used in Module 4 give visual output as raster images, others as vectors. Sometimes, we would like to tweak these outputs to make them more visually appealling, or clearer, or more useful. A program like MS Paint is only useful for dealing with raster images (and then, only in certain kinds of cases). We need a program that can deal with both, and also, lets us edit the image by treating each edit we do as a mostly-transparent layer on top of the image. That way, we can add, rearrange, hide or reveal, our edits to create a composite image. A free program that is immensely useful in this regard is  Inkscape . Inkscape is also quite useful in that we can open a pdf file in it, break the visual elements of the PDF into individual layers, and then rearrange/touch up/fix them up to make them more esthetically appealing.", 
            "title": "Sprucing up a PDF in Inkscape"
        }, 
        {
            "location": "/supporting materials/inkscape/#raster-versus-vector", 
            "text": "The first thing to know is that graphic images come in two distinct flavours   raster and vector.  Raster images are composed of pixels, such that if you zoom into them, they become a pointilist blur (if you remember  Ferris Beuller's Day Off , there's a scene where Cameron stares and stares at a painting, falling into its individual points of colour). Vector images on the other hand are described by mathematical functions, of lines and arcs and areas. No matter how deep you delve into these kinds of images, the image is always sharp   because the zoom is just another function of the mathematics describing the image.  Rasters:  blocks of colours  Vectors:  descriptions of points and arcs  In this first exercise, we will take the plot we generated in Module 4's exercise on topic modeling in R where we made a bar chart showing the number of articles by year. In R we exported that plot as a PDF. In Inkscape, we can import that pdf and 'explode' it so that we can manipulate its parts individually. We are going to take the simple bar chart and make it more legible, more visually appealing, for incorporation on a webpage.    Download and install  Inkscape    NB Mac the installation instructions are a bit more complicated for Mac. Pay attention and follow closely!    Download the  PDF we generated in R called publication-year.pdf (this downloads the PDF)    Open that PDF. It's a pretty plain graphic. Right away there are at least two things we could do to make it more visually appealling. We could change the orientation of the characters in the y-axis to make them more legible. We can highlight bars of interest. And we could apply a colour scheme more generally that would make our graphic legible to folks with colour-blindness (see the 'going further' section at bottom).    Start Inkscape.     Click File   Import   and then navigate to where you save the  publication-year.pdf .     Click Ok when you've got it selected.     In the next pop-up, just accept the default settings and click 'ok'. Your Inkscape window should now look like this:     The PDF is now a layer in the new image you are creating in Inkscape. You can save this drawing,  with its information about the layers and what is in each one  by clicking File   Save As. ( Visit GitHub for my version ). 'SVG' stands for 'scalable vector graphic'. (SVG is a kind of text file that describes the complete geometry of your illustration).    Do you see the bounding box around the plot? If you grab any of those handles (the double-arrow things), you can make it bigger or smaller on your sheet.  To retain the image proportions, hold ctrl + shift as you drag.      We can't edit any of the other elements yet   we can't change the colour of the bars, or the fonts of the text. We have to tell Inkscape to 'explode' these elements into their own 'objects'. In the menu bar at top, got to Object   Ungroup. There are now a series of overlapping bounding boxes around each object.    Zoom in (by pressing the + sign on your keyboard) so that you're looking at the numbers of the y-axis. We're going to rotate these by 90 degrees to make them more legible. Select the arrow icon from the toolbar on the left side of the screen.    Click on the '50'. You'll get a bounding box around it.     Click Object   Rotate 90 CW. The 50 is now rotated! Do the same for the other numbers.     Save. (If you double-click on the number, you might trigger the 'text edit' function. If you do that, no problem   you can change the font, change the number... although if you did that, it'd be a bit dishonest, right? Click on the arrow pointer icon in the toolbar again to get out of the text-editing function).    Let's imagine, for whatever reason, that you wanted to change one of the bars to a different colour, to highlight its importance to your argument. With the arrow icon, click on one of the bars so that you get the bounding box around it. Then, click on one of the colours from the palette at the bottom. Boom! You've got a newly colourized bar.     Save    Add a legend. Write it so that the important message you want your viewer to get is immediately clear. Choose a font from Inkscape's included fonts that  supports  your message.    To export your image so that you can use it in a website or paper, click Edit   Select All in All Layers. Every element of your image will now have a bounding box around it.    Go to File   Export Bitmap. Never mind the options in the popup; just hit 'Export'. Inkscape will automatically assign your drawing a name with .png; here's  my version on GitHub .     Remember  if you want to edit this image again later, hit the 'save' button to save it as an svg. The svg will preserve all your layer information, while the png file is the visual representation (the png is in fact a raster graphic). Most browsers can handle svg files, so you could use that in your website; programs like Word seem to be able to handle raster graphics better than they do svg. You might want to experiment. In any event, every journal has different requirements for image formats. You would export your image to whatever those specifications are.", 
            "title": "Raster versus Vector"
        }, 
        {
            "location": "/supporting materials/inkscape/#going-further", 
            "text": "In  infoheap's tutorial on inkscape , you will learn how to load a custom colour palette. Why might you want to do that? You should be designing your work so that it is as universally accessible as possible. Many folks are colour-blind.    Use  Color Brewer  to generate a colour-blind safe palette.   Then look for the 'GIMP and Inkscape   GIMP color palette for this scheme.'   Click on that link, and you'll get a text file with the scheme you generated.   Use that scheme to alter the colours on your plot.", 
            "title": "Going further"
        }, 
        {
            "location": "/supporting materials/gh-pages/", 
            "text": "Creating a GitHub pages site with MkDocs\n\n\nIn \nModule 5\n, you learned how to set up and serve a basic webpage on GitHub's gh-pages. GitHub allows you to host a website on a personal URL with the syntax \nhttps://myname.github.io\n. Many developers will use a static site generator to create a simple website run on gh-pages. Some popular static site generators are \nJekyll\n, \nHugo\n, and \nMkDocs\n. In fact, this website is built with MkDocs.\n\n\nStatic site generators are command line tools that use templates to build and create websites. Users define basic information in a yaml or toml filetype, write in markdown, and use a simple command to build it into a functioning static site. Due to the popularity of static site generators, most contain simple methods to link it to gh-pages. \n\n\nIn this exercise, we will use MkDocs in the DH Box to build a site that updates to GitHub and pushes changes to a \nhttps://myname.github.io\n URL. In this exercise we will use DH Box to download MkDocs and build the site. Although your changes will be saved in a GitHub repository, when your DH Box account expires, you will have to clone the website again and redowload everything. Therefore, if you want to use your MkDocs site past the purposes of this course, you may want to follow these steps on your desktop. These steps will be similar to using Mac's terminal, but may be different on a Windows machine.\n\n\nPreparing MkDocs in DH Box\n\n\n\n\n\n\nNavigate to the DH Box command line.\n\n\n\n\n\n\nType \n$ sudo pip install mkdocs\n into the command line.\n\n\n\n\n\n\nType \n$ mkdocs --version\n to ensure MkDocs was installed properly.\n\n\n\n\n\n\nType \n$ mkdocs new my-site\n or whatever you want your site to be called. Mine is called \nstatic-site\n.\n\n\n\n\n\n\nTypically, you could now view your site by typing \n$ mkdocs serve\n. This would not work within DH Box, though, since DH Box already runs within your browser. It will work from your desktop terminal.\n\n\n\n\n\n\nType \n$ ls\n to view the files MkDocs created. You will define your site within the \nmkdocs.yml\n file.\n\n\n\n\n\n\nType \n$ nano mkdocs.yml\n to open the config file.\n\n\n\n\n\n\nChange the \nsite_name\n to your site's name.\n\n\n\n\n\n\nPaste the following underneath \nsite_name\n:\n\n\npages:\n    - Home: index.md\ntheme: readthedocs\n\n\n\nThis sets up our Home and About pages and adds the theme called Read the Docs.\n\n\n\n\n\n\nHit ctrl-x, Y, enter to save and exit nano.\n\n\n\n\n\n\n\n\n\n\n\n\nCreating a git repository\n\n\n\n\n\n\nNavigate to GitHub and create a new repository with the same name as your MkDocs folder. For example, my MkDocs folder is called \nstatic-site\n. Therefore, I call my GitHub repository \nstatic-site\n.\n\n\n\n\n\n\nNavigate back to the command line.\n\n\n\n\n\n\nType \n$ git init\n to initialize the MkDocs folder as a git repository.\n\n\n\n\n\n\nType \n$ git add .\n to add the files and folders.\n\n\n\n\n\n\nType \n$ git commit -m \"First commit\"\n to commit your changes.\n\n\n\n\n\n\nType \n$ git remote add origin https://github.com/\nyourusername\n/\nyourrepository\n.git\n, making sure to add your URL to the GitHub repository.\n\n\n\n\n\n\nType \n$ git push -u origin master\n to push your changes.\n\n\n\n\n\n\nType \n$ mkdocs gh-deploy\n to push your folder to a gh-pages site.\n\n\nMkDocs will show a message saying \nINFO    -  Your documentation should shortly be available at: https://myname.github.io/my-site/\n. This means it pushed your changes to a project folder and not the main \nhttps://myname.github.io/\n. \n\n\n\n\n\n\nNavigate to your URL \nhttps://myname.github.io/my-site/\n. You should now see a webpage that looks a lot like the course website.\n\n\n\n\n\n\n\n\n\n\n\n\nAdding content to your site\n\n\nLet's say we now want to add an About page and a Blog page to our site. We need to define and create these files.\n\n\n\n\n\n\nNavigate to the command line.\n\n\n\n\n\n\nType \n$ ls\n to view your MkDocs files and folders. You will notice now a \nsite\n folder has been generated. This is simply the site folder created by MkDocs which is served on your \ngithub.io\n site.\n\n\n\n\n\n\nType \n$ nano mkdocs.yml\n to open the config file.\n\n\n\n\n\n\nAdd an about page and a posts page. Your file should contain the following:\n\n\nsite_name: Static Site Example\npages:\n    - Home: index.md\n    - About: about.md\n    - Blog: blog.md\ntheme: readthedocs\n\n\n\n\n\n\n\nHit ctrl-x, Y, enter to save and exit nano.\n\n\n\n\n\n\nType \n$ cd docs\n to enter the docs folder.\n\n\n\n\n\n\nType \n$ touch about.md\n to create the About file and \ntouch blog.md\n to create the blog file.\n\n\n\n\n\n\nType \n$ ls\n to make sure the files were created.\n\n\n\n\n\n\nType \n$ cd ..\n to go back to the main MkDocs folder.\n\n\n\n\n\n\nType \n$ git add .\n to add the files.\n\n\n\n\n\n\nType \n$ git commit -m \"Added about and blog files\"\n to commit your changes.\n\n\n\n\n\n\nType \n$ git push -u origin master\n to push your changes.\n\n\nThis ensures all your changes are pushed to your GitHub repository.\n\n\n\n\n\n\nType \n$ mkdocs gh-deploy\n to push your folder to a gh-pages site.\n\n\n\n\n\n\nReload your gh-pages site \nhttps://myname.github.io/\n.\n\n\nIt may take some time for your changes to update to your gh-pages site.\n\n\n\n\n\n\nBack in your \ndocs\n folder, you can use nano to edit any of the markdown files. If you add any files, make sure to declare them in \nmkdocs.yml\n. \n\n\n\n\n\n\n\n\n\n\n\n\nCustomizing your theme\n\n\nMkDocs also allows you to customize your theme. We don't want to directly edit the theme files, so we will create a new one.\n\n\n\n\n\n\nNavigate to the command line.\n\n\n\n\n\n\nIn your main MkDocs folder, type \n$ nano mkdocs.yml\n to open your configuration file.\n\n\n\n\n\n\nAdd the following text below the other configuration details:\n\n\nextra_css:\n  - 'css/extra.css'\n\n\n\nThis declaration tells the site to first look for styles in a folder called \ncss\n in a file called \nextra.css\n before going to the default theme.\n\n\n\n\n\n\nHit ctrl-x, Y, enter to save and exit nano.\n\n\n\n\n\n\nType \n$ cd docs\n to navigate to the docs folder.\n\n\n\n\n\n\nType \n$ mkdir css\n to create a folder called css.\n\n\n\n\n\n\nType \n$ cd css\n to enter the css folder.\n\n\n\n\n\n\nType \n$ nano extra.css\n to create an enter a css file called extra.\n\n\nLet's change the font of our site to Open Sans.\n\n\n\n\n\n\nAdd the following your css file:\n\n\nh1, h2, h3, h4, h5, h6, legend {\n    font-family: 'Open Sans', sans-serif;\n}\n\n\n\n\n\n\n\nHit ctrl-x, Y, enter to save and exit nano.\n\n\n\n\n\n\nType \n$ cd ..\n twice to go back up to your main MkDocs folder.\n\n\n\n\n\n\nUse git to add your files, commit, push, and then use mkdocs to deploy to gh-pages to view your changes.\n\n\nYou can now use you knowledge to customize your theme. Remember to use your browser inspector tool to check which rules you want to change.", 
            "title": "GitHub Pages"
        }, 
        {
            "location": "/supporting materials/gh-pages/#creating-a-github-pages-site-with-mkdocs", 
            "text": "In  Module 5 , you learned how to set up and serve a basic webpage on GitHub's gh-pages. GitHub allows you to host a website on a personal URL with the syntax  https://myname.github.io . Many developers will use a static site generator to create a simple website run on gh-pages. Some popular static site generators are  Jekyll ,  Hugo , and  MkDocs . In fact, this website is built with MkDocs.  Static site generators are command line tools that use templates to build and create websites. Users define basic information in a yaml or toml filetype, write in markdown, and use a simple command to build it into a functioning static site. Due to the popularity of static site generators, most contain simple methods to link it to gh-pages.   In this exercise, we will use MkDocs in the DH Box to build a site that updates to GitHub and pushes changes to a  https://myname.github.io  URL. In this exercise we will use DH Box to download MkDocs and build the site. Although your changes will be saved in a GitHub repository, when your DH Box account expires, you will have to clone the website again and redowload everything. Therefore, if you want to use your MkDocs site past the purposes of this course, you may want to follow these steps on your desktop. These steps will be similar to using Mac's terminal, but may be different on a Windows machine.", 
            "title": "Creating a GitHub pages site with MkDocs"
        }, 
        {
            "location": "/supporting materials/gh-pages/#preparing-mkdocs-in-dh-box", 
            "text": "Navigate to the DH Box command line.    Type  $ sudo pip install mkdocs  into the command line.    Type  $ mkdocs --version  to ensure MkDocs was installed properly.    Type  $ mkdocs new my-site  or whatever you want your site to be called. Mine is called  static-site .    Typically, you could now view your site by typing  $ mkdocs serve . This would not work within DH Box, though, since DH Box already runs within your browser. It will work from your desktop terminal.    Type  $ ls  to view the files MkDocs created. You will define your site within the  mkdocs.yml  file.    Type  $ nano mkdocs.yml  to open the config file.    Change the  site_name  to your site's name.    Paste the following underneath  site_name :  pages:\n    - Home: index.md\ntheme: readthedocs  This sets up our Home and About pages and adds the theme called Read the Docs.    Hit ctrl-x, Y, enter to save and exit nano.", 
            "title": "Preparing MkDocs in DH Box"
        }, 
        {
            "location": "/supporting materials/gh-pages/#creating-a-git-repository", 
            "text": "Navigate to GitHub and create a new repository with the same name as your MkDocs folder. For example, my MkDocs folder is called  static-site . Therefore, I call my GitHub repository  static-site .    Navigate back to the command line.    Type  $ git init  to initialize the MkDocs folder as a git repository.    Type  $ git add .  to add the files and folders.    Type  $ git commit -m \"First commit\"  to commit your changes.    Type  $ git remote add origin https://github.com/ yourusername / yourrepository .git , making sure to add your URL to the GitHub repository.    Type  $ git push -u origin master  to push your changes.    Type  $ mkdocs gh-deploy  to push your folder to a gh-pages site.  MkDocs will show a message saying  INFO    -  Your documentation should shortly be available at: https://myname.github.io/my-site/ . This means it pushed your changes to a project folder and not the main  https://myname.github.io/ .     Navigate to your URL  https://myname.github.io/my-site/ . You should now see a webpage that looks a lot like the course website.", 
            "title": "Creating a git repository"
        }, 
        {
            "location": "/supporting materials/gh-pages/#adding-content-to-your-site", 
            "text": "Let's say we now want to add an About page and a Blog page to our site. We need to define and create these files.    Navigate to the command line.    Type  $ ls  to view your MkDocs files and folders. You will notice now a  site  folder has been generated. This is simply the site folder created by MkDocs which is served on your  github.io  site.    Type  $ nano mkdocs.yml  to open the config file.    Add an about page and a posts page. Your file should contain the following:  site_name: Static Site Example\npages:\n    - Home: index.md\n    - About: about.md\n    - Blog: blog.md\ntheme: readthedocs    Hit ctrl-x, Y, enter to save and exit nano.    Type  $ cd docs  to enter the docs folder.    Type  $ touch about.md  to create the About file and  touch blog.md  to create the blog file.    Type  $ ls  to make sure the files were created.    Type  $ cd ..  to go back to the main MkDocs folder.    Type  $ git add .  to add the files.    Type  $ git commit -m \"Added about and blog files\"  to commit your changes.    Type  $ git push -u origin master  to push your changes.  This ensures all your changes are pushed to your GitHub repository.    Type  $ mkdocs gh-deploy  to push your folder to a gh-pages site.    Reload your gh-pages site  https://myname.github.io/ .  It may take some time for your changes to update to your gh-pages site.    Back in your  docs  folder, you can use nano to edit any of the markdown files. If you add any files, make sure to declare them in  mkdocs.yml .", 
            "title": "Adding content to your site"
        }, 
        {
            "location": "/supporting materials/gh-pages/#customizing-your-theme", 
            "text": "MkDocs also allows you to customize your theme. We don't want to directly edit the theme files, so we will create a new one.    Navigate to the command line.    In your main MkDocs folder, type  $ nano mkdocs.yml  to open your configuration file.    Add the following text below the other configuration details:  extra_css:\n  - 'css/extra.css'  This declaration tells the site to first look for styles in a folder called  css  in a file called  extra.css  before going to the default theme.    Hit ctrl-x, Y, enter to save and exit nano.    Type  $ cd docs  to navigate to the docs folder.    Type  $ mkdir css  to create a folder called css.    Type  $ cd css  to enter the css folder.    Type  $ nano extra.css  to create an enter a css file called extra.  Let's change the font of our site to Open Sans.    Add the following your css file:  h1, h2, h3, h4, h5, h6, legend {\n    font-family: 'Open Sans', sans-serif;\n}    Hit ctrl-x, Y, enter to save and exit nano.    Type  $ cd ..  twice to go back up to your main MkDocs folder.    Use git to add your files, commit, push, and then use mkdocs to deploy to gh-pages to view your changes.  You can now use you knowledge to customize your theme. Remember to use your browser inspector tool to check which rules you want to change.", 
            "title": "Customizing your theme"
        }, 
        {
            "location": "/supporting materials/leaflet.txt/", 
            "text": "Making a map website with Leaflet.js\n\n\nThe \nleaflet.js\n library allows you to create quite nice interactive maps in a web-browser, that are also mobile friendly. Here, we'll build a map that uses our georectified map as the base layer (rather than as an image overlay). I won't go into the details, but rather will provide you enough guidance to get going. The documentation for Leaflet is quite extensive, and many other tutorials abound.\n\n\nSetup\n\n\n\n\n\n\nCreate a new GitHub repository for this exercise. \n\n\n\n\n\n\nCreate a new branch called \ngh-pages\n. We will be putting our html on the gh-pages branch, so that \nyour username\n/github.io/\nrepo\nmap.html\n can serve us up the webpage when we're done.\n\n\n\n\n\n\nLeaflet comes with a number of \nexcellent tutorials\n. We're going to look at the \nfirst one\n. \n\n\n\n\n\n\nGo to the \nleaflet quick-start tutorial\n and read through it carefully. In essence, you create a webpage that draws its instructions on how to handle geographic data and how to style that data from the leaflet.js source. That way, the browser knows how to render all the geographic information you're about to give it. \n\n\nDo you see where leaflet is calling on geographic information? This bit:\n\n\nL.tileLayer('http://{s}.tiles.mapbox.com/v3/MapID/{z}/{x}/{y}.png', {\n    attribution: 'Map data \ncopy; \na href=\"http://openstreetmap.org\"\nOpenStreetMap\n/a\n contributors, \na href=\"http://creativecommons.org/licenses/by-sa/2.0/\"\nCC-BY-SA\n/a\n, Imagery \u00a9 \na href=\"http://mapbox.com\"\nMapbox\n/a\n',\n    maxZoom: 18\n}).addTo(map);\n\n\n\nis calling on a background layer from the mapbox service. Instead of using mapbox, We can slot the url to the georectified map we made in module 4 in there! That is, swap out the url from the line beginning \nL.tileLayer\n. You'd change the copyright notice, etc, too, obviously.\n\n\nThe other bits of code that create callouts, polygons, and so on, are all using decimal degrees to locate the drawn elements on top of that map.\n\n\nThis code:\n\n\nL.marker([51.5, -0.09]).addTo(map)\n            .bindPopup(\"\nb\nHello world!\n/b\nbr /\nI am a popup.\").openPopup();\n\n\n\ncan be copied and repeated in the document, with new coordinates in decimal degrees for each new point. In the second line, between the quotation marks, you can use regular html to style your text, include pictures, and so on.\n\n\n\n\n\n\nIn a new browser window, open the \nexample map for the quickstart guide\n.\n\n\n\n\n\n\nRight-click the page and select view source.\n\n\n\n\n\n\nSo let's get started.\n\n\n\n\n\n\nCreate a new html document in your gh-pages branch of your repo.\n\n\n\n\n\n\nCopy the html from the quickstart map (right-click and select 'view source' on \nLeaflet's quick start example page\n.\n\n\n\n\n\n\nPaste the code in your new html document in your gh-pages branch of your repo. Call it 'map.html' and commit your changes.\n\n\n\n\n\n\nChange the source map to point to a georectified map you made in Module 4. Using the Ottawa Fire Insurance map I used as an example in module 4, I created \nthis map\n. \n\n\n\n\n\n\nRight click and view my page source to see what I changed up.\n\n\nNB You could keep the basic mapbox service base map, and render the Ottawa Fire Insurance map as an overlay [reference documentation]http://leafletjs.com/reference-1.0.3.html#imageoverlay). Or you could do a series of overlays, showing the change in the city over time. (My favourite example of a leaflet-powered historical map visualization is the \nSlave Revolt in Jamaica project\n by Vincent Brown). But you don't necessarily have to do this.\n\n\n\n\n\n\nAdd a series of markers with historical information by duplicating and then changing up the \nL.marker\n settings to your own data. Commit your changes!\n\n\nThis all just makes the map. The rest of the webpage would have to be styled as you would normally for a webpage. That is, you'd probably want to add an explanation about what the map shows, how it was created, and how the user ought to interact with it, links to your source data, and so on. The easiest place to add all that kind of information would be between these two tags in the page source:\n\n\n/script\n\n\n/body\n\n\n\n\n\n\n\n\nGoing further\n\n\nLet's say you have a whole bunch of information that you want to represent on the map. Perhaps it's in a well organized CSV file, with a latitude and a longitude column in decimal degrees. Adding points one at a time to the map as described above would take ages. Instead, let's convert that CSV to geojson, and then use \nbootleaf\n to make a map.\n\n\nBootleaf is a template that uses a common html template package, '\nBootstrap\n' as a wrapper for a leaflet powered map that draws its points of interest from a geojson file. To get this up and running, do the following steps:\n\n\n\n\n\n\nGo to the \nGitHub repo for bootleaf\n.\n\n\n\n\n\n\nFork a copy to a new repo (you have to be logged into github.com) by hitting the 'fork' button.\n\n\n\n\n\n\nIn your copy of bootleaf, you now have a gh-pages version of the site. If you go to \nyourusername\n.github.io/bootleaf\n you should see an active version of the map.\n\n\n\n\n\n\nNow, the map is grabbing its data from a series of geojson files. You can use \nthe 'to geo json' service\n to convert your CSV to geojson. There are other services.\n\n\n\n\n\n\nClone your repository in your desktop (by pressing the clone your repo in desktop).\n\n\n\n\n\n\nOpen your desktop client, and make sure you're in the gh-pages branch\n\n\n\n\n\n\nUsing your Windows explorer or Mac finder, put your newly created geojson file in the data folder.\n\n\n\n\n\n\nCommit and sync your changes.\n\n\n\n\n\n\nTo add your data to the dropdown menu, you need to change the code in the index.html file:\n\n\n  \nli class=\"dropdown\"\n\n    \na class=\"dropdown-toggle\" id=\"downloadDrop\" href=\"#\" role=\"button\" data-toggle=\"dropdown\"\ni class=\"fa fa-cloud-download white\"\n/i\nnbsp;\nnbsp;Download \nb class=\"caret\"\n/b\n/a\n\n    \nul class=\"dropdown-menu\"\n\n      \nli\na href=\"data/boroughs.geojson\" download=\"boroughs.geojson\" target=\"_blank\" data-toggle=\"collapse\" data-target=\".navbar-collapse.in\"\ni class=\"fa fa-download\"\n/i\nnbsp;\nnbsp;Boroughs\n/a\n/li\n\n      \nli\na href=\"data/subways.geojson\" download=\"subways.geojson\" target=\"_blank\" data-toggle=\"collapse\" data-target=\".navbar-collapse.in\"\ni class=\"fa fa-download\"\n/i\nnbsp;\nnbsp;Subway Lines\n/a\n/li\n\n      \nli\na href=\"data/DOITT_THEATER_01_13SEPT2010.geojson\" download=\"theaters.geojson\" target=\"_blank\" data-toggle=\"collapse\" data-target=\".navbar-collapse.in\"\ni class=\"fa fa-download\"\n/i\nnbsp;\nnbsp;Theaters\n/a\n/li\n\n      \nli\na href=\"data/DOITT_MUSEUM_01_13SEPT2010.geojson\" download=\"museums.geojson\" target=\"_blank\" data-toggle=\"collapse\" data-target=\".navbar-collapse.in\"\ni class=\"fa fa-download\"\n/i\nnbsp;\nnbsp;Museums\n/a\n/li\n\n    \n/ul\n\n\n\n\nAnd since you probably don't want that other stuff, you could delete it.\n\n\n\n\n\n\nYou could change the 'about' pop-up here:\n\n\ndiv class=\"tab-pane fade active in\" id=\"about\"\n\n  \np\nA simple, responsive template for building web mapping applications with \na href=\"http://getbootstrap.com/\"\nBootstrap 3\n/a\n, \na href=\"http://leafletjs.com/\" target=\"_blank\"\nLeaflet\n/a\n, and \na href=\"http://twitter.github.io/typeahead.js/\" target=\"_blank\"\ntypeahead.js\n/a\n. Open source, MIT licensed, and available on \na href=\"https://github.com/bmcbride/bootleaf\" target=\"_blank\"\nGitHub\n/a\n.\n/p\n\n  \ndiv class=\"panel panel-primary\"\n\n    \ndiv class=\"panel-heading\"\nFeatures\n/div\n\n    \nul class=\"list-group\"\n\n      \nli class=\"list-group-item\"\nFullscreen mobile-friendly map template with responsive navbar and modal placeholders\n/li\n\n      \nli class=\"list-group-item\"\njQuery loading of external GeoJSON files\n/li\n\n      \nli class=\"list-group-item\"\nLogical multiple layer marker clustering via the \na href=\"https://github.com/Leaflet/Leaflet.markercluster\" target=\"_blank\"\nleaflet marker cluster plugin\n/a\n/li\n\n      \nli class=\"list-group-item\"\nElegant client-side multi-layer feature search with autocomplete using \na href=\"http://twitter.github.io/typeahead.js/\" target=\"_blank\"\ntypeahead.js\n/a\n/li\n\n      \nli class=\"list-group-item\"\nResponsive sidebar feature list synced with map bounds, which includes sorting and filtering via \na href=\"http://listjs.com/\" target=\"_blank\"\nlist.js\n/a\n/li\n\n      \nli class=\"list-group-item\"\nMarker icons included in grouped layer control via the \na href=\"https://github.com/ismyrnow/Leaflet.groupedlayercontrol\" target=\"_blank\"\ngrouped layer control plugin\n/a\n/li\n\n    \n/ul\n\n  \n/div\n\n\n\n\n\n\n\n\nAnd you have to remove this:\n\n\n    \n!-- Remove this maptiks analytics code from your BootLeaf implementation --\n\n    \nscript src=\"//cdn.maptiks.com/maptiks-leaflet.min.js\"\n/script\n\n    \nscript\nmaptiks.trackcode='c7ca251e-9c17-47ef-ac33-e0fb24e05976';\n/script\n\n    \n!-- End maptiks analytics code --\n\n\n\n\nNow the really hard part: putting your own base maps in. \n\n\n\n\n\n\nTo insert your own base map, you have to find, and modify, a file called app.js. You should be able to find it by following this path: bootleaf/assets/js/app.js\n\n\n\n\n\n\nYou need to change these lines to point to your maps\n\n\n/* Basemap Layers */\nvar mapquestOSM = L.tileLayer(\"http://{s}.mqcdn.com/tiles/1.0.0/osm/{z}/{x}/{y}.png\", {\n  maxZoom: 19,\n  subdomains: [\"otile1\", \"otile2\", \"otile3\", \"otile4\"],\n  attribution: 'Tiles courtesy of \na href=\"http://www.mapquest.com/\" target=\"_blank\"\nMapQuest\n/a\n \nimg src=\"http://developer.mapquest.com/content/osm/mq_logo.png\"\n. Map data (c) \na href=\"http://www.openstreetmap.org/\" target=\"_blank\"\nOpenStreetMap\n/a\n contributors, CC-BY-SA.'\n});\nvar mapquestOAM = L.tileLayer(\"http://{s}.mqcdn.com/tiles/1.0.0/sat/{z}/{x}/{y}.jpg\", {\n  maxZoom: 18,\n  subdomains: [\"oatile1\", \"oatile2\", \"oatile3\", \"oatile4\"],\n  attribution: 'Tiles courtesy of \na href=\"http://www.mapquest.com/\" target=\"_blank\"\nMapQuest\n/a\n. Portions Courtesy NASA/JPL-Caltech and U.S. Depart. of Agriculture, Farm Service Agency'\n});\nvar mapquestHYB = L.layerGroup([L.tileLayer(\"http://{s}.mqcdn.com/tiles/1.0.0/sat/{z}/{x}/{y}.jpg\", {\n  maxZoom: 18,\n  subdomains: [\"oatile1\", \"oatile2\", \"oatile3\", \"oatile4\"]\n}), L.tileLayer(\"http://{s}.mqcdn.com/tiles/1.0.0/hyb/{z}/{x}/{y}.png\", {\n  maxZoom: 19,\n  subdomains: [\"oatile1\", \"oatile2\", \"oatile3\", \"oatile4\"],\n  attribution: 'Labels courtesy of \na href=\"http://www.mapquest.com/\" target=\"_blank\"\nMapQuest\n/a\n \nimg src=\"http://developer.mapquest.com/content/osm/mq_logo.png\"\n. Map data (c) \na href=\"http://www.openstreetmap.org/\" target=\"_blank\"\nOpenStreetMap\n/a\n contributors, CC-BY-SA. Portions Courtesy NASA/JPL-Caltech and U.S. Depart. of Agriculture, Farm Service Agency'\n})]);\n\n\n\n...and then you'd have to go through the rest of that file and change up the .geojson pointers to point to your own data.\n\n\n\n\n\n\nVisit GitHub for a \ntemplate for mapping with leaflet\n, drawing all of your point data and ancillary information from a CSV file. Study the index.html file carefully to identify which lines you'd modify to change the base map, and to identify how elements in the CSV are being rendered on the screen. \nVisit this example that a former student made\n.\n\n\n\n\n\n\nFurther reading:\n Arian Katsimbras on making beautiful maps with \nTilemill\n.", 
            "title": "Leaflet"
        }, 
        {
            "location": "/supporting materials/leaflet.txt/#making-a-map-website-with-leafletjs", 
            "text": "The  leaflet.js  library allows you to create quite nice interactive maps in a web-browser, that are also mobile friendly. Here, we'll build a map that uses our georectified map as the base layer (rather than as an image overlay). I won't go into the details, but rather will provide you enough guidance to get going. The documentation for Leaflet is quite extensive, and many other tutorials abound.", 
            "title": "Making a map website with Leaflet.js"
        }, 
        {
            "location": "/supporting materials/leaflet.txt/#setup", 
            "text": "Create a new GitHub repository for this exercise.     Create a new branch called  gh-pages . We will be putting our html on the gh-pages branch, so that  your username /github.io/ repo map.html  can serve us up the webpage when we're done.    Leaflet comes with a number of  excellent tutorials . We're going to look at the  first one .     Go to the  leaflet quick-start tutorial  and read through it carefully. In essence, you create a webpage that draws its instructions on how to handle geographic data and how to style that data from the leaflet.js source. That way, the browser knows how to render all the geographic information you're about to give it.   Do you see where leaflet is calling on geographic information? This bit:  L.tileLayer('http://{s}.tiles.mapbox.com/v3/MapID/{z}/{x}/{y}.png', {\n    attribution: 'Map data  copy;  a href=\"http://openstreetmap.org\" OpenStreetMap /a  contributors,  a href=\"http://creativecommons.org/licenses/by-sa/2.0/\" CC-BY-SA /a , Imagery \u00a9  a href=\"http://mapbox.com\" Mapbox /a ',\n    maxZoom: 18\n}).addTo(map);  is calling on a background layer from the mapbox service. Instead of using mapbox, We can slot the url to the georectified map we made in module 4 in there! That is, swap out the url from the line beginning  L.tileLayer . You'd change the copyright notice, etc, too, obviously.  The other bits of code that create callouts, polygons, and so on, are all using decimal degrees to locate the drawn elements on top of that map.  This code:  L.marker([51.5, -0.09]).addTo(map)\n            .bindPopup(\" b Hello world! /b br / I am a popup.\").openPopup();  can be copied and repeated in the document, with new coordinates in decimal degrees for each new point. In the second line, between the quotation marks, you can use regular html to style your text, include pictures, and so on.    In a new browser window, open the  example map for the quickstart guide .    Right-click the page and select view source.", 
            "title": "Setup"
        }, 
        {
            "location": "/supporting materials/leaflet.txt/#so-lets-get-started", 
            "text": "Create a new html document in your gh-pages branch of your repo.    Copy the html from the quickstart map (right-click and select 'view source' on  Leaflet's quick start example page .    Paste the code in your new html document in your gh-pages branch of your repo. Call it 'map.html' and commit your changes.    Change the source map to point to a georectified map you made in Module 4. Using the Ottawa Fire Insurance map I used as an example in module 4, I created  this map .     Right click and view my page source to see what I changed up.  NB You could keep the basic mapbox service base map, and render the Ottawa Fire Insurance map as an overlay [reference documentation]http://leafletjs.com/reference-1.0.3.html#imageoverlay). Or you could do a series of overlays, showing the change in the city over time. (My favourite example of a leaflet-powered historical map visualization is the  Slave Revolt in Jamaica project  by Vincent Brown). But you don't necessarily have to do this.    Add a series of markers with historical information by duplicating and then changing up the  L.marker  settings to your own data. Commit your changes!  This all just makes the map. The rest of the webpage would have to be styled as you would normally for a webpage. That is, you'd probably want to add an explanation about what the map shows, how it was created, and how the user ought to interact with it, links to your source data, and so on. The easiest place to add all that kind of information would be between these two tags in the page source:  /script  /body", 
            "title": "So let's get started."
        }, 
        {
            "location": "/supporting materials/leaflet.txt/#going-further", 
            "text": "Let's say you have a whole bunch of information that you want to represent on the map. Perhaps it's in a well organized CSV file, with a latitude and a longitude column in decimal degrees. Adding points one at a time to the map as described above would take ages. Instead, let's convert that CSV to geojson, and then use  bootleaf  to make a map.  Bootleaf is a template that uses a common html template package, ' Bootstrap ' as a wrapper for a leaflet powered map that draws its points of interest from a geojson file. To get this up and running, do the following steps:    Go to the  GitHub repo for bootleaf .    Fork a copy to a new repo (you have to be logged into github.com) by hitting the 'fork' button.    In your copy of bootleaf, you now have a gh-pages version of the site. If you go to  yourusername .github.io/bootleaf  you should see an active version of the map.    Now, the map is grabbing its data from a series of geojson files. You can use  the 'to geo json' service  to convert your CSV to geojson. There are other services.    Clone your repository in your desktop (by pressing the clone your repo in desktop).    Open your desktop client, and make sure you're in the gh-pages branch    Using your Windows explorer or Mac finder, put your newly created geojson file in the data folder.    Commit and sync your changes.    To add your data to the dropdown menu, you need to change the code in the index.html file:     li class=\"dropdown\" \n     a class=\"dropdown-toggle\" id=\"downloadDrop\" href=\"#\" role=\"button\" data-toggle=\"dropdown\" i class=\"fa fa-cloud-download white\" /i nbsp; nbsp;Download  b class=\"caret\" /b /a \n     ul class=\"dropdown-menu\" \n       li a href=\"data/boroughs.geojson\" download=\"boroughs.geojson\" target=\"_blank\" data-toggle=\"collapse\" data-target=\".navbar-collapse.in\" i class=\"fa fa-download\" /i nbsp; nbsp;Boroughs /a /li \n       li a href=\"data/subways.geojson\" download=\"subways.geojson\" target=\"_blank\" data-toggle=\"collapse\" data-target=\".navbar-collapse.in\" i class=\"fa fa-download\" /i nbsp; nbsp;Subway Lines /a /li \n       li a href=\"data/DOITT_THEATER_01_13SEPT2010.geojson\" download=\"theaters.geojson\" target=\"_blank\" data-toggle=\"collapse\" data-target=\".navbar-collapse.in\" i class=\"fa fa-download\" /i nbsp; nbsp;Theaters /a /li \n       li a href=\"data/DOITT_MUSEUM_01_13SEPT2010.geojson\" download=\"museums.geojson\" target=\"_blank\" data-toggle=\"collapse\" data-target=\".navbar-collapse.in\" i class=\"fa fa-download\" /i nbsp; nbsp;Museums /a /li \n     /ul   And since you probably don't want that other stuff, you could delete it.    You could change the 'about' pop-up here:  div class=\"tab-pane fade active in\" id=\"about\" \n   p A simple, responsive template for building web mapping applications with  a href=\"http://getbootstrap.com/\" Bootstrap 3 /a ,  a href=\"http://leafletjs.com/\" target=\"_blank\" Leaflet /a , and  a href=\"http://twitter.github.io/typeahead.js/\" target=\"_blank\" typeahead.js /a . Open source, MIT licensed, and available on  a href=\"https://github.com/bmcbride/bootleaf\" target=\"_blank\" GitHub /a . /p \n   div class=\"panel panel-primary\" \n     div class=\"panel-heading\" Features /div \n     ul class=\"list-group\" \n       li class=\"list-group-item\" Fullscreen mobile-friendly map template with responsive navbar and modal placeholders /li \n       li class=\"list-group-item\" jQuery loading of external GeoJSON files /li \n       li class=\"list-group-item\" Logical multiple layer marker clustering via the  a href=\"https://github.com/Leaflet/Leaflet.markercluster\" target=\"_blank\" leaflet marker cluster plugin /a /li \n       li class=\"list-group-item\" Elegant client-side multi-layer feature search with autocomplete using  a href=\"http://twitter.github.io/typeahead.js/\" target=\"_blank\" typeahead.js /a /li \n       li class=\"list-group-item\" Responsive sidebar feature list synced with map bounds, which includes sorting and filtering via  a href=\"http://listjs.com/\" target=\"_blank\" list.js /a /li \n       li class=\"list-group-item\" Marker icons included in grouped layer control via the  a href=\"https://github.com/ismyrnow/Leaflet.groupedlayercontrol\" target=\"_blank\" grouped layer control plugin /a /li \n     /ul \n   /div     And you have to remove this:       !-- Remove this maptiks analytics code from your BootLeaf implementation -- \n     script src=\"//cdn.maptiks.com/maptiks-leaflet.min.js\" /script \n     script maptiks.trackcode='c7ca251e-9c17-47ef-ac33-e0fb24e05976'; /script \n     !-- End maptiks analytics code --   Now the really hard part: putting your own base maps in.     To insert your own base map, you have to find, and modify, a file called app.js. You should be able to find it by following this path: bootleaf/assets/js/app.js    You need to change these lines to point to your maps  /* Basemap Layers */\nvar mapquestOSM = L.tileLayer(\"http://{s}.mqcdn.com/tiles/1.0.0/osm/{z}/{x}/{y}.png\", {\n  maxZoom: 19,\n  subdomains: [\"otile1\", \"otile2\", \"otile3\", \"otile4\"],\n  attribution: 'Tiles courtesy of  a href=\"http://www.mapquest.com/\" target=\"_blank\" MapQuest /a   img src=\"http://developer.mapquest.com/content/osm/mq_logo.png\" . Map data (c)  a href=\"http://www.openstreetmap.org/\" target=\"_blank\" OpenStreetMap /a  contributors, CC-BY-SA.'\n});\nvar mapquestOAM = L.tileLayer(\"http://{s}.mqcdn.com/tiles/1.0.0/sat/{z}/{x}/{y}.jpg\", {\n  maxZoom: 18,\n  subdomains: [\"oatile1\", \"oatile2\", \"oatile3\", \"oatile4\"],\n  attribution: 'Tiles courtesy of  a href=\"http://www.mapquest.com/\" target=\"_blank\" MapQuest /a . Portions Courtesy NASA/JPL-Caltech and U.S. Depart. of Agriculture, Farm Service Agency'\n});\nvar mapquestHYB = L.layerGroup([L.tileLayer(\"http://{s}.mqcdn.com/tiles/1.0.0/sat/{z}/{x}/{y}.jpg\", {\n  maxZoom: 18,\n  subdomains: [\"oatile1\", \"oatile2\", \"oatile3\", \"oatile4\"]\n}), L.tileLayer(\"http://{s}.mqcdn.com/tiles/1.0.0/hyb/{z}/{x}/{y}.png\", {\n  maxZoom: 19,\n  subdomains: [\"oatile1\", \"oatile2\", \"oatile3\", \"oatile4\"],\n  attribution: 'Labels courtesy of  a href=\"http://www.mapquest.com/\" target=\"_blank\" MapQuest /a   img src=\"http://developer.mapquest.com/content/osm/mq_logo.png\" . Map data (c)  a href=\"http://www.openstreetmap.org/\" target=\"_blank\" OpenStreetMap /a  contributors, CC-BY-SA. Portions Courtesy NASA/JPL-Caltech and U.S. Depart. of Agriculture, Farm Service Agency'\n})]);  ...and then you'd have to go through the rest of that file and change up the .geojson pointers to point to your own data.    Visit GitHub for a  template for mapping with leaflet , drawing all of your point data and ancillary information from a CSV file. Study the index.html file carefully to identify which lines you'd modify to change the base map, and to identify how elements in the CSV are being rendered on the screen.  Visit this example that a former student made .    Further reading:  Arian Katsimbras on making beautiful maps with  Tilemill .", 
            "title": "Going further"
        }
    ]
}